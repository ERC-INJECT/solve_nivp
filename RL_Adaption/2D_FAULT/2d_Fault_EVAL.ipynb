{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ab5326",
   "metadata": {},
   "source": [
    "# RL-controlled adaptive time stepping (user-defined)\n",
    "\n",
    "This notebook evaluates reinforcement-learning controllers built on top of\n",
    "\n",
    "`solve_nivp`. The reward shaping, observation mapping, and policy choices\n",
    "\n",
    "are defined in the notebook itself and are **not** part of the package API.\n",
    "\n",
    "Treat them as example user code that you can adapt or replace. Running this\n",
    "\n",
    "evaluation is optional and not required for installing, testing, or using\n",
    "\n",
    "the core solver library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3d156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plants.faults import strikeslip\n",
    "import os\n",
    "\n",
    "import solve_nivp  # Ensure that this module is in your PYTHONPATH\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3158bb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as anp\n",
    "from scipy.optimize import root\n",
    "from abc import ABC, abstractmethod\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp\n",
    "from tqdm import tqdm\n",
    "import time \n",
    "\n",
    "import torch as th\n",
    "from torch import nn\n",
    "from stable_baselines3.td3.policies import TD3Policy\n",
    "from stable_baselines3.sac.policies import SACPolicy  \n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "from stable_baselines3 import TD3, DDPG, SAC\n",
    "from stable_baselines3.common.env_util import make_vec_env, DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "from solve_nivp.rl.env import AdaptiveStepperEnv\n",
    "from solve_nivp.rl.callbacks import RewardCallback, CustomMetricsCallback\n",
    "\n",
    "sizes = 30\n",
    "plt.rcParams.update({\n",
    "    'font.size': sizes,\n",
    "    'axes.titlesize': sizes,\n",
    "    'axes.labelsize': sizes,\n",
    "    'xtick.labelsize': sizes,\n",
    "    'ytick.labelsize': sizes,\n",
    "    'legend.fontsize': sizes,\n",
    "    'figure.titlesize': sizes,\n",
    "    'text.usetex': True,\n",
    "    'font.family': 'Times',\n",
    "    # Embed fonts in vector outputs so text is editable\n",
    "    'pdf.fonttype': 42,\n",
    "    'ps.fonttype': 42,\n",
    "    # Keep LaTeX preamble minimal to prevent bloat and ensure Times math\n",
    "    'text.latex.preamble': r'\\usepackage{amsmath}\\usepackage{mathptmx}',\n",
    "    # Cleaner default look; we'll style axes explicitly in plotting code\n",
    "    'axes.grid': False,\n",
    "})\n",
    "\n",
    "\n",
    "import solve_nivp  # Ensure that this module is in your PYTHONPATH\n",
    "th.set_default_dtype(th.float32)\n",
    "\n",
    "from solve_nivp import solve_ivp_ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d0906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATA_DIR = str((Path.cwd() / \"Data\").resolve())\n",
    "\n",
    "# Initialize fault model\n",
    "fault = strikeslip.qs_strikeslip_fault(\n",
    "    zdepth=3, xlength=3, Nz=1, Nx=1,  \n",
    "    G=30000., rho=2.5e-3, zeta=0.8/3, \n",
    "    Ks_path=\"./Data/\", gamma_s=25., gamma_w=10., \n",
    "    sigma_ref=100., depth_ini=0., vinf=3.171e-10, \n",
    "    Dmu_estimate=.5,\n",
    ")\n",
    "\n",
    "# Get plant parameters\n",
    "MA, KS, ES, SIGMA_N, VINF_raw = fault.get_plant()\n",
    "\n",
    "# Print diagnostic information\n",
    "print(f\"Theoretical value of total stiffness assuming a patch of dimensions Lx x Lz: {KS.sum()}\")\n",
    "print(f\"Approximate value of total stiffness assuming a patch of length Lx: {3.333333333333333e-07}\")\n",
    "print(f\"Calculated value: {3.962559156372552e-07}\")\n",
    "print(f\"Units are in: mm,N,ms,MPa,gr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff165ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "N_DOFS = fault.N                                # int\n",
    "VINF = VINF_raw * np.ones(N_DOFS)*0               # scalar vinf\n",
    "\n",
    "# Sparse block matrix for the solver (CSR)\n",
    "# A = block_diag([MA, I, I]) with zeros off the block-diagonals\n",
    "I_N = sp.eye(N_DOFS, format='csr')\n",
    "A = sp.block_diag([sp.csr_matrix(MA), I_N, I_N], format='csr')\n",
    "\n",
    "# Component slice layout\n",
    "component_slices = [\n",
    "    slice(0, N_DOFS),\n",
    "    slice(N_DOFS, 2 * N_DOFS),\n",
    "    slice(2 * N_DOFS, 3 * N_DOFS)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bc9a0d",
   "metadata": {},
   "source": [
    "print(f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e7b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from numba import njit\n",
    "\n",
    "KS_dense = np.ascontiguousarray(KS, dtype=np.float64)\n",
    "ES_dense = np.ascontiguousarray(ES, dtype=np.float64)\n",
    "VINF_vec = np.ascontiguousarray(VINF, dtype=np.float64)\n",
    "\n",
    "_rhs_buffer = np.empty(3 * N_DOFS, dtype=np.float64)\n",
    "_rhs_jac_buffer = np.zeros((3 * N_DOFS, 3 * N_DOFS), dtype=np.float64)\n",
    "\n",
    "@njit(cache=True)\n",
    "def _rhs_kernel(y, KS, ES, VINF, out):\n",
    "    n = VINF.shape[0]\n",
    "    for i in range(n):\n",
    "        sum_ku = 0.0\n",
    "        sum_ev = 0.0\n",
    "        yi = y[i]\n",
    "        for j in range(n):\n",
    "            sum_ku += KS[i, j] * y[n + j]\n",
    "            sum_ev += ES[i, j] * (y[j] - VINF[j])\n",
    "        out[i] = -(sum_ku + sum_ev)\n",
    "        out[n + i] = yi - VINF[i]\n",
    "        out[2 * n + i] = yi if yi >= 0.0 else -yi\n",
    "\n",
    "def rhs(t, y):\n",
    "    _rhs_kernel(y, KS_dense, ES_dense, VINF_vec, _rhs_buffer)\n",
    "    return _rhs_buffer.copy()\n",
    "\n",
    "def rhs_jac(t, y, Fk_val=None):\n",
    "    n = N_DOFS\n",
    "    jac = _rhs_jac_buffer\n",
    "    jac.fill(0.0)\n",
    "    jac[:n, :n] = -ES_dense\n",
    "    jac[:n, n:2 * n] = -KS_dense\n",
    "    rows_primary = n + np.arange(n)\n",
    "    jac[rows_primary, np.arange(n)] = 1.0\n",
    "    slip_signs = np.sign(y[:n]).astype(np.float64)\n",
    "    jac[2 * n + np.arange(n), np.arange(n)] = slip_signs\n",
    "    return jac.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf4959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DMU = -0.1   # Friction drop amount (adjust based on your problem)\n",
    "DC = 100.0 / fault.Dscale    # Characteristic slip distance (adjust based on your problem)\n",
    "MU_RES = 0.5\n",
    "\n",
    "SIGMA_N_vec = np.ascontiguousarray(SIGMA_N, dtype=np.float64)\n",
    "\n",
    "_con_force_buffer = np.zeros(3 * N_DOFS, dtype=np.float64)\n",
    "_con_force_jac_buffer = np.zeros((3 * N_DOFS, 3 * N_DOFS), dtype=np.float64)\n",
    "\n",
    "@njit(cache=True)\n",
    "def _con_force_kernel(state, sigma_n, mu_res, dmu, dc, out):\n",
    "    n = sigma_n.shape[0]\n",
    "    for i in range(n):\n",
    "        slip_i = state[2 * n + i]\n",
    "        mu_val = mu_res * (1.0 - (dmu / mu_res) * math.exp(-slip_i / dc))\n",
    "        out[i] = mu_val * sigma_n[i]\n",
    "\n",
    "@njit(cache=True)\n",
    "def _con_force_jac_kernel(state, sigma_n, dmu, dc, out):\n",
    "    n = sigma_n.shape[0]\n",
    "    for i in range(n):\n",
    "        slip_i = state[2 * n + i]\n",
    "        dmu_dslip = (dmu / dc) * math.exp(-slip_i / dc)\n",
    "        out[i, 2 * n + i] = sigma_n[i] * dmu_dslip\n",
    "\n",
    "def con_force(state, fk=None):\n",
    "    _con_force_buffer.fill(0.0)\n",
    "    _con_force_kernel(state, SIGMA_N_vec, MU_RES, DMU, DC, _con_force_buffer)\n",
    "    return _con_force_buffer.copy()\n",
    "\n",
    "\n",
    "def con_force_jacobian(state, t=None, Fk_val=None):\n",
    "    _con_force_jac_buffer.fill(0.0)\n",
    "    _con_force_jac_kernel(state, SIGMA_N_vec, DMU, DC, _con_force_jac_buffer)\n",
    "    return _con_force_jac_buffer.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e960b7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# projection and solver options\n",
    "projection_opts = {\n",
    "    'con_force_func': con_force,\n",
    "    'rhok': np.ones(N_DOFS, dtype=float),\n",
    "    'component_slices': component_slices,\n",
    "    'constraint_indices': np.arange(N_DOFS, dtype=np.int32),\n",
    "    # 'jac_func': con_force_jacobian,  # analytical Jacobian for CoulombProjection\n",
    "\n",
    "}\n",
    "\n",
    "# Tuned nonlinear-solver options shared by VI and SSN\n",
    "solver_opts_common = dict(\n",
    "    tol=1e-6,\n",
    "    max_iter=100,\n",
    "    rhs_jac=rhs_jac,      # enables exact residual Jacobian in integrators\n",
    "    # linear_solver='dense',\n",
    "    # use_broyden=True,\n",
    "    # adaptive_lam = False\n",
    " )\n",
    "\n",
    "# Adaptive controller tuned for nonsmooth dynamics\n",
    "adaptive_opts = dict(\n",
    "    h0=5e-2,\n",
    "    h_min=1e-7,\n",
    "    h_down=0.6,\n",
    "    h_up=1.8,\n",
    "    method_order=1,           # conservative order for nonsmooth dynamics\n",
    "    skip_error_indices=[],\n",
    "    controller='h211b',              # smoother steps with PI control\n",
    "    b_param=4.0,         # only if controller == \"H211b\"\n",
    "    mode = 'ratio'\n",
    "\n",
    " )\n",
    "\n",
    "# SSN-specific tweaks for robustness and speed\n",
    "solver_opts_ssn = dict(solver_opts_common)\n",
    "solver_opts_ssn.update({\n",
    "    \"vi_strict_block_lipschitz\": False,   # was True by default\n",
    "    \"vi_max_block_adjust_iters\": 10,       # smaller safety cap\n",
    "    \"globalization\": 'line_search',    # more robust globalization\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef55cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = np.zeros(3 * N_DOFS)\n",
    "friction_force = con_force(y0)\n",
    "uc = -np.linalg.solve(KS,friction_force[:N_DOFS]) #displacement at frictional instability\n",
    "\n",
    "u0 = uc * (1 + 1e-5) # peturb uc displacement slighty\n",
    "\n",
    "y0[ N_DOFS : 2 * N_DOFS] = u0\n",
    "Uint =  0.5 * np.dot(u0, KS @ u0) #initial internal energy\n",
    "Uintc =  0.5 * np.dot(uc, KS @ uc) # internal energy at frictional instability used to normalize energy\n",
    "\n",
    "\n",
    "# print(f\"Initial slip values (s0): {u0}\")\n",
    "# print(f\"Initial state (y0): {y0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b483ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Full semismooth Newton solves: pure Python vs Numba, plus BDF reference ===\n",
    "# We run the full integrator twice, once forcing pure Python (use_numba=False)\n",
    "# and once forcing Numba acceleration (use_numba=True). Then we also solve\n",
    "# the regularized smooth problem with SciPy's BDF for comparison.\n",
    "tmax=  30 * fault.second / fault.Tscale\n",
    "\n",
    "t_span = (0.0, tmax)\n",
    "# # Make a copy of base solver opts for semismooth Newton\n",
    "# solver_opts_ssn = solver_opts.copy()\n",
    "# solver_opts_ssn['max_iter'] = 100\n",
    "\n",
    "\n",
    "projection_opts_nb = dict(projection_opts)\n",
    "projection_opts_nb['use_numba'] = True   # force Numba path\n",
    "\n",
    "\n",
    "\n",
    "# --- Semismooth Newton (Numba) ---\n",
    "start_nb = time.time()\n",
    "(t_vals_ssn_nb,\n",
    " y_vals_ssn_nb,\n",
    " h_vals_ssn_nb,\n",
    " fk_vals_ssn_nb,\n",
    " solver_info_ssn_nb) = solve_ivp_ns(\n",
    "    fun=rhs,\n",
    "    t_span=t_span,\n",
    "    y0=y0,\n",
    "    method='composite',\n",
    "    projection='coulomb',\n",
    "    solver='VI',\n",
    "    projection_opts=projection_opts_nb,\n",
    "    solver_opts=solver_opts_ssn,\n",
    "    adaptive=True,\n",
    "    adaptive_opts=adaptive_opts,\n",
    "    # atol=1e-8,\n",
    "    # rtol=1e-6,\n",
    "    h0=30/4 * fault.second / fault.Tscale,\n",
    "    component_slices=component_slices,\n",
    "    verbose=True,\n",
    "    A=A\n",
    ")\n",
    "rt_nb = time.time() - start_nb\n",
    "print(f\"Semismooth (Numba)  solve complete in {rt_nb:.3f} s\")\n",
    "\n",
    "# Keep backward-compatible variable nams for downstream post-processing.\n",
    "# We choose the Numba result as the primary (you can swap to *_py if you prefer).\n",
    "t_vals_ssn = t_vals_ssn_nb\n",
    "y_vals_ssn = y_vals_ssn_nb\n",
    "h_vals_ssn = h_vals_ssn_nb\n",
    "y_fk_vals_placeholder = fk_vals_ssn_nb  # not used directly, keep for clarity\n",
    "fk_vals_ssn = fk_vals_ssn_nb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5a9812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(t_vals_ssn * fault.Tscale / fault.second, np.mean(y_vals_ssn[:,:N_DOFS],axis=1) * fault.Vscale, label='Velocity (ssn)')\n",
    "# plt.xlabel('Time (s)')\n",
    "# plt.ylabel('Velocity (m/s)')\n",
    "# plt.title('Velocity vs Time (SSN)')\n",
    "# # plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(t_vals_ssn * fault.Tscale / fault.second,y_vals_ssn[:,:N_DOFS] * fault.Vscale, label='Velocity (ssn)')\n",
    "# plt.xlabel('Timemarkert# .ylabel('Velocity (m/s)')\n",
    "plt# .title('Velocity vs Time (SSN)')\n",
    "# p# lt.legend()\n",
    "plt# .grid()\n",
    "plt# .show()\n",
    "t_scaled = t_vals_ssn * fault.Tscale / fault.second\n",
    "mean_vel = np.mean(y_vals_ssn[:, :N_DOFS], axis=1) * fault.Vscale\n",
    "\n",
    "plt.plot(t_scaled, mean_vel, linestyle='-', color='C0', label='Velocity (ssn)')\n",
    "plt.scatter(t_scaled, mean_vel, color='C0', s=25)  # one marker per time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abf08ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_vals_ssn[-1] * fault.Tscale / fault.second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7242108e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941babec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = estimate_v_max_range(y0, KS, ES, SIGMA_N, MU_RES, DMU, DC, VINF)\n",
    "# v_max = results['kb_estimate']\n",
    "# print(v_max)\n",
    "# # print(0.07/fault.Vscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568833e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# # alpha_reference = 2.0  # legacy default retained for reference\n",
    "# results = estimate_v_max_range(y0, KS, ES, SIGMA_N, MU_RES, DMU, DC, VINF)\n",
    "# v_max = results['kb_estimate']\n",
    "# # print(v_max)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def build_reward_fn(alpha_value: float):\n",
    "    \"\"\"\n",
    "    Build a reward function parameterized by alpha_value.\n",
    "\n",
    "    alpha_value controls how harshly we punish numerical error via A = exp(-alpha*E).\n",
    "    Larger alpha_value => more accuracy-obsessed agent.\n",
    "    \"\"\"\n",
    "\n",
    "    alpha_value = float(alpha_value)\n",
    "\n",
    "    def my_reward_fn(solver_perf, dt_attempt, xk, env):\n",
    "        # solver_perf layout from env.increment_env:\n",
    "        # [ runtime_inc,\n",
    "        #   dts,\n",
    "        #   error_LO,\n",
    "        #   error_lil1,\n",
    "        #   error_HI,\n",
    "        #   E,                # global RMS scaled error\n",
    "        #   success_LO,\n",
    "        #   success_lil1,\n",
    "        #   success_HI,\n",
    "        #   kiter_LO,\n",
    "        #   iter_lil1,\n",
    "        #   kiter_HI ]\n",
    "        (\n",
    "            runtime_inc,\n",
    "            dts,\n",
    "            error_LO,\n",
    "            error_lil1,\n",
    "            error_HI,\n",
    "            E,\n",
    "            success_LO,\n",
    "            success_lil1,\n",
    "            success_HI,\n",
    "            kiter_LO,\n",
    "            iter_lil1,\n",
    "            kiter_HI,\n",
    "        ) = solver_perf\n",
    "\n",
    "        # We'll need this a few times\n",
    "        dt_range = env.dt_max - env.dt_min\n",
    "\n",
    "        # If the integrator did not advance time (reject / failure path):\n",
    "        # dts will be 0.0, success flags likely False.\n",
    "        if dts == 0.0:\n",
    "            # Penalize requesting a too-large step that got rejected.\n",
    "            # Normalize dt_attempt to [0,1] and assign negative reward.\n",
    "            dt_norm_attempt = (dt_attempt - env.dt_min) / dt_range\n",
    "            # Clip to be safe\n",
    "            # dt_norm_attempt = float(np.clip(dt_norm_attempt, 0.0, 1.0))\n",
    "            # print(f\" Step rejected. dt_attempt: {dt_attempt}, dt_norm_attempt: {dt_norm_attempt}\")\n",
    "            return -dt_norm_attempt  # more aggressive ask that failed => more negative\n",
    "\n",
    "        # Otherwise: this step was accepted and advanced time by dts.\n",
    "\n",
    "        # -----------------\n",
    "        # S1: step size score\n",
    "        # -----------------\n",
    "        # We reward taking a large *accepted* step.\n",
    "        dt_norm = (dts - env.dt_min) / dt_range\n",
    "        S1 = float(np.clip(dt_norm, 0.0, 1.0))\n",
    "\n",
    "        # -----------------\n",
    "        # S2: runtime score\n",
    "        # -----------------\n",
    "        # Track per-step runtime and update the running maximum to normalize S2 dynamically\n",
    "        if not hasattr(env, 'runtime_history'):\n",
    "            env.runtime_history = []\n",
    "        env.runtime_history.append(float(runtime_inc))\n",
    "\n",
    "        # Maintain a running max runtime seen so far\n",
    "        if not hasattr(env, 'max_runtime_seen'):\n",
    "            env.max_runtime_seen = float(runtime_inc)\n",
    "        else:\n",
    "            if runtime_inc > env.max_runtime_seen:\n",
    "                env.max_runtime_seen = float(runtime_inc)\n",
    "        # track min as well as max\n",
    "        if not hasattr(env, 'min_runtime_seen'):\n",
    "            env.min_runtime_seen = float(runtime_inc)\n",
    "        else:\n",
    "            if runtime_inc < env.min_runtime_seen:\n",
    "                env.min_runtime_seen = float(runtime_inc)\n",
    "        # Compute S2 using the dynamic max; clamp denominator to avoid division by zero\n",
    "        rt_min =  float(env.min_runtime_seen)\n",
    "        rt_max = max(rt_min, float(env.max_runtime_seen))\n",
    "        denom = max(1e-8, rt_max - rt_min)\n",
    "        S2 = 1 - (float(runtime_inc) - rt_min) / denom\n",
    "\n",
    "        A = float(np.exp(-alpha_value * E))\n",
    "        # print(f\" S1: {S1}, S2: {S2},  A: {A}\")\n",
    "        # Final reward\n",
    "        reward = S1 * S2 * A\n",
    "        return reward\n",
    "\n",
    "    return my_reward_fn\n",
    "\n",
    "\n",
    "\n",
    "def build_obs_fn(alpha_value: float):\n",
    "    \"\"\"\n",
    "    Factory that returns an observation function using the SAME alpha as the reward.\n",
    "    This keeps reward shaping and observation aligned.\n",
    "\n",
    "    The returned function will be called as:\n",
    "        obs_fn(dt_attempt, converged, xk, solver_perf, fk, env)\n",
    "\n",
    "    Where:\n",
    "      - dt_attempt: the candidate step the agent just asked for (float or None at reset)\n",
    "      - converged: 1 if the accepted high-accuracy solve succeeded, else 0 (None at reset)\n",
    "      - xk: current state vector after the env step (or initial state at reset)\n",
    "      - solver_perf: list of solver diagnostics from env.increment_env(...) (None at reset)\n",
    "      - fk: current residual (not used here, but passed in by env)\n",
    "      - env: the AdaptiveStepperEnv instance\n",
    "    \"\"\"\n",
    "    alpha_value = float(alpha_value)\n",
    "\n",
    "    def my_obs_fn(dt_attempt, converged, xk, solver_perf, fk=None, env=None):\n",
    "        assert env is not None, \"env must be provided by AdaptiveStepperEnv\"\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 1. Extract physics state features from xk\n",
    "        # ------------------------------------------------------------------\n",
    "        n = N_DOFS  # assuming N_DOFS is defined in outer scope\n",
    "        v = xk[:n]          # velocities / slip rates\n",
    "        u = xk[n:2*n]       # displacements / slips\n",
    "\n",
    "        avg_v = float(np.mean(v))\n",
    "\n",
    "        # Internal elastic energy 0.5 * u^T K u\n",
    "        ks_u = KS @ u       # KS must be defined in outer scope\n",
    "        E_int = 0.5 * float(np.dot(u, ks_u))\n",
    "\n",
    "        # Normalize internal energy by Uintc (critical energy), if available and >0\n",
    "        try:\n",
    "            if Uintc > 0.0:\n",
    "                E_norm = float(E_int / Uintc)\n",
    "            else:\n",
    "                E_norm = float(E_int)\n",
    "        except NameError:\n",
    "            # Fallback if Uintc isn't in scope\n",
    "            E_norm = float(E_int)\n",
    "\n",
    "        if solver_perf is not None:\n",
    "            dts = solver_perf[1]\n",
    "            conv_flag_bipolar = 1.0 if dts > 0.0 else -1.0\n",
    "        else:\n",
    "            conv_flag_bipolar = -1.0\n",
    "        # ------------------------------------------------------------------\n",
    "        # 2. Reset branch (env.reset calls us with dt_attempt=None, solver_perf=None)\n",
    "        # ------------------------------------------------------------------\n",
    "        if dt_attempt is None or solver_perf is None:\n",
    "            # We still return a valid 6-D observation vector.\n",
    "            # Fill in placeholders for solver-derived quantities.\n",
    "            dt_norm_default = 0.5  # neutral midpoint in [0,1]\n",
    "            conv_flag = 0.0 if converged is None else float(converged)\n",
    "\n",
    "            return np.array([\n",
    "                E_norm,                # elastic energy normalized\n",
    "                avg_v / (0.07/fault.Vscale),         # avg slip rate normalized (v_max provided externally)\n",
    "                (env.iter_error / 4.0) if hasattr(env, 'iter_error') else 0.0,\n",
    "                conv_flag,             # \"converged\" flag at reset (0 or None -> 0)\n",
    "                0.0,                   # exp(-alpha * 0) = 1.0 as a neutral accuracy proxy\n",
    "                conv_flag_bipolar * dt_norm_default,       # normalized dt ~ mid\n",
    "            ], dtype=np.float64)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 3. Normal step branch: unpack solver_perf\n",
    "        # ------------------------------------------------------------------\n",
    "        (\n",
    "            runtime_inc,\n",
    "            dts,\n",
    "            error_LO,\n",
    "            error_lil1,\n",
    "            error_HI,\n",
    "            E_global,        # global RMS scaled error (Richardson)\n",
    "            success_LO,\n",
    "            success_lil1,\n",
    "            success_HI,\n",
    "            kiter_LO,\n",
    "            iter_lil1,\n",
    "            kiter_HI,\n",
    "        ) = solver_perf\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 4. Normalize dt_attempt into [0,1]\n",
    "        # ------------------------------------------------------------------\n",
    "        dt_range = env.dt_max - env.dt_min\n",
    "        if dt_range <= 0.0:\n",
    "            dt_norm = 0.0\n",
    "        else:\n",
    "            dt_norm = (dt_attempt - env.dt_min) / dt_range\n",
    "            dt_norm = float(np.clip(dt_norm, 0.0, 1.0))\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 5. Accuracy proxy, aligned with reward shaping\n",
    "        # ------------------------------------------------------------------\n",
    "        # 6. Convergence flag\n",
    "        # ------------------------------------------------------------------\n",
    "        # env.step sets converged = 1 if dts > 0 else 0 before calling obs_fn.\n",
    "        conv_flag = float(converged) if converged is not None else 0.0\n",
    "        if conv_flag:\n",
    "            acc = 1/(1 + E_global)\n",
    "        else:\n",
    "            # print(f\"Not converged step detected in obs_fn.\")\n",
    "            acc = 0.0\n",
    "\n",
    "        # print(f\"E_norm: {E_norm}, avg_v/v_max: {avg_v / v_max}, iter_error: {(env.iter_error / 4.0) if hasattr(env, 'iter_error') else 0.0}, conv_flag: {conv_flag}, acc: {acc}, dt_norm: {dt_norm}\")\n",
    "        # ------------------------------------------------------------------\n",
    "        # 7. Assemble observation vector\n",
    "        # ------------------------------------------------------------------\n",
    "        print(f\"observed step {conv_flag_bipolar*dt_norm}\")\n",
    "        return np.array([\n",
    "            E_norm,                        # normalized elastic energy\n",
    "            avg_v / (0.07/fault.Vscale),                 # normalized average slip rate\n",
    "            (env.iter_error / 4.0) if hasattr(env, 'iter_error') else 0.0,\n",
    "            conv_flag,                     # 1 if high-accuracy solve accepted, else 0\n",
    "            acc,                   # exp(-alpha * error), same shaping as reward\n",
    "            conv_flag_bipolar *dt_norm,                       # normalized proposed dt in [0,1]\n",
    "        ], dtype=np.float64)\n",
    "\n",
    "    return my_obs_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class RewardCallback(BaseCallback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        Vscale=1.0,\n",
    "        Dscale=1.0,\n",
    "        Tscale=1.0,\n",
    "        update_freq=50,\n",
    "        offset_increment=10.0,\n",
    "        num_envs=1,\n",
    "        verbose=0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Live training monitor for SB3 in Jupyter.\n",
    "\n",
    "        Panels per env:\n",
    "          (0) step-by-step reward trace, with green vertical lines at episode ends\n",
    "          (1) per-episode return\n",
    "          (2) avg_v (green, left y) and avg_u (blue, right y) vs physical time t_k1,\n",
    "              zoomed to last ~2-3 episodes\n",
    "          (3) steps_per_episode (here we record Sim_time at episode end)\n",
    "\n",
    "        Vscale, Dscale, Tscale are just multiplicative scalars in case your units\n",
    "        are nondimensional internally and you want to rescale for plotting.\n",
    "        \"\"\"\n",
    "        super().__init__(verbose)\n",
    "        self.update_freq = int(update_freq)\n",
    "        self.offset_increment = float(offset_increment)\n",
    "\n",
    "        self.Vscale = float(Vscale)  # scales avg_v\n",
    "        self.Dscale = float(Dscale)  # scales avg_u\n",
    "        self.Tscale = float(Tscale)  # not currently used in plot, reserved\n",
    "\n",
    "        # will get overwritten in _init_callback\n",
    "        self.num_envs = int(num_envs)\n",
    "\n",
    "        # buffers (will be resized in _init_callback)\n",
    "        self.rewards = None\n",
    "        self.episode_rewards = None\n",
    "        self.current_episode_rewards = None\n",
    "\n",
    "        self.avg_v_values = None\n",
    "        self.avg_u_values = None\n",
    "        self.t_k1_values = None\n",
    "\n",
    "        self.steps = None\n",
    "        self.episodes = None\n",
    "        self.episode_end_steps = None\n",
    "        self.episode_end_indices = None\n",
    "        self.current_offsets = None\n",
    "        self.steps_per_episode = None\n",
    "\n",
    "        self.fig = None\n",
    "        self.axs = None     # shape [num_envs][4]\n",
    "        self.axs2 = None    # list of twin y-axes, len = num_envs\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        \"\"\"Called by SB3 at the beginning of .learn().\"\"\"\n",
    "        # detect actual number of envs from VecEnv\n",
    "        self.num_envs = getattr(self.training_env, \"num_envs\", 1)\n",
    "\n",
    "        # allocate buffers for each env\n",
    "        self.rewards = [[] for _ in range(self.num_envs)]               # step-wise reward\n",
    "        self.episode_rewards = [[] for _ in range(self.num_envs)]       # per-episode return\n",
    "        self.current_episode_rewards = [0.0] * self.num_envs            # running sum this ep\n",
    "\n",
    "        self.avg_v_values = [[] for _ in range(self.num_envs)]          # avg_v trace\n",
    "        self.avg_u_values = [[] for _ in range(self.num_envs)]          # avg_u trace\n",
    "        self.t_k1_values  = [[] for _ in range(self.num_envs)]          # physical time trace (offset per ep)\n",
    "\n",
    "        self.steps = [0] * self.num_envs\n",
    "        self.episodes = [0] * self.num_envs\n",
    "\n",
    "        self.episode_end_steps   = [[] for _ in range(self.num_envs)]   # x-locations of ep ends in panel 0\n",
    "        self.episode_end_indices = [[] for _ in range(self.num_envs)]   # indices for trimming panel 2\n",
    "        self.current_offsets     = [0.0] * self.num_envs                # time offset added after each ep\n",
    "        self.steps_per_episode   = [[] for _ in range(self.num_envs)]   # store Sim_time snapshot at ep end\n",
    "\n",
    "        # set up figure\n",
    "        self.fig, self.axs = plt.subplots(self.num_envs, 4, figsize=(25, 5 * self.num_envs))\n",
    "        if self.num_envs == 1:\n",
    "            # normalize shape so we can always index axs[i][col]\n",
    "            self.axs = [self.axs]\n",
    "\n",
    "        # make twin y-axes for avg_u in panel (2)\n",
    "        self.axs2 = []\n",
    "        for i in range(self.num_envs):\n",
    "            # panel 0: reward vs steps\n",
    "            self.axs[i][0].set_xlabel(\"Steps\")\n",
    "            self.axs[i][0].set_ylabel(\"Reward\")\n",
    "            self.axs[i][0].axhline(y=0, color=\"r\", linestyle=\"--\")\n",
    "\n",
    "            # panel 1: per-episode return\n",
    "            self.axs[i][1].set_xlabel(\"Episodes\")\n",
    "            self.axs[i][1].set_ylabel(\"Total Reward\")\n",
    "\n",
    "            # panel 2: avg_v / avg_u vs t_k1\n",
    "            self.axs[i][2].set_xlabel(\"t_k1\")\n",
    "            self.axs[i][2].set_ylabel(\"avg_v\", color=\"g\")\n",
    "            self.axs[i][2].tick_params(axis=\"y\", labelcolor=\"g\")\n",
    "            self.axs[i][2].grid(True)\n",
    "\n",
    "            # panel 3: steps_per_episode\n",
    "            self.axs[i][3].set_xlabel(\"Episodes\")\n",
    "            self.axs[i][3].set_ylabel(\"Steps per Episode / Sim_time\")\n",
    "\n",
    "            ax2 = self.axs[i][2].twinx()\n",
    "            ax2.set_ylabel(\"avg_u\", color=\"b\")\n",
    "            ax2.tick_params(axis=\"y\", labelcolor=\"b\")\n",
    "            self.axs2.append(ax2)\n",
    "\n",
    "        display(self.fig)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        Called by SB3 every environment step (actually every rollout collection step).\n",
    "\n",
    "        We read self.locals[\"rewards\"], self.locals[\"dones\"], self.locals[\"infos\"].\n",
    "        With VecEnv:\n",
    "            rewards -> np.array shape (n_envs,)\n",
    "            dones   -> np.array shape (n_envs,)\n",
    "            infos   -> list[dict] length n_envs\n",
    "        With single env:\n",
    "            they can be scalars/dicts; we wrap them to lists.\n",
    "        \"\"\"\n",
    "        rewards = self.locals[\"rewards\"]\n",
    "        dones   = self.locals[\"dones\"]\n",
    "        infos   = self.locals[\"infos\"]\n",
    "\n",
    "        # unify shape for num_envs==1\n",
    "        if self.num_envs == 1 and not isinstance(rewards, (list, tuple, np.ndarray)):\n",
    "            rewards = [rewards]\n",
    "            dones   = [dones]\n",
    "            infos   = [infos]\n",
    "\n",
    "        for i in range(self.num_envs):\n",
    "            r_i = float(rewards[i])\n",
    "            self.rewards[i].append(r_i)\n",
    "            self.current_episode_rewards[i] += r_i\n",
    "            self.steps[i] += 1\n",
    "\n",
    "            info_i = infos[i]\n",
    "\n",
    "\n",
    "            xk = info_i.get(\"xk\", None)\n",
    "            if xk is not None:\n",
    "                n = N_DOFS\n",
    "                v = xk[:n]* self.Vscale\n",
    "                u = xk[n:2*n]* self.Dscale\n",
    "            #     mean_vel.append(float(np.mean(v)))\n",
    "            # mean_slip.append(float(np.mean(u)))\n",
    "\n",
    "            # # pull diagnostics from env.info\n",
    "            # # we scaled in env.step(), now we optionally rescale for plotting\n",
    "            # avg_v_i  = float(info_i.get(\"avg_v\", 0.0)) * self.Vscale\n",
    "            # avg_u_i  = float(info_i.get(\"avg_u\", 0.0)) * self.Dscale\n",
    "            t_k1_i   = float(info_i.get(\"t_k1\", 0.0))\n",
    "            sim_time = float(info_i.get(\"Sim_time\", 0.0))\n",
    "\n",
    "            # append traces\n",
    "            self.avg_v_values[i].append(float(np.mean(v)))\n",
    "            self.avg_u_values[i].append(float(np.mean(u)))\n",
    "            # add offset to make t_k1 increase monotonically across episodes\n",
    "            self.t_k1_values[i].append(t_k1_i + self.current_offsets[i])\n",
    "\n",
    "            # handle episode boundary\n",
    "            if dones[i]:\n",
    "                # store episode return\n",
    "                self.episode_rewards[i].append(self.current_episode_rewards[i])\n",
    "                # mark step index where this episode ended\n",
    "                self.episode_end_steps[i].append(len(self.rewards[i]))\n",
    "                # mark index in avg_v_values so we can \"zoom to last 2-3 episodes\"\n",
    "                self.episode_end_indices[i].append(len(self.avg_v_values[i]))\n",
    "\n",
    "                # snapshot \"steps per episode\" panel: we're using sim_time here\n",
    "                self.steps_per_episode[i].append(sim_time)\n",
    "\n",
    "                # reset accumulators for next ep\n",
    "                self.current_episode_rewards[i] = 0.0\n",
    "                self.episodes[i] += 1\n",
    "                self.steps[i] = 0\n",
    "\n",
    "                # bump offset so next episode's t_k1 timeline is shifted forward\n",
    "                if self.t_k1_values[i]:\n",
    "                    last_t = self.t_k1_values[i][-1]\n",
    "                    self.current_offsets[i] = last_t + self.offset_increment\n",
    "\n",
    "        # redraw occasionally\n",
    "        total_steps_all_envs = sum(self.steps)\n",
    "        if (total_steps_all_envs % self.update_freq) == 0:\n",
    "            self._update_plot()\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _update_plot(self):\n",
    "        \"\"\"Refresh the 4-panel figure for each env.\"\"\"\n",
    "        for i in range(self.num_envs):\n",
    "            # clear each panel\n",
    "            self.axs[i][0].cla()\n",
    "            self.axs[i][1].cla()\n",
    "            self.axs[i][2].cla()\n",
    "            self.axs2[i].cla()\n",
    "            self.axs[i][3].cla()\n",
    "\n",
    "            # (0) reward per step, with episode boundaries\n",
    "            self.axs[i][0].plot(self.rewards[i], \"b-\")\n",
    "            self.axs[i][0].set_xlabel(\"Steps\")\n",
    "            self.axs[i][0].set_ylabel(\"Reward\")\n",
    "            self.axs[i][0].axhline(y=0, color=\"r\", linestyle=\"--\")\n",
    "            for step_idx in self.episode_end_steps[i]:\n",
    "                self.axs[i][0].axvline(x=step_idx, color=\"g\", linestyle=\"--\")\n",
    "\n",
    "            # (1) episode return\n",
    "            if len(self.episode_rewards[i]) > 1:\n",
    "                self.axs[i][1].plot(\n",
    "                    self.episode_rewards[i][1:],\n",
    "                    color=\"r\",\n",
    "                    marker=\"o\",\n",
    "                    linestyle=\"-\",\n",
    "                )\n",
    "            self.axs[i][1].set_xlabel(\"Episodes\")\n",
    "            self.axs[i][1].set_ylabel(\"Total Reward\")\n",
    "\n",
    "            # choose window for panel (2): last ~2-3 episodes\n",
    "            if len(self.episode_end_indices[i]) >= 3:\n",
    "                start_index = self.episode_end_indices[i][-3]\n",
    "            elif len(self.episode_end_indices[i]) == 2:\n",
    "                start_index = self.episode_end_indices[i][-2]\n",
    "            else:\n",
    "                start_index = 0\n",
    "            end_index = len(self.avg_v_values[i])\n",
    "\n",
    "            # (2) avg_v vs physical time\n",
    "            self.axs[i][2].plot(\n",
    "                self.t_k1_values[i][start_index:end_index],\n",
    "                self.avg_v_values[i][start_index:end_index],\n",
    "                color=\"g\",\n",
    "                marker=\"o\",\n",
    "                linestyle=\"-\",\n",
    "                label=\"avg_v\",\n",
    "            )\n",
    "            self.axs[i][2].set_xlabel(\"t_k1\")\n",
    "            self.axs[i][2].set_ylabel(\"avg_v\", color=\"g\")\n",
    "            self.axs[i][2].tick_params(axis=\"y\", labelcolor=\"g\")\n",
    "            self.axs[i][2].grid(True)\n",
    "\n",
    "            # (2 twin) avg_u vs physical time\n",
    "            self.axs2[i].plot(\n",
    "                self.t_k1_values[i][start_index:end_index],\n",
    "                self.avg_u_values[i][start_index:end_index],\n",
    "                color=\"b\",\n",
    "                marker=\"x\",\n",
    "                linestyle=\"--\",\n",
    "                label=\"avg_u\",\n",
    "            )\n",
    "            self.axs2[i].set_ylabel(\"avg_u\", color=\"b\")\n",
    "            self.axs2[i].tick_params(axis=\"y\", labelcolor=\"b\")\n",
    "\n",
    "            # combined legend\n",
    "            lines_1, labels_1 = self.axs[i][2].get_legend_handles_labels()\n",
    "            lines_2, labels_2 = self.axs2[i].get_legend_handles_labels()\n",
    "            self.axs[i][2].legend(\n",
    "                lines_1 + lines_2,\n",
    "                labels_1 + labels_2,\n",
    "                loc=\"upper left\",\n",
    "            )\n",
    "\n",
    "            # (3) steps_per_episode (we're storing Sim_time snapshots)\n",
    "            if len(self.steps_per_episode[i]) > 1:\n",
    "                self.axs[i][3].plot(self.steps_per_episode[i][1:], \"b-\")\n",
    "            self.axs[i][3].set_xlabel(\"Episodes\")\n",
    "            self.axs[i][3].set_ylabel(\"Steps per Episode / Sim_time\")\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        display(self.fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5620a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from solve_nivp.projections import CoulombProjection\n",
    "from solve_nivp.nonlinear_solvers import ImplicitEquationSolver\n",
    "from solve_nivp.integrations import CompositeMethod, Trapezoidal\n",
    "from solve_nivp.ODESystem import ODESystem\n",
    "from solve_nivp.ODESolver import ODESolver\n",
    "from solve_nivp.rl.callbacks import CustomMetricsCallback\n",
    "from gymnasium import spaces\n",
    "from pathlib import Path\n",
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "\n",
    "# Component slices (each DOF separately) and constraint indices (only DOF 0 is constrained)\n",
    "# Component slice layout\n",
    "component_slices = [\n",
    "    slice(0, N_DOFS),\n",
    "    slice(N_DOFS, 2 * N_DOFS),\n",
    "    slice(2 * N_DOFS, 3 * N_DOFS)\n",
    "]\n",
    "component_slices_solver = [slice(0, 3 * N_DOFS)]  # solver works on full state vector\n",
    "# For constrained DOF 0 we set rhok=1.0; unconstrained DOF 1 gets 0.\n",
    "rhok = np.ones(N_DOFS, dtype=float)\n",
    "\n",
    "# Create the projection operator\n",
    "projection = CoulombProjection(\n",
    "    con_force_func=con_force,\n",
    "    rhok=rhok,\n",
    "    component_slices=component_slices,\n",
    "    constraint_indices=np.arange(N_DOFS, dtype=np.int32),\n",
    "    use_numba=True,  # use Numba acceleration\n",
    "    jac_func=con_force_jacobian,  # analytical Jacobian for CoulombProjection\n",
    ")\n",
    "\n",
    "# Configure the implicit solver\n",
    "solver_mp = ImplicitEquationSolver(\n",
    "    method='VI',\n",
    "    proj=projection,\n",
    "    component_slices=component_slices,\n",
    "    tol=solver_opts_ssn.get('tol', 1e-6),\n",
    "    max_iter=solver_opts_ssn.get('max_iter', 50),\n",
    "    vi_strict_block_lipschitz=solver_opts_ssn.get(\"vi_strict_block_lipschitz\", False),\n",
    "    vi_max_block_adjust_iters=solver_opts_ssn.get(\"vi_max_block_adjust_iters\", 10),\n",
    ")\n",
    "solver_mp.rhs_jacobian = rhs_jac  # expose analytical RHS Jacobian to integrators\n",
    "\n",
    "# Use the composite method (which calls the solver internally)\n",
    "method_mp = CompositeMethod(solver=solver_mp, A=A)\n",
    "\n",
    "\n",
    "\n",
    "# --- Adaptive RL training over an alpha sweep ---\n",
    "# Here we iterate over a list of alpha values, train a policy for each,\n",
    "# and persist the resulting models, normalization stats, and metadata\n",
    "# into per-alpha subdirectories for easy organization.\n",
    "dt0 = 1e-3\n",
    "t0 = t_span[0]\n",
    "tnmax = t_span[1]\n",
    "dt_min = 1e-6  # * fault.second / fault.Tscale\n",
    "dt_max = 30 / 5 * fault.second / fault.Tscale\n",
    "nparams = (1e-6, 100)\n",
    "\n",
    "# Define the observation space (here we use the 3 state variables plus 3 extra features regarding solver performance)\n",
    "obs_dim = 6\n",
    "obs_space = spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float64)\n",
    "\n",
    "# alpha_grid = [0.5, 1.0, 2.0, 4.0, 16.0, 32.0, 128.0]  # customize as desired\n",
    "alpha_grid = [8.0]  # customize as desired\n",
    "\n",
    "total_timesteps = 20_000\n",
    "RUN_ROOT = Path(\"rl_runs\")\n",
    "RUN_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _alpha_label(alpha_value: float) -> str:\n",
    "    sign = \"m\" if alpha_value < 0 else \"\"\n",
    "    magnitude = str(abs(alpha_value)).replace(\".\", \"p\")\n",
    "    return f\"{sign}{magnitude}\"\n",
    "\n",
    "_base_policy_kwargs = dict(\n",
    "    activation_fn=th.nn.ReLU,\n",
    "    net_arch=dict(pi=[64, 64], qf=[64, 64, 64], n_critics=5)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac5e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_callback = RewardCallback(num_envs=1,Vscale = fault.Vscale, Dscale = fault.Dscale, Tscale = fault.Tscale/fault.second, update_freq=100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f5431a",
   "metadata": {},
   "source": [
    "## Evaluation: roll out each trained alpha and compare to adaptive integrator\n",
    "\n",
    "This section loads each trained RL policy from `rl_runs/alpha_*`, runs a rollout on the same fault setup, and plots:\n",
    "- Average velocity over DOFs vs. time\n",
    "- Average slip over DOFs vs. time\n",
    "It also adds the adaptive integrator baseline computed earlier and builds a small runtime table.\n",
    "\n",
    "Notes:\n",
    "- No training happens here; we only load saved artifacts.\n",
    "- We assume models were saved by the training cell into `rl_runs/alpha_<label>/model(.zip)` with optional `vec_norm.pkl`.\n",
    "- Observation normalization is the same as training (your `my_obs_fn`) and uses per-discretization scalars already defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f455da07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import json\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sb3_contrib import TQC\n",
    "# from stable_baselines3 import TD3\n",
    "# from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "# import time\n",
    "# try:\n",
    "#     import pandas as pd\n",
    "#     from IPython.display import display\n",
    "#     _HAVE_PANDAS = True\n",
    "# except Exception:\n",
    "#     _HAVE_PANDAS = False\n",
    "\n",
    "# adaptive_opts = dict(\n",
    "#     h0=5e-3,\n",
    "#     h_min=1e-7,\n",
    "#     h_down=0.6,\n",
    "#     h_up=1.8,\n",
    "#     method_order=1,        # p = 1 â†’ nonsmooth first-order scheme, so k = p+1 = 2\n",
    "#     controller=\"H211b\",   # <--- THIS picks the PI digital filter variant to test\n",
    "#     b_param=4.0,         # only if controller == \"H211b\"\n",
    "#     skip_error_indices=[],\n",
    "# )\n",
    "\n",
    "# # Utility: format alpha subfolder names identical to training\n",
    "# def _alpha_label(alpha_value: float) -> str:\n",
    "#     sign = \"m\" if alpha_value < 0 else \"\"\n",
    "#     magnitude = str(abs(alpha_value)).replace(\".\", \"p\")\n",
    "#     return f\"{sign}{magnitude}\"\n",
    "\n",
    "# RUN_ROOT = Path(\"TEST_RL_ALPHA_SWEEP\").resolve()\n",
    "\n",
    "# ALGO_REGISTRY = {\n",
    "#     \"TQC\": TQC,\n",
    "#     # \"TD3\": TD3,\n",
    "# }\n",
    "\n",
    "# # Number of 1 for averaging runtime\n",
    "# N_EVAL_RUNS = 1  # Set to 1 for quick testing, 10 for proper averaging\n",
    "\n",
    "# # Discover trained runs from disk (alpha_* folders)\n",
    "# alpha_dirs = sorted([p for p in RUN_ROOT.glob(\"alpha_*\") if p.is_dir()])\n",
    "# if not alpha_dirs:\n",
    "#     raise FileNotFoundError(f\"No trained runs found under {RUN_ROOT}. Expected folders like alpha_1p0/TQC, alpha_10p0/TD3, ...\")\n",
    "# dt0 = 1e-3\n",
    "\n",
    "# # Helper to rebuild the same environment used for training\n",
    "# def make_eval_env(reward_fn, obs_fn, alpha_value: float):\n",
    "#     return AdaptiveStepperEnv(\n",
    "#         system=rhs,\n",
    "#         dt0=dt0,\n",
    "#         t0=0.0,\n",
    "#         x0=y0,\n",
    "#         tnmax=tnmax,\n",
    "#         dt_min=dt_min,\n",
    "#         dt_max=dt_max,\n",
    "#         nparams=nparams,\n",
    "#         integrator=method_mp,\n",
    "#         component_slices=component_slices,\n",
    "#         reward_fn=reward_fn,  # not used in deterministic rollout, but env expects it\n",
    "#         obs_fn=obs_fn,\n",
    "#         obs_space=obs_space,\n",
    "#         verbose=False,\n",
    "#         # atol=1e-8,\n",
    "#         # rtol=1e-6,\n",
    "#         alpha=alpha_value,   # ensure env.alpha is set (for completeness)\n",
    "# )\n",
    "\n",
    "# def rollout_policy_on_env(model, vec_env, run_name=\"(unnamed)\"):\n",
    "#     \"\"\"\n",
    "#     Rollout policy using the vec_env (which may have VecNormalize wrapper).\n",
    "#     \"\"\"\n",
    "#     obs = vec_env.reset()\n",
    "#     times = []\n",
    "#     mean_vel = []\n",
    "#     mean_slip = []\n",
    "#     start = time.time()\n",
    "    \n",
    "#     # Repeated-time guard\n",
    "#     last_t = None\n",
    "#     repeat_count = 0\n",
    "#     broke_stuck = False\n",
    "#     last_converged = None\n",
    "    \n",
    "#     # Check if we're dealing with a vectorized env or not\n",
    "#     is_vectorized = hasattr(vec_env, 'num_envs')\n",
    "    \n",
    "#     while True:\n",
    "#         action, _ = model.predict(obs, deterministic=True)\n",
    "#         obs, reward, done, info = vec_env.step(action)\n",
    "        \n",
    "#         # Handle both vectorized and non-vectorized environments\n",
    "#         if is_vectorized:\n",
    "#             # Extract from vec_env wrapper (arrays/lists)\n",
    "#             done_val = done[0] if isinstance(done, (list, np.ndarray)) else done\n",
    "#             info_dict = info[0] if isinstance(info, list) else info\n",
    "#         else:\n",
    "#             done_val = done\n",
    "#             info_dict = info\n",
    "        \n",
    "#         if done_val:\n",
    "#             break\n",
    "            \n",
    "#         t_now = info_dict.get(\"t_k1\", np.nan)\n",
    "        \n",
    "#         # Track last converged flag if provided by env\n",
    "#         if isinstance(info_dict, dict) and (\"converged\" in info_dict):\n",
    "#             last_converged = info_dict.get(\"converged\")\n",
    "        \n",
    "#         # Check for repeated time values to avoid infinite loops\n",
    "#         current_t = None if t_now is None or np.isnan(t_now) else float(t_now)\n",
    "#         if current_t is not None:\n",
    "#             if last_t is not None and np.isclose(current_t, last_t, rtol=0.0, atol=1e-12):\n",
    "#                 repeat_count += 1\n",
    "#                 if repeat_count >= 20:\n",
    "#                     print(f\"[{run_name}] WARNING: Time stuck at {current_t} for 10 iterations, breaking loop\")\n",
    "#                     broke_stuck = True\n",
    "#                     break\n",
    "#             else:\n",
    "#                 repeat_count = 0  # Reset counter if time advances\n",
    "#             last_t = current_t\n",
    "        \n",
    "#         xk = info_dict.get(\"xk\", None)\n",
    "#         if xk is not None:\n",
    "#             n = N_DOFS\n",
    "#             v = xk[:n]\n",
    "#             u = xk[n:2*n]\n",
    "#             times.append(float(t_now))\n",
    "#             mean_vel.append(float(np.mean(v)))\n",
    "#             mean_slip.append(float(np.mean(u)))\n",
    "    \n",
    "#     wall = time.time() - start\n",
    "    \n",
    "#     # Determine success: prefer env-provided converged, else reach tnmax and not stuck\n",
    "#     # Access the underlying env to get tnmax\n",
    "#     if hasattr(vec_env, 'venv'):\n",
    "#         base_env = vec_env.venv.envs[0]\n",
    "#     elif hasattr(vec_env, 'envs'):\n",
    "#         base_env = vec_env.envs[0]\n",
    "#     else:\n",
    "#         base_env = vec_env\n",
    "#     tnmax_val = getattr(base_env, 'tnmax', None)\n",
    "    \n",
    "#     if last_converged is not None:\n",
    "#         success = bool(last_converged)\n",
    "#     else:\n",
    "#         if len(times) > 0 and tnmax_val is not None:\n",
    "#             success = (times[-1] >= 0.999 * float(tnmax_val)) and not broke_stuck\n",
    "#         else:\n",
    "#             success = not broke_stuck\n",
    "    \n",
    "#     # Convert to arrays\n",
    "#     return np.asarray(times), np.asarray(mean_vel), np.asarray(mean_slip), wall, success\n",
    "\n",
    "# # Storage for plotting and table\n",
    "# series = {}  # key -> dict(times, mean_v, mean_u, runtime)\n",
    "\n",
    "# for alpha_dir in alpha_dirs:\n",
    "#     algo_dirs = sorted([p for p in alpha_dir.iterdir() if p.is_dir()])\n",
    "#     if not algo_dirs:\n",
    "#         print(f\"Skipping {alpha_dir.name}: no algorithm runs found\")\n",
    "#         continue\n",
    "    \n",
    "#     for algo_dir in algo_dirs:\n",
    "#         algo_name = algo_dir.name\n",
    "#         ModelClass = ALGO_REGISTRY.get(algo_name)\n",
    "#         if ModelClass is None:\n",
    "#             print(f\"Skipping {algo_dir}: unknown algorithm '{algo_name}'\")\n",
    "#             continue\n",
    "        \n",
    "#         meta_path = algo_dir / \"metadata.json\"\n",
    "#         model_path_zip = algo_dir / \"model.zip\"\n",
    "#         model_path_plain = algo_dir / \"model\"\n",
    "#         vecnorm_path = algo_dir / \"vec_norm.pkl\"\n",
    "        \n",
    "#         if not meta_path.exists():\n",
    "#             print(f\"Skipping {algo_dir}: missing metadata.json\")\n",
    "#             continue\n",
    "        \n",
    "#         with open(meta_path, \"r\", encoding=\"utf-8\") as fh:\n",
    "#             meta = json.load(fh)\n",
    "        \n",
    "#         alpha_value = float(meta.get(\"alpha\", np.nan))\n",
    "#         if np.isnan(alpha_value):\n",
    "#             # Try to parse from parent folder name: alpha_1p0 -> 1.0\n",
    "#             try:\n",
    "#                 alabel = alpha_dir.name.split(\"alpha_\")[-1]\n",
    "#                 alpha_value = float(alabel.replace(\"p\", \".\").replace(\"m\", \"-\"))\n",
    "#             except Exception:\n",
    "#                 print(f\"Could not infer alpha from {alpha_dir.name}; skipping {algo_dir}\")\n",
    "#                 continue\n",
    "        \n",
    "#         key = f\"{algo_name} alpha={alpha_value:g}\"\n",
    "#         print(f\"Evaluating {key} from {(algo_dir.relative_to(RUN_ROOT))}...\")\n",
    "        \n",
    "#         # Build reward/obs EXACTLY as training for this alpha\n",
    "#         reward_fn = build_reward_fn(alpha_value)\n",
    "#         obs_fn = build_obs_fn(alpha_value)\n",
    "        \n",
    "#         # Create env factory with proper closure\n",
    "#         def make_env_fn(reward_fn=reward_fn, obs_fn=obs_fn, alpha_value=alpha_value):\n",
    "#             return make_eval_env(reward_fn, obs_fn, alpha_value)\n",
    "        \n",
    "#         # Create vec_env with the same structure as training\n",
    "#         vec_env = DummyVecEnv([make_env_fn])\n",
    "        \n",
    "#         # Load VecNormalize if it exists\n",
    "#         if vecnorm_path.exists():\n",
    "#             try:\n",
    "#                 vec_env = VecNormalize.load(str(vecnorm_path), vec_env)\n",
    "#                 vec_env.training = False  # Ensure we're in eval mode\n",
    "#                 vec_env.norm_reward = False  # Don't normalize rewards during eval\n",
    "#                 vec_env.norm_obs = False  # Don't normalize rewards during eval\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Warning: Failed to load VecNormalize from {vecnorm_path}: {e}\")\n",
    "#                 print(\"Proceeding without normalization\")\n",
    "#         else:\n",
    "#             print(f\"Warning: No VecNormalize found at {vecnorm_path}, proceeding without normalization\")\n",
    "        \n",
    "#         # Load model with the vec_env\n",
    "#         mdl_path = str(model_path_zip) if model_path_zip.exists() else str(model_path_plain)\n",
    "#         if not Path(mdl_path).exists():\n",
    "#             print(f\"Skipping {algo_dir}: no model found at {model_path_zip} or {model_path_plain}\")\n",
    "#             vec_env.close()\n",
    "#             continue\n",
    "        \n",
    "#         try:\n",
    "#             model = ModelClass.load(mdl_path, env=vec_env, device=\"cpu\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading model from {mdl_path}: {e}\")\n",
    "#             vec_env.close()\n",
    "#             continue\n",
    "        \n",
    "#         # Rollout - average over N_EVAL_RUNS runs for robust runtime measurement\n",
    "#         try:\n",
    "#             runtimes = []\n",
    "#             success_runs = []\n",
    "#             t_arr_store = None\n",
    "#             v_mean_store = None\n",
    "#             u_mean_store = None\n",
    "            \n",
    "#             for run_idx in range(N_EVAL_RUNS):\n",
    "#                 # Reset the environment for each run\n",
    "#                 vec_env.reset()\n",
    "#                 t_arr, v_mean, u_mean, wall, success = rollout_policy_on_env(model, vec_env, run_name=f\"{key}_run{run_idx}\")\n",
    "#                 runtimes.append(wall)\n",
    "#                 success_runs.append(bool(success))\n",
    "#                 if run_idx == 0:\n",
    "#                     # Store trajectory data from first run only\n",
    "#                     t_arr_store = t_arr\n",
    "#                     v_mean_store = v_mean\n",
    "#                     u_mean_store = u_mean\n",
    "            \n",
    "#             avg_runtime = np.mean(runtimes)\n",
    "#             std_runtime = np.std(runtimes) if len(runtimes) > 1 else 0.0\n",
    "#             any_success = any(success_runs)\n",
    "#             success_rate = sum(success_runs) / len(success_runs)\n",
    "            \n",
    "#             print(f\"Finished {key}: avg_wall={avg_runtime:.3f}s Â± {std_runtime:.3f}s, \"\n",
    "#                   f\"steps={len(t_arr_store) if t_arr_store is not None else 0}, \"\n",
    "#                   f\"t_end={(t_arr_store[-1] if t_arr_store is not None and len(t_arr_store)>0 else 'NA')}, \"\n",
    "#                   f\"success_rate={success_rate:.1%} ({sum(success_runs)}/{len(success_runs)})\")\n",
    "            \n",
    "#             series[key] = {\n",
    "#                 \"times\": t_arr_store if t_arr_store is not None else np.array([]),\n",
    "#                 \"mean_v\": v_mean_store if v_mean_store is not None else np.array([]),\n",
    "#                 \"mean_u\": u_mean_store if u_mean_store is not None else np.array([]),\n",
    "#                 \"runtime_s\": float(avg_runtime),\n",
    "#                 \"runtime_std\": float(std_runtime),\n",
    "#                 \"converged\": bool(any_success),\n",
    "#                 \"success_rate\": float(success_rate),\n",
    "#             }\n",
    "#         except Exception as e:\n",
    "#             print(f\"FAILED {key}: {e}\")\n",
    "#             import traceback\n",
    "#             traceback.print_exc()\n",
    "#         finally:\n",
    "#             vec_env.close()\n",
    "#             del vec_env, model\n",
    "\n",
    "# # Baseline: adaptive integrator\n",
    "# def adaptive_integrator_baseline():\n",
    "#     \"\"\"Run the baseline adaptive integrator N_EVAL_RUNS times for consistent comparison.\"\"\"\n",
    "#     runtimes_b = []\n",
    "#     t_store = None\n",
    "#     y_store = None\n",
    "#     success_runs = []\n",
    "    \n",
    "#     for run_idx in range(N_EVAL_RUNS):\n",
    "#         try:\n",
    "#             start_b = time.time()\n",
    "#             projection_opts_nb = dict(projection_opts)\n",
    "#             projection_opts_nb['use_numba'] = True\n",
    "#             (t_vals_b, y_vals_b, h_vals_b, fk_vals_b, solver_info_b) = solve_nivp.solve_ivp_ns(\n",
    "#                 fun=rhs,\n",
    "#                 t_span=t_span,\n",
    "#                 y0=y0,\n",
    "#                 method='composite',\n",
    "#                 projection='coulomb',\n",
    "#                 solver='VI',\n",
    "#                 projection_opts=projection_opts_nb,\n",
    "#                 solver_opts=solver_opts_ssn,\n",
    "#                 adaptive=True,\n",
    "#                 adaptive_opts=adaptive_opts,\n",
    "#                 # atol=1e-8,\n",
    "#                 # rtol=1e-6,\n",
    "#                 h0=adaptive_opts['h0'],\n",
    "#                 component_slices=component_slices,\n",
    "#                 verbose=False,\n",
    "#                 A=A,\n",
    "#             )\n",
    "#             wall_b = time.time() - start_b\n",
    "#             runtimes_b.append(wall_b)\n",
    "            \n",
    "#             # Check success\n",
    "#             success_b = (len(t_vals_b) > 0 and t_vals_b[-1] >= 0.999 * t_span[1])\n",
    "#             success_runs.append(success_b)\n",
    "            \n",
    "#             if run_idx == 0:\n",
    "#                 # Store trajectory from first run\n",
    "#                 t_store = t_vals_b\n",
    "#                 y_store = y_vals_b\n",
    "#         except Exception as e:\n",
    "#             print(f\"Baseline run {run_idx} failed: {e}\")\n",
    "#             runtimes_b.append(np.nan)\n",
    "#             success_runs.append(False)\n",
    "    \n",
    "#     # Remove failed runs from timing\n",
    "#     runtimes_b = [r for r in runtimes_b if not np.isnan(r)]\n",
    "    \n",
    "#     if len(runtimes_b) == 0:\n",
    "#         print(\"WARNING: All baseline runs failed!\")\n",
    "#         return None, None, None, np.nan, np.nan, False\n",
    "    \n",
    "#     n = N_DOFS\n",
    "#     v_b = np.mean(y_store[:, :n], axis=1)\n",
    "#     u_b = np.mean(y_store[:, n:2*n], axis=1)\n",
    "#     avg_runtime_b = np.mean(runtimes_b)\n",
    "#     std_runtime_b = np.std(runtimes_b) if len(runtimes_b) > 1 else 0.0\n",
    "#     success_rate_b = sum(success_runs) / len(success_runs)\n",
    "#     any_success_b = any(success_runs)\n",
    "    \n",
    "#     print(f\"Adaptive integrator: avg_wall={avg_runtime_b:.3f}s Â± {std_runtime_b:.3f}s, \"\n",
    "#           f\"success_rate={success_rate_b:.1%} ({sum(success_runs)}/{len(success_runs)})\")\n",
    "    \n",
    "#     return t_store, v_b, u_b, avg_runtime_b, std_runtime_b, any_success_b\n",
    "\n",
    "# # Run baseline\n",
    "# try:\n",
    "#     t_b, v_b, u_b, wall_b, wall_b_std, succ_b = adaptive_integrator_baseline()\n",
    "#     if t_b is not None:\n",
    "#         series[\"adaptive_integrator\"] = {\n",
    "#             \"times\": np.asarray(t_b),\n",
    "#             \"mean_v\": np.asarray(v_b),\n",
    "#             \"mean_u\": np.asarray(u_b),\n",
    "#             \"runtime_s\": float(wall_b),\n",
    "#             \"runtime_std\": float(wall_b_std),\n",
    "#             \"converged\": bool(succ_b),\n",
    "#         }\n",
    "# except Exception as e:\n",
    "#     print(f\"Baseline evaluation failed: {e}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()\n",
    "\n",
    "# # Only plot if we have data\n",
    "# if series:\n",
    "#     # Plot velocity\n",
    "#     plt.figure(figsize=(10,6))\n",
    "#     for name, data in series.items():\n",
    "#         if len(data[\"times\"]) > 0:\n",
    "#             t_plot = data[\"times\"] * fault.Tscale / fault.second\n",
    "#             v_plot = data[\"mean_v\"] * fault.Vscale\n",
    "#             plt.plot(t_plot, v_plot, label=name)\n",
    "#     plt.xlabel(\"Time (s)\")\n",
    "#     plt.ylabel(\"Average velocity (m/s)\")\n",
    "#     plt.title(\"Average velocity vs time: RL policies and adaptive integrator\")\n",
    "#     plt.grid(True)\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Plot slip\n",
    "#     plt.figure(figsize=(10,6))\n",
    "#     for name, data in series.items():\n",
    "#         if len(data[\"times\"]) > 0:\n",
    "#             t_plot = data[\"times\"] * fault.Tscale / fault.second\n",
    "#             u_plot = data[\"mean_u\"] * fault.Dscale\n",
    "#             plt.plot(t_plot, u_plot, label=name)\n",
    "#     plt.xlabel(\"Time (s)\")\n",
    "#     plt.ylabel(\"Average slip (m)\")\n",
    "#     plt.title(\"Average slip vs time: RL policies and adaptive integrator\")\n",
    "#     plt.grid(True)\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Plot runtime vs log10(alpha) â€” skip non-converged entries\n",
    "#     from collections import defaultdict\n",
    "#     runtime_by_algo = defaultdict(list)\n",
    "#     baseline_runtime = None\n",
    "    \n",
    "#     for name, data in series.items():\n",
    "#         # Extract alpha from the key name (e.g., \"TQC alpha=1.0\")\n",
    "#         if \"alpha=\" in name:\n",
    "#             parts = name.split(\"alpha=\")\n",
    "#             algo = parts[0].strip()\n",
    "#             alpha = float(parts[1])\n",
    "#             if data.get(\"converged\", True):  # only include converged runs\n",
    "#                 runtime = data[\"runtime_s\"]\n",
    "#                 runtime_by_algo[algo].append((alpha, runtime))\n",
    "#         elif \"adaptive_integrator\" in name:\n",
    "#             if data.get(\"converged\", True):\n",
    "#                 baseline_runtime = data[\"runtime_s\"]\n",
    "    \n",
    "#     if runtime_by_algo:\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         for algo_name, pairs in runtime_by_algo.items():\n",
    "#             if not pairs:\n",
    "#                 continue\n",
    "#             pairs_sorted = sorted(pairs, key=lambda x: x[0])\n",
    "#             alphas = [p[0] for p in pairs_sorted]\n",
    "#             runtimes = [p[1] for p in pairs_sorted]\n",
    "#             plt.plot(np.log10(alphas), runtimes, marker='o', label=algo_name, linewidth=2, markersize=8)\n",
    "        \n",
    "#         if baseline_runtime is not None:\n",
    "#             plt.axhline(baseline_runtime, color='k', linestyle='--', linewidth=2, label='adaptive integrator baseline')\n",
    "        \n",
    "#         plt.xlabel(\"log10(alpha)\")\n",
    "#         plt.ylabel(\"Runtime (s)\")\n",
    "#         plt.title(\"Runtime vs log10(alpha): RL policies and adaptive integrator (converged only)\")\n",
    "#         plt.grid(True)\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "    \n",
    "#     # Runtime table with success rate\n",
    "#     rows = []\n",
    "#     for name, data in series.items():\n",
    "#         row = {\n",
    "#             \"method\": name, \n",
    "#             \"wall_time_s\": data[\"runtime_s\"], \n",
    "#             \"std_s\": data.get(\"runtime_std\", 0.0),\n",
    "#             \"converged\": data.get(\"converged\", True)\n",
    "#         }\n",
    "#         if \"success_rate\" in data:\n",
    "#             row[\"success_rate\"] = f\"{data['success_rate']:.1%}\"\n",
    "#         rows.append(row)\n",
    "    \n",
    "#     if _HAVE_PANDAS:\n",
    "#         df = pd.DataFrame(rows).sort_values([\"converged\", \"wall_time_s\"], ascending=[False, True]).reset_index(drop=True)\n",
    "#         print(\"\\nRuntime summary (seconds):\")\n",
    "#         display(df)\n",
    "#     else:\n",
    "#         print(\"\\nRuntime summary (seconds) â€” converged first:\")\n",
    "#         for row in sorted(rows, key=lambda r: (not r[\"converged\"], r[\"wall_time_s\"])):\n",
    "#             tag = \" (failed)\" if not row[\"converged\"] else \"\"\n",
    "#             std_str = f\" Â± {row['std_s']:.3f}s\" if row['std_s'] > 0 else \"\"\n",
    "#             success_str = f\" (success: {row.get('success_rate', 'N/A')})\" if 'success_rate' in row else \"\"\n",
    "#             print(f\"- {row['method']}: {row['wall_time_s']:.3f}s{std_str}{tag}{success_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83f347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import time\n",
    "# import json\n",
    "# from pathlib import Path\n",
    "# import matplotlib.pyplot as plt\n",
    "# import scipy.sparse as sp\n",
    "\n",
    "# from gymnasium import spaces\n",
    "# from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "# from sb3_contrib import TQC\n",
    "# # from stable_baselines3 import TD3\n",
    "\n",
    "# from solve_nivp.projections import CoulombProjection\n",
    "# from solve_nivp.nonlinear_solvers import ImplicitEquationSolver\n",
    "# from solve_nivp.integrations import CompositeMethod\n",
    "# from solve_nivp.rl.env import AdaptiveStepperEnv\n",
    "# import solve_nivp\n",
    "# from plants.faults import strikeslip\n",
    "\n",
    "# ###############################################################################\n",
    "# # Configuration\n",
    "# ###############################################################################\n",
    "# ALGO_REGISTRY = {\n",
    "#     \"TQC\": TQC,\n",
    "#     # \"TD3\": TD3,\n",
    "# }\n",
    "\n",
    "# RUN_ROOT = Path(\"TEST_RL_ALPHA_SWEEP\").resolve()\n",
    "# N_EVAL_RUNS = 5  # Number of evaluation runs for runtime averaging\n",
    "\n",
    "# # Grid specifications with their specific v_max values\n",
    "# GRID_SPECS = [\n",
    "#     # {\"Nz\": 1, \"Nx\": 1, \"label\": \"1x1\", \"v_max\": 0.07},  # Training grid\n",
    "#     {\"Nz\": 25, \"Nx\": 25, \"label\": \"25x25\", \"v_max\": 0.07},  # Same v_max as 1x1\n",
    "#     {\"Nz\": 50, \"Nx\": 50, \"label\": \"50x50\", \"v_max\": 0.07},  # Same v_max as 1x1\n",
    "#     {\"Nz\": 75, \"Nx\": 75, \"label\": \"75x75\", \"v_max\": 0.07},  # Same v_max as 1x1\n",
    "#     {\"Nz\": 100, \"Nx\": 100, \"label\": \"100x100\", \"v_max\": 0.07},  # Same v_max as 1x1\n",
    "\n",
    "#     {\"Nz\": 50, \"Nx\": 50, \"label\": \"50x50\", \"v_max\": 0.07},  # Same v_max as 1x1\n",
    "\n",
    "#     # {\"Nz\": 5, \"Nx\": 5, \"label\": \"5x5\", \"v_max\": 0.18},  # Different v_max\n",
    "# ]\n",
    "\n",
    "# # Time span (assuming you have this defined globally)\n",
    "# # t_span = [0.0, your_final_time]\n",
    "\n",
    "# # Friction parameters (must match training)\n",
    "# DMU = -0.1\n",
    "# DC = 100.0  # Will be scaled by fault.Dscale\n",
    "# MU_RES = 0.5\n",
    "\n",
    "# ###############################################################################\n",
    "# # Build fault and solver for a given grid\n",
    "# ###############################################################################\n",
    "# def build_fault_and_solver(Nz, Nx):\n",
    "#     \"\"\"\n",
    "#     Build a fault model and solver stack for a given grid size.\n",
    "#     Returns dict with all necessary components.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # 1. Create fault for this grid\n",
    "#     fault_local = strikeslip.qs_strikeslip_fault(\n",
    "#         zdepth=3, xlength=3, Nz=Nz, Nx=Nx,\n",
    "#         G=30000., rho=2.5e-3, zeta=0.8/3,\n",
    "#         Ks_path=\"./Data/\", gamma_s=25., gamma_w=10.,\n",
    "#         sigma_ref=100., depth_ini=0., vinf=3.171e-10,\n",
    "#         Dmu_estimate=.5,\n",
    "#     )\n",
    "\n",
    "#     # 2. Extract plant matrices\n",
    "#     MA_l, KS_l, ES_l, SIGMA_N_l, VINF_raw_l = fault_local.get_plant()\n",
    "#     N_DOFS_l = fault_local.N\n",
    "#     VINF_l = VINF_raw_l * np.ones(N_DOFS_l) * 0  # Set to zero as in your code\n",
    "\n",
    "#     # 3. Build block diagonal A matrix\n",
    "#     I_N = sp.eye(N_DOFS_l, format='csr')\n",
    "#     A_l = sp.block_diag([sp.csr_matrix(MA_l), I_N, I_N], format='csr')\n",
    "\n",
    "#     # 4. Component slices\n",
    "#     component_slices_l = [\n",
    "#         slice(0, N_DOFS_l),              # v\n",
    "#         slice(N_DOFS_l, 2 * N_DOFS_l),   # u\n",
    "#         slice(2 * N_DOFS_l, 3 * N_DOFS_l) # s\n",
    "#     ]\n",
    "\n",
    "#     # 5. Scaled DC for this grid\n",
    "#     DC_scaled = DC / fault_local.Dscale\n",
    "\n",
    "#     # 6. Contact force function\n",
    "#     def con_force_l(state, fk=None):\n",
    "#         n = N_DOFS_l\n",
    "#         slip_hist = state[2*n:3*n]\n",
    "#         mu_vals = MU_RES * (1 - DMU / MU_RES * np.exp(-slip_hist / DC_scaled))\n",
    "#         out = np.zeros_like(state)\n",
    "#         out[:n] = mu_vals * SIGMA_N_l\n",
    "#         return out\n",
    "\n",
    "#     # 7. Initial state\n",
    "#     y0_l = np.zeros(3 * N_DOFS_l)\n",
    "#     friction_force0 = con_force_l(y0_l)\n",
    "#     uc = -np.linalg.solve(KS_l, friction_force0[:N_DOFS_l])\n",
    "#     y0_l[N_DOFS_l:2*N_DOFS_l] = uc * (1.0 + 1e-5)\n",
    "\n",
    "#     # 8. Critical energy for normalization\n",
    "#     Uintc_l = 0.5 * float(uc @ (KS_l @ uc))\n",
    "\n",
    "#     # 9. Projection operator\n",
    "#     projection_l = CoulombProjection(\n",
    "#         con_force_func=con_force_l,\n",
    "#         rhok=np.ones(N_DOFS_l, dtype=float),\n",
    "#         component_slices=component_slices_l,\n",
    "#         constraint_indices=list(range(N_DOFS_l)),\n",
    "#         use_numba=True,\n",
    "#     )\n",
    "\n",
    "#     # 10. VI solver\n",
    "\n",
    "#     # Tuned nonlinear-solver options shared by VI and SSN\n",
    "#     solver_opts_common = dict(\n",
    "#         tol=1e-8,\n",
    "#         max_iter=200,\n",
    "#     )\n",
    "\n",
    "#     # Adaptive controller tuned for nonsmooth dynamics\n",
    "#     adaptive_opts = dict(\n",
    "#         h0=5e-2,\n",
    "#         h_min=1e-7,\n",
    "#         h_down=0.6,\n",
    "#         h_up=1.8,\n",
    "#         method_order=1,           # conservative order for nonsmooth dynamics\n",
    "#         skip_error_indices=[],\n",
    "#         controller='h211b',              # smoother steps with PI control\n",
    "#         b_param=4.0,         # only if controller == \"H211b\"\n",
    "#         mode = 'ratio'\n",
    "\n",
    "#     )\n",
    "\n",
    "#     # SSN-specific tweaks for robustness and speed\n",
    "#     solver_opts_ssn = dict(solver_opts_common)\n",
    "#     solver_opts_ssn.update({\n",
    "#         \"vi_strict_block_lipschitz\": False,   # was True by default\n",
    "#         \"vi_max_block_adjust_iters\": 5,       # smaller safety cap\n",
    "#         \"globalization\": 'line_search',    # more robust globalization\n",
    "#     })\n",
    "#     solver_mp_l = ImplicitEquationSolver(\n",
    "#         method='VI',\n",
    "#         proj=projection_l,\n",
    "#         component_slices=component_slices_l,\n",
    "#         tol=solver_opts_ssn.get('tol', 1e-6),\n",
    "#         max_iter=solver_opts_ssn.get('max_iter', 50),\n",
    "#         vi_strict_block_lipschitz=solver_opts_ssn.get('vi_strict_block_lipschitz', False),\n",
    "#         vi_max_block_adjust_iters=solver_opts_ssn.get('vi_max_block_adjust_iters', 10),\n",
    "#     )\n",
    "\n",
    "#     # 11. Composite method\n",
    "#     method_mp_l = CompositeMethod(solver=solver_mp_l, A=A_l)\n",
    "\n",
    "#     # 12. RHS function\n",
    "#     def rhs_l(t, y):\n",
    "#         n = N_DOFS_l\n",
    "#         v = y[:n]\n",
    "#         u = y[n:2*n]\n",
    "\n",
    "#         vdot = -(KS_l @ u) - (ES_l @ (v - VINF_l))\n",
    "#         udot = v - VINF_l\n",
    "#         sdot = np.abs(v)\n",
    "#         return np.concatenate((vdot, udot, sdot))\n",
    "\n",
    "#     return dict(\n",
    "#         fault=fault_local,\n",
    "#         KS=KS_l,\n",
    "#         ES=ES_l,\n",
    "#         SIGMA_N=SIGMA_N_l,\n",
    "#         VINF=VINF_l,\n",
    "#         N_DOFS=N_DOFS_l,\n",
    "#         A=A_l,\n",
    "#         component_slices=component_slices_l,\n",
    "#         con_force=con_force_l,\n",
    "#         y0=y0_l,\n",
    "#         Uintc=Uintc_l,\n",
    "#         method_mp=method_mp_l,\n",
    "#         rhs=rhs_l,\n",
    "#         DC_scaled=DC_scaled,\n",
    "#         solver_opts_ssn=solver_opts_ssn,\n",
    "#     )\n",
    "\n",
    "# ###############################################################################\n",
    "# # Build observation and reward functions (WITH SIGNED DT_NORM)\n",
    "# ###############################################################################\n",
    "# def build_reward_fn(alpha_value: float):\n",
    "#     \"\"\"Your existing reward function\"\"\"\n",
    "#     alpha_value = float(alpha_value)\n",
    "\n",
    "#     def my_reward_fn(solver_perf, dt_attempt, xk, env):\n",
    "#         (\n",
    "#             runtime_inc,\n",
    "#             dts,\n",
    "#             error_LO,\n",
    "#             error_lil1,\n",
    "#             error_HI,\n",
    "#             E,\n",
    "#             success_LO,\n",
    "#             success_lil1,\n",
    "#             success_HI,\n",
    "#             kiter_LO,\n",
    "#             iter_lil1,\n",
    "#             kiter_HI,\n",
    "#         ) = solver_perf\n",
    "\n",
    "#         dt_range = env.dt_max - env.dt_min\n",
    "\n",
    "#         if dts == 0.0:\n",
    "#             dt_norm_attempt = (dt_attempt - env.dt_min) / dt_range\n",
    "#             return -dt_norm_attempt\n",
    "\n",
    "#         # S1: step size score\n",
    "#         dt_norm = (dts - env.dt_min) / dt_range\n",
    "#         S1 = float(np.clip(dt_norm, 0.0, 1.0))\n",
    "\n",
    "#         # S2: runtime score (simplified version)\n",
    "#         if not hasattr(env, 'runtime_history'):\n",
    "#             env.runtime_history = []\n",
    "#         env.runtime_history.append(float(runtime_inc))\n",
    "\n",
    "#         if not hasattr(env, 'max_runtime_seen'):\n",
    "#             env.max_runtime_seen = float(runtime_inc)\n",
    "#         else:\n",
    "#             if runtime_inc > env.max_runtime_seen:\n",
    "#                 env.max_runtime_seen = float(runtime_inc)\n",
    "\n",
    "#         if not hasattr(env, 'min_runtime_seen'):\n",
    "#             env.min_runtime_seen = float(runtime_inc)\n",
    "#         else:\n",
    "#             if runtime_inc < env.min_runtime_seen:\n",
    "#                 env.min_runtime_seen = float(runtime_inc)\n",
    "\n",
    "#         rt_min = float(env.min_runtime_seen)\n",
    "#         rt_max = max(rt_min, float(env.max_runtime_seen))\n",
    "#         denom = max(1e-8, rt_max - rt_min)\n",
    "#         S2 = 1 - (float(runtime_inc) - rt_min) / denom\n",
    "\n",
    "#         # A: accuracy score\n",
    "#         A = float(np.exp(-alpha_value * E))\n",
    "\n",
    "#         reward = S1 * S2 * A\n",
    "#         return reward\n",
    "\n",
    "#     return my_reward_fn\n",
    "\n",
    "# def make_obs_reward(alpha_value, grid, v_max_overr\n",
    "#         Build observation and reward functions for evaluation.\n",
    "#     Uses v_max_override for velocity normalization.\n",
    "#     \"\"\"\n",
    "#     reward_fn = build_reward_fn(alpha_value)\n",
    "\n",
    "#     def obs_fn(dt_attempt, converged, xk, solver_perf, fk=None, env=None):\n",
    "#         assert env is not None, \"env must be provided\"\n",
    "\n",
    "#         # Extract physics state\n",
    "#         n = grid['N_DOFS']\n",
    "#         v = xk[:n]\n",
    "#         u = xk[n:2*n]\n",
    "\n",
    "#         avg_v = float(np.mean(v))\n",
    "\n",
    "#         # Elastic energy\n",
    "#         ks_u = grid['KS'] @ u\n",
    "#         E_int = 0.5 * float(np.dot(u, ks_u))\n",
    "#         E_norm = float(E_int / grid['Uintc']) if grid['Uintc'] > 0 else float(E_int)\n",
    "\n",
    "#         # Bipolar convergence flag for signed dt_norm\n",
    "#         if solver_perf is not None:\n",
    "#             dts = solver_perf[1]\n",
    "#             conv_flag_bipolar = 1.0 if dts > 0.0 else -1.0\n",
    "#         else:\n",
    "#             conv_flag_bipolar = -1.0\n",
    "\n",
    "#         # RESET BRANCH\n",
    "#         if dt_attempt is None or solver_perf is None:\n",
    "#             dt_norm_default = 0.5\n",
    "#             conv_flag = 0.0 if converged is None else float(converged)\n",
    "\n",
    "#             return np.array([\n",
    "#                 E_norm,\n",
    "#                 avg_v / (v_max_override / grid['fault'].Vscale),  # Use override\n",
    "#                 (env.iter_error / 4.0) if hasattr(env, 'iter_error') else 0.0,\n",
    "#                 conv_flag,\n",
    "#                 0.0,  # accuracy proxy neutral\n",
    "#                 conv_flag_bipolar * dt_norm_default,  # SIGNED!\n",
    "#             ], dtype=np.float64)\n",
    "\n",
    "#         # NORMAL STEP BRANCH\n",
    "#         (\n",
    "#             runtime_inc,\n",
    "#             dts,\n",
    "#             error_LO,\n",
    "#             error_lil1,\n",
    "#             error_HI,\n",
    "#             E_global,\n",
    "#             success_LO,\n",
    "#             success_lil1,\n",
    "#             success_HI,\n",
    "#             kiter_LO,\n",
    "#             iter_lil1,\n",
    "#             kiter_HI,\n",
    "#         ) = solver_perf\n",
    "\n",
    "#         # Normalize dt_attempt\n",
    "#         dt_range = env.dt_max - env.dt_min\n",
    "#         if dt_range <= 0.0:\n",
    "#             dt_norm = 0.0\n",
    "#         else:\n",
    "#             dt_norm = (dt_attempt - env.dt_min) / dt_range\n",
    "#             dt_norm = float(np.clip(dt_norm, 0.0, 1.0))\n",
    "\n",
    "#         # Convergence and accuracy\n",
    "#         conv_flag = float(converged) if converged is not None else 0.0\n",
    "#         if conv_flag:\n",
    "#             acc = 1.0 / (1.0 + E_global)\n",
    "#         else:\n",
    "#             acc = 0.0\n",
    "\n",
    "#         # Return observation with SIGNED dt_norm\n",
    "#         return np.array([\n",
    "#             E_norm,\n",
    "#             avg_v / (v_max_override / grid['fault'].Vscale),  # Use override\n",
    "#             (env.iter_error / 4.0) if hasattr(env, 'iter_error') else 0.0,\n",
    "#             conv_flag,\n",
    "#             acc,\n",
    "#             conv_flag_bipolar * dt_norm,  # SIGNED!\n",
    "#         ], dtype=np.float64)\n",
    "\n",
    "#     return reward_fn, obs_fn\n",
    "\n",
    "# ###############################################################################\n",
    "# # Rollout helper\n",
    "# ###############################################################################\n",
    "# def rollout_policy_on_vecenv(model, vec_env, run_name=\"(unnamed)\"):\n",
    "#     \"\"\"Roll out a policy on a VecEnv and collect trajectory data.\"\"\"\n",
    "#     vec_env.training = False\n",
    "#     vec_env.norm_obs = False\n",
    "#     vec_env.norm_reward = False\n",
    "\n",
    "#     base_env = vec_env.venv.envs[0]\n",
    "\n",
    "#     obs = vec_env.reset()\n",
    "#     times = []\n",
    "#     mean_vel = []\n",
    "#     mean_slip = []\n",
    "\n",
    "#     last_t = None\n",
    "#     repeat_count = 0\n",
    "#     broke_stuck = False\n",
    "#     last_converged = None\n",
    "#     last_xk = None\n",
    "\n",
    "#     start = time.time()\n",
    "#     done = False\n",
    "\n",
    "#     while not done:\n",
    "#         action, _ = model.predict(obs, deterministic=True)\n",
    "#         obs, rewards, dones, infos = vec_env.step(action)\n",
    "\n",
    "#         done = bool(dones[0])\n",
    "#         info = infos[0]\n",
    "\n",
    "#         t_now = info.get(\"t_k1\", np.nan)\n",
    "#         xk = info.get(\"xk\", None)\n",
    "#         if xk is not None:\n",
    "#             last_xk = xk\n",
    "\n",
    "#         if \"converged\" in info:\n",
    "#             last_converged = info.get(\"converged\")\n",
    "\n",
    "#         # Check for stuck time\n",
    "#         current_t = None if t_now is None or np.isnan(t_now) else float(t_now)\n",
    "#         if current_t is not None:\n",
    "#             if last_t is not None and current_t == last_t:\n",
    "#                 repeat_count += 1\n",
    "#                 if repeat_count >= 10:\n",
    "#                     print(f\"[{run_name}] WARNING: time stuck at {current_t}\")\n",
    "#                     broke_stuck = True\n",
    "#                     break\n",
    "#             else:\n",
    "#                 repeat_count = 0\n",
    "#             last_t = current_t\n",
    "\n",
    "#         # Store trajectory data\n",
    "#         if xk is not None and current_t is not None:\n",
    "#             n = xk.shape[0] // 3\n",
    "#             v_block = xk[:n]\n",
    "#             u_block = xk[n:2*n]\n",
    "#             times.append(current_t)\n",
    "#             mean_vel.append(float(np.mean(v_block)))\n",
    "#             mean_slip.append(float(np.mean(u_block)))\n",
    "\n",
    "#     wall_time = time.time() - start\n",
    "\n",
    "#     # Determine success\n",
    "#     if last_converged is not None:\n",
    "#         success = bool(last_converged)\n",
    "#     else:\n",
    "#         if len(times) > 0 and hasattr(base_env, 'tnmax'):\n",
    "#             success = (times[-1] >= 0.999 * float(base_env.tnmax)) and not broke_stuck\n",
    "#         else:\n",
    "#             success = not broke_stuck\n",
    "\n",
    "#     # Final slip mean\n",
    "#     if last_xk is not None:\n",
    "#         n = last_xk.shape[0] // 3\n",
    "#         s_block = last_xk[2*n:3*n]\n",
    "#         s_end_mean = float(np.mean(s_block))\n",
    "#     else:\n",
    "#         s_end_mean = np.nan\n",
    "\n",
    "#     print(f\"[{run_name}] Done: wall_time={wall_time:.3f}s, steps={len(times)}, \"\n",
    "#           f\"final_t={(times[-1] if len(times)>0 else 'NA'):.3f}, \"\n",
    "#           f\"s_end={s_end_mean:.6e}\")\n",
    "\n",
    "#     return (\n",
    "#         np.asarray(times, dtype=float),\n",
    "#         np.asarray(mean_vel, dtype=float),\n",
    "#         np.asarray(mean_slip, dtype=float),\n",
    "#         float(wall_time),\n",
    "#         bool(success),\n",
    "#         s_end_mean,\n",
    "#     )\n",
    "\n",
    "# ###############################################################################\n",
    "# # Evaluate policies on a specific grid\n",
    "# ###############################################################################\n",
    "# def evaluate_policies_for_grid(grid, spec, alpha_dirs):\n",
    "#     \"\"\"Evaluate all trained policies on a specific grid configuration.\"\"\"\n",
    "\n",
    "#     v_max_override = spec['v_max']\n",
    "#     grid_label = spec['label']\n",
    "\n",
    "#     # Time step bounds (adjust as needed)\n",
    "#     local_dt_min = 1e-6\n",
    "#     local_dt_max = 30 / 5 * grid['fault'].second / grid['fault'].Tscale\n",
    "\n",
    "#     series = {}\n",
    "\n",
    "#     for alpha_dir in alpha_dirs:\n",
    "#         algo_dirs = sorted([p for p in alpha_dir.iterdir() if p.is_dir()])\n",
    "\n",
    "#         for algo_dir in algo_dirs:\n",
    "#             algo_name = algo_dir.name\n",
    "#             ModelClass = ALGO_REGISTRY.get(algo_name)\n",
    "#             if ModelClass is None:\n",
    "#                 continue\n",
    "\n",
    "#             meta_path = algo_dir / \"metadata.json\"\n",
    "#             model_path = algo_dir / \"model.zip\"\n",
    "#             if not model_path.exists():\n",
    "#                 model_path = algo_dir / \"model\"\n",
    "#             vecnorm_path = algo_dir / \"vec_norm.pkl\"\n",
    "\n",
    "#             if not meta_path.exists() or not model_path.exists():\n",
    "#                 continue\n",
    "\n",
    "#             with open(meta_path, \"r\", encoding=\"utf-8\") as fh:\n",
    "#                 meta = json.load(fh)\n",
    "\n",
    "#             alpha_value = float(meta.get(\"alpha\", np.nan))\n",
    "#             if not np.isfinite(alpha_value):\n",
    "#                 continue\n",
    "\n",
    "#             # Build reward/obs functions for this grid\n",
    "#             reward_fn, obs_fn = make_obs_reward(alpha_value, grid, v_max_override)\n",
    "\n",
    "#             # Environment factory\n",
    "#             def make_env():\n",
    "#                 obs_space = spaces.Box(\n",
    "#                     low=-np.inf,\n",
    "#                     high=np.inf,\n",
    "#                     shape=(6,),\n",
    "#                     dtype=np.float64\n",
    "#                 )\n",
    "#                 return AdaptiveStepperEnv(\n",
    "#                     system=grid['rhs'],\n",
    "#                     dt0=local_dt_min,\n",
    "#                     t0=t_span[0],\n",
    "#                     x0=grid['y0'],\n",
    "#                     tnmax=t_span[1],\n",
    "#                     dt_min=local_dt_min,\n",
    "#                     dt_max=local_dt_max,\n",
    "#                     nparams=(1e-6, 100),\n",
    "#                     integrator=grid['method_mp'],\n",
    "#                     component_slices=grid['component_slices'],\n",
    "#                     reward_fn=reward_fn,\n",
    "#                     obs_fn=obs_fn,\n",
    "#                     obs_space=obs_space,\n",
    "#                     verbose=False,\n",
    "#                     alpha=alpha_value,\n",
    "#                 )\n",
    "\n",
    "#             # Load model\n",
    "#             model = ModelClass.load(str(model_path), device=\"cpu\")\n",
    "\n",
    "#             # Run evaluations\n",
    "#             runtimes = []\n",
    "#             success_runs = []\n",
    "#             slip_end_list = []\n",
    "#             t_store = None\n",
    "#             v_store = None\n",
    "#             u_store = None\n",
    "\n",
    "#             key = f\"{algo_name} alpha={alpha_value:g}\"\n",
    "#             print(f\"\\nEvaluating {key} on grid {grid_label}...\")\n",
    "\n",
    "#             for run_idx in range(N_EVAL_RUNS):\n",
    "#                 # Create fresh env for each run\n",
    "#                 base_vec_env = DummyVecEnv([make_env])\n",
    "\n",
    "#                 # Load VecNormalize if available\n",
    "#                 if vecnorm_path.exists():\n",
    "#                     vec_env = VecNormalize.load(str(vecnorm_path), base_vec_env)\n",
    "#                     vec_env.training = False\n",
    "#                     vec_env.norm_obs = False  # Disable since we handle normalization\n",
    "#                     vec_env.norm_reward = False\n",
    "#                 else:\n",
    "#                     vec_env = base_vec_env\n",
    "\n",
    "#                 # Attach env to model\n",
    "#                 model.set_env(vec_env)\n",
    "\n",
    "#                 # Rollout\n",
    "#                 t_arr, v_mean, u_mean, wall, success, s_end_mean = rollout_policy_on_vecenv(\n",
    "#                     model, vec_env, run_name=f\"{key} run{run_idx}\"\n",
    "#                 )\n",
    "\n",
    "#                 runtimes.append(wall)\n",
    "#                 success_runs.append(success)\n",
    "#                 slip_end_list.append(s_end_mean)\n",
    "\n",
    "#                 if run_idx == 0:\n",
    "#                     t_store = t_arr\n",
    "#                     v_store = v_mean\n",
    "#                     u_store = u_mean\n",
    "\n",
    "#                 vec_env.close()\n",
    "\n",
    "#                 # Stop if first run failed (deterministic policy)\n",
    "#                 if run_idx == 0 and not success:\n",
    "#                     break\n",
    "\n",
    "#             # Record results if any run succeeded\n",
    "#             if any(success_runs):\n",
    "#                 times_arr = np.asarray(t_store, dtype=float) if t_store is not None else np.array([], dtype=float)\n",
    "#                 mean_v_arr = np.asarray(v_store, dtype=float) if v_store is not None else np.array([], dtype=float)\n",
    "#                 mean_u_arr = np.asarray(u_store, dtype=float) if u_store is not None else np.array([], dtype=float)\n",
    "#                 finite_slips = [s for s in slip_end_list if np.isfinite(s)]\n",
    "#                 s_end_mean = float(np.mean(finite_slips)) if finite_slips else float(\"nan\")\n",
    "#                 series[key] = dict(\n",
    "#                     times=times_arr,\n",
    "#                     mean_v=mean_v_arr,\n",
    "#                     mean_u=mean_u_arr,\n",
    "#                     runtime_s=float(np.mean(runtimes)),\n",
    "#                     runtime_std=float(np.std(runtimes)) if len(runtimes) > 1 else 0.0,\n",
    "#                     converged=True,\n",
    "#                     success_rate=sum(success_runs) / len(success_runs),\n",
    "#                     s_end_mean=s_end_mean,\n",
    "#                 )\n",
    "#                 print(f\"  Success rate: {series[key]['success_rate']:.1%}, \"\n",
    "#                       f\"Runtime: {series[key]['runtime_s']:.3f}s Â± {series[key]['runtime_std']:.3f}s\")\n",
    "\n",
    "#     return series\n",
    "\n",
    "# ###############################################################################\n",
    "# # PI Baseline\n",
    "# ###############################################################################\n",
    "# def adaptive_baseline_for_grid(grid, adaptive_opts):\n",
    "#     \"\"\"Run classical PI adaptive controller as baseline.\"\"\"\n",
    "\n",
    "#     projection_opts_nb = dict(\n",
    "#         con_force_func=grid['con_force'],\n",
    "#         rhok=np.ones(grid['N_DOFS'], dtype=float),\n",
    "#         component_slices=grid['component_slices'],\n",
    "#         constraint_indices=list(range(grid['N_DOFS'])),\n",
    "#         use_numba=True,\n",
    "#     )\n",
    "\n",
    "#     solver_opts = grid.get('solver_opts_ssn')\n",
    "#     if solver_opts is None:\n",
    "#         raise KeyError(\"Grid dictionary missing 'solver_opts_ssn'. Rebuild grid with build_fault_and_solver().\")\n",
    "\n",
    "#     local_adapt = dict(adaptive_opts)\n",
    "#     # local_adapt[\"use_PI\"] = True\n",
    "\n",
    "#     runtimes_b = []\n",
    "#     success_runs = []\n",
    "#     slip_end_list = []\n",
    "#     t_store = None\n",
    "#     y_store = None\n",
    "\n",
    "#     for run_idx in range(N_EVAL_RUNS):\n",
    "#         start_b = time.time()\n",
    "#         try:\n",
    "#             (t_vals_b, y_vals_b, h_vals_b, fk_vals_b, solver_info_b) = solve_nivp.solve_ivp_ns(\n",
    "#                 fun=grid['rhs'],\n",
    "#                 t_span=t_span,\n",
    "#                 y0=grid['y0'],\n",
    "#                 method='composite',\n",
    "#                 projection='coulomb',\n",
    "#                 solver='VI',\n",
    "#                 projection_opts=projection_opts_nb,\n",
    "#                 solver_opts=solver_opts,\n",
    "#                 adaptive=True,\n",
    "#                 adaptive_opts=local_adapt,\n",
    "#                 h0=local_adapt.get('h0', 5e-3),\n",
    "#                 component_slices=grid['component_slices'],\n",
    "#                 verbose=False,\n",
    "#                 A=grid['A'],\n",
    "#             )\n",
    "#             wall_b = time.time() - start_b\n",
    "\n",
    "#             success = (len(t_vals_b) > 0 and t_vals_b[-1] >= 0.999 * t_span[1])\n",
    "#             success_runs.append(success)\n",
    "#             runtimes_b.append(wall_b)\n",
    "\n",
    "#             n = grid['N_DOFS']\n",
    "#             s_final_block = y_vals_b[-1, 2*n:3*n]\n",
    "#             s_end_mean = float(np.mean(s_final_block))\n",
    "#             slip_end_list.append(s_end_mean)\n",
    "\n",
    "#             if run_idx == 0:\n",
    "#                 t_store = t_vals_b\n",
    "#                 y_store = y_vals_b\n",
    "\n",
    "#             print(f\"  PI baseline run {run_idx}: {wall_b:.3f}s, success={success}\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"  PI baseline run {run_idx} failed: {e}\")\n",
    "#             success_runs.append(False)\n",
    "\n",
    "#     if not any(success_runs):\n",
    "#         print(\"  WARNING: PI baseline failed all runs\")\n",
    "#         return None\n",
    "\n",
    "#     runtimes_b = [r for r in runtimes_b if np.isfinite(r)]\n",
    "\n",
    "#     n = grid['N_DOFS']\n",
    "#     v_b = np.mean(y_store[:, :n], axis=1)\n",
    "#     u_b = np.mean(y_store[:, n:2*n], axis=1)\n",
    "\n",
    "#     return dict(\n",
    "#         times=np.asarray(t_store, dtype=float) if t_store is not None else np.array([], dtype=float),\n",
    "#         mean_v=np.asarray(v_b, dtype=float),\n",
    "#         mean_u=np.asarray(u_b, dtype=float),\n",
    "#         runtime_s=float(np.mean(runtimes_b)) if runtimes_b else float(\"nan\"),\n",
    "#         runtime_std=float(np.std(runtimes_b)) if len(runtimes_b) > 1 else 0.0,\n",
    "#         converged=True,\n",
    "#         success_rate=sum(success_runs) / len(success_runs),\n",
    "#         s_end_mean=float(np.mean([s for s in slip_end_list if np.isfinite(s)])) if slip_end_list else float(\"nan\"),\n",
    "#     )\n",
    "\n",
    "# ###############################################################################\n",
    "# # Main Execution\n",
    "# ###############################################################################\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Discover trained models\n",
    "#     alpha_dirs = sorted([p for p in RUN_ROOT.glob(\"alpha_*\") if p.is_dir()])\n",
    "#     if not alpha_dirs:\n",
    "#         raise FileNotFoundError(f\"No trained runs found under {RUN_ROOT}\")\n",
    "\n",
    "#     # PI controller settings\n",
    "#     adaptive_opts_pi = dict(\n",
    "#         h0=5e-3,\n",
    "#         h_min=1e-7,\n",
    "#         h_down=0.6,\n",
    "#         h_up=1.8,\n",
    "#         method_order=1,\n",
    "#         controller=\"H211b\",\n",
    "#         b_param=4.0,\n",
    "#         skip_error_indices=[],\n",
    "#     )\n",
    "\n",
    "#     # Results storage\n",
    "#     all_results = {}\n",
    "\n",
    "#     # Evaluate on each grid\n",
    "#     for spec in GRID_SPECS:\n",
    "#         print(f\"\\n{'='*60}\")\n",
    "#         print(f\"Evaluating on {spec['label']} grid (v_max={spec['v_max']})\")\n",
    "#         print('='*60)\n",
    "\n",
    "#         # Build grid\n",
    "#         grid = build_fault_and_solver(spec[\"Nz\"], spec[\"Nx\"])\n",
    "\n",
    "#         # Evaluate RL policies\n",
    "#         series = evaluate_policies_for_grid(grid, spec, alpha_dirs)\n",
    "\n",
    "#         # Evaluate PI baseline\n",
    "#         print(f\"\\nRunning PI baseline on {spec['label']}...\")\n",
    "#         baseline = adaptive_baseline_for_grid(grid, adaptive_opts_pi)\n",
    "#         if baseline is not None:\n",
    "#             series[\"PI_baseline\"] = baseline\n",
    "\n",
    "#         all_results[spec['label']] = series\n",
    "\n",
    "#         # Save results\n",
    "#         cache_path = Path(f\"eval_results_3by3_{spec['label']}.json\")\n",
    "\n",
    "#         scale_t = grid['fault'].Tscale / grid['fault'].second\n",
    "#         scale_v = grid['fault'].Vscale\n",
    "#         scale_u = grid['fault'].Dscale\n",
    "\n",
    "#         # Convert numpy arrays to lists for JSON serialization (scaled to physical units)\n",
    "#         json_series = {}\n",
    "#         for key, data in series.items():\n",
    "#             times = np.asarray(data.get(\"times\", []), dtype=float)\n",
    "#             mean_v = np.asarray(data.get(\"mean_v\", []), dtype=float)\n",
    "#             mean_u = np.asarray(data.get(\"mean_u\", []), dtype=float)\n",
    "#             s_end_mean = data[\"s_end_mean\"] if \"s_end_mean\" in data else None\n",
    "\n",
    "#             times_scaled = (times * scale_t).tolist() if times.size > 0 else []\n",
    "#             mean_v_scaled = (mean_v * scale_v).tolist() if mean_v.size > 0 else []\n",
    "#             mean_u_scaled = (mean_u * scale_u).tolist() if mean_u.size > 0 else []\n",
    "#             if s_end_mean is not None and np.isfinite(s_end_mean):\n",
    "#                 s_end_mean_scaled = float(s_end_mean) * scale_u\n",
    "#             else:\n",
    "#                 s_end_mean_scaled = None\n",
    "\n",
    "#             json_series[key] = {\n",
    "#                 \"times\": times_scaled,\n",
    "#                 \"mean_v\": mean_v_scaled,\n",
    "#                 \"mean_u\": mean_u_scaled,\n",
    "#                 \"runtime_s\": data.get(\"runtime_s\", float(\"nan\")),\n",
    "#                 \"runtime_std\": data.get(\"runtime_std\", 0.0),\n",
    "#                 \"converged\": data.get(\"converged\", False),\n",
    "#                 \"success_rate\": data.get(\"success_rate\", 1.0),\n",
    "#                 \"s_end_mean\": s_end_mean_scaled,\n",
    "#             }\n",
    "\n",
    "#         with open(cache_path, \"w\") as f:\n",
    "#             json.dump(json_series, f, indent=2)\n",
    "#         print(f\"Saved results to {cache_path}\")\n",
    "\n",
    "#     # Plot mean velocity trajectories for each grid (scaled to physical units)\n",
    "#     for spec in GRID_SPECS:\n",
    "#         series = all_results.get(spec['label'], {})\n",
    "#         if not series:\n",
    "#             continue\n",
    "\n",
    "#         fault_obj = build_fault_and_solver(spec[\"Nz\"], spec[\"Nx\"])['fault']\n",
    "#         scale_t = fault_obj.Tscale / fault_obj.second\n",
    "#         scale_v = fault_obj.Vscale\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plotted = False\n",
    "#         for name, data in series.items():\n",
    "#             times = np.asarray(data.get(\"times\", []), dtype=float)\n",
    "#             mean_v = np.asarray(data.get(\"mean_v\", []), dtype=float)\n",
    "#             if times.size == 0 or mean_v.size == 0:\n",
    "#                 continue\n",
    "\n",
    "#             plt.plot(times, mean_v, label=name)\n",
    "#             plotted = True\n",
    "\n",
    "#         if not plotted:\n",
    "#             plt.close()\n",
    "#             continue\n",
    "\n",
    "#         plt.xlabel(\"Time\")\n",
    "#         plt.ylabel(\"Mean velocity\")\n",
    "#         plt.title(f\"Mean velocity vs time ({spec['label']} grid)\")\n",
    "#         plt.grid(True, alpha=0.3)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "\n",
    "#         out_path = Path(f\"mean_velocity_{spec['label']}.png\")\n",
    "#         plt.savefig(out_path, dpi=150, bbox_inches='tight')\n",
    "#         print(f\"Saved velocity plot to {out_path}\")\n",
    "#         plt.show()\n",
    "\n",
    "#     print(\"\\n\" + \"=\"*60)\n",
    "#     print(\"EVALUATION COMPLETE\")\n",
    "#     print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bd1f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666d7e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import time\n",
    "# import json\n",
    "# from pathlib import Path\n",
    "# import matplotlib.pyplot as plt\n",
    "# import scipy.sparse as sp\n",
    "\n",
    "# from gymnasium import spaces\n",
    "# from stable_baselines3 import TD3            # for RL_TD3_ONLY mode\n",
    "# # from sb3_contrib import TQC               # not used here\n",
    "# from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "# from solve_nivp.projections import CoulombProjection\n",
    "# from solve_nivp.nonlinear_solvers import ImplicitEquationSolver\n",
    "# from solve_nivp.integrations import CompositeMethod\n",
    "# from solve_nivp.rl.env import AdaptiveStepperEnv\n",
    "# import solve_nivp\n",
    "# from plants.faults import strikeslip\n",
    "\n",
    "# # ============================\n",
    "# # Select what to run\n",
    "# # ============================\n",
    "# MODE = \"PI_ONLY\"        # options: \"PI_ONLY\", \"RL_TD3_ONLY\"\n",
    "# RUN_ROOT = Path(\"TEST_RL_ALPHA_SWEEP\").resolve()\n",
    "# N_EVAL_RUNS = 2\n",
    "\n",
    "# # ----------------------------\n",
    "# # Required: provide your time span here\n",
    "# # ----------------------------\n",
    "# # Example (replace with yours):\n",
    "# # t_span = [0.0, 40.0]   # nondimensional end time used in your runs\n",
    "# # t_span = [0.0, 30.0]\n",
    "\n",
    "# # ============================\n",
    "# # Grids and model physics\n",
    "# # ============================\n",
    "# GRID_SPECS = [\n",
    "#     {\"Nz\": 25, \"Nx\": 25, \"label\": \"25x25\", \"v_max\": 0.07},\n",
    "#     {\"Nz\": 50, \"Nx\": 50, \"label\": \"50x50\", \"v_max\": 0.07},\n",
    "#     {\"Nz\": 75, \"Nx\": 75, \"label\": \"75x75\", \"v_max\": 0.07},\n",
    "#     # {\"Nz\": 100, \"Nx\": 100, \"label\": \"100x100\", \"v_max\": 0.07},\n",
    "# ]\n",
    "\n",
    "# DMU = -0.1\n",
    "# DC = 100.0\n",
    "# MU_RES = 0.5\n",
    "\n",
    "# def build_fault_and_solver(Nz, Nx):\n",
    "#     fault_local = strikeslip.qs_strikeslip_fault(\n",
    "#         zdepth=5, xlength=5, Nz=Nz, Nx=Nx,\n",
    "#         G=30000., rho=2.5e-3, zeta=0.8/3,\n",
    "#         Ks_path=\"./Data/\", gamma_s=25., gamma_w=10.,\n",
    "#         sigma_ref=100., depth_ini=0., vinf=3.171e-10,\n",
    "#         Dmu_estimate=.5,\n",
    "#     )\n",
    "#     MA_l, KS_l, ES_l, SIGMA_N_l, VINF_raw_l = fault_local.get_plant()\n",
    "#     N_DOFS_l = fault_local.N\n",
    "#     VINF_l = VINF_raw_l * np.ones(N_DOFS_l) * 0\n",
    "\n",
    "#     I_N = sp.eye(N_DOFS_l, format='csr')\n",
    "#     A_l = sp.block_diag([sp.csr_matrix(MA_l), I_N, I_N], format='csr')\n",
    "\n",
    "#     component_slices_l = [\n",
    "#         slice(0, N_DOFS_l),\n",
    "#         slice(N_DOFS_l, 2 * N_DOFS_l),\n",
    "#         slice(2 * N_DOFS_l, 3 * N_DOFS_l),\n",
    "#     ]\n",
    "\n",
    "#     DC_scaled = DC / fault_local.Dscale\n",
    "\n",
    "#     def con_force_l(state, fk=None):\n",
    "#         n = N_DOFS_l\n",
    "#         slip_hist = state[2*n:3*n]\n",
    "#         mu_vals = MU_RES * (1 - DMU / MU_RES * np.exp(-slip_hist / DC_scaled))\n",
    "#         out = np.zeros_like(state)\n",
    "#         out[:n] = mu_vals * SIGMA_N_l\n",
    "#         return out\n",
    "\n",
    "#     y0_l = np.zeros(3 * N_DOFS_l)\n",
    "#     friction_force0 = con_force_l(y0_l)\n",
    "#     uc = -np.linalg.solve(KS_l, friction_force0[:N_DOFS_l])\n",
    "#     y0_l[N_DOFS_l:2*N_DOFS_l] = uc * (1.0 + 1e-5)\n",
    "#     Uintc_l = 0.5 * float(uc @ (KS_l @ uc))\n",
    "\n",
    "#     projection_l = CoulombProjection(\n",
    "#         con_force_func=con_force_l,\n",
    "#         rhok=np.ones(N_DOFS_l, dtype=float),\n",
    "#         component_slices=component_slices_l,\n",
    "#         constraint_indices=list(range(N_DOFS_l)),\n",
    "#         use_numba=True,\n",
    "#     )\n",
    "\n",
    "#     solver_opts_ssn = dict(\n",
    "#         tol=1e-8,\n",
    "#         max_iter=200,\n",
    "#         vi_strict_block_lipschitz=False,\n",
    "#         vi_max_block_adjust_iters=5,\n",
    "#         globalization='line_search',\n",
    "#     )\n",
    "\n",
    "#     solver_mp_l = ImplicitEquationSolver(\n",
    "#         method='VI',\n",
    "#         proj=projection_l,\n",
    "#         component_slices=component_slices_l,\n",
    "#         tol=solver_opts_ssn.get('tol', 1e-6),\n",
    "#         max_iter=solver_opts_ssn.get('max_iter', 50),\n",
    "#         vi_strict_block_lipschitz=solver_opts_ssn.get('vi_strict_block_lipschitz', False),\n",
    "#         vi_max_block_adjust_iters=solver_opts_ssn.get('vi_max_block_adjust_iters', 10),\n",
    "#     )\n",
    "#     method_mp_l = CompositeMethod(solver=solver_mp_l, A=A_l)\n",
    "\n",
    "#     def rhs_l(t, y):\n",
    "#         n = N_DOFS_l\n",
    "#         v = y[:n]\n",
    "#         u = y[n:2*n]\n",
    "#         vdot = -(KS_l @ u) - (ES_l @ (v - VINF_l))\n",
    "#         udot = v - VINF_l\n",
    "#         sdot = np.abs(v)\n",
    "#         return np.concatenate((vdot, udot, sdot))\n",
    "\n",
    "#     return dict(\n",
    "#         fault=fault_local, KS=KS_l, ES=ES_l, SIGMA_N=SIGMA_N_l, VINF=VINF_l,\n",
    "#         N_DOFS=N_DOFS_l, A=A_l, component_slices=component_slices_l,\n",
    "#         con_force=con_force_l, y0=y0_l, Uintc=Uintc_l, method_mp=method_mp_l,\n",
    "#         rhs=rhs_l, DC_scaled=DC_scaled, solver_opts_ssn=solver_opts_ssn\n",
    "#     )\n",
    "\n",
    "# # ============================\n",
    "# # PI baseline runner\n",
    "# # ============================\n",
    "# def adaptive_baseline_for_grid(grid, adaptive_opts, n_runs=N_EVAL_RUNS):\n",
    "#     projection_opts_nb = dict(\n",
    "#         con_force_func=grid['con_force'],\n",
    "#         rhok=np.ones(grid['N_DOFS'], dtype=float),\n",
    "#         component_slices=grid['component_slices'],\n",
    "#         constraint_indices=list(range(grid['N_DOFS'])),\n",
    "#         use_numba=True,\n",
    "#     )\n",
    "#     solver_opts = grid.get('solver_opts_ssn')\n",
    "\n",
    "#     local_adapt = dict(adaptive_opts)\n",
    "\n",
    "#     runtimes_b = []\n",
    "#     success_runs = []\n",
    "#     slip_end_list = []\n",
    "#     t_store = None\n",
    "#     y_store = None\n",
    "\n",
    "#     for run_idx in range(n_runs):\n",
    "#         start_b = time.time()\n",
    "#         try:\n",
    "#             t_vals_b, y_vals_b, h_vals_b, fk_vals_b, solver_info_b = solve_nivp.solve_ivp_ns(\n",
    "#                 fun=grid['rhs'], t_span=t_span, y0=grid['y0'],\n",
    "#                 method='composite', projection='coulomb', solver='VI',\n",
    "#                 projection_opts=projection_opts_nb, solver_opts=solver_opts,\n",
    "#                 adaptive=True, adaptive_opts=local_adapt,\n",
    "#                 h0=local_adapt.get('h0', 5e-3),\n",
    "#                 component_slices=grid['component_slices'],\n",
    "#                 verbose=False, A=grid['A'],\n",
    "#             )\n",
    "#             wall_b = time.time() - start_b\n",
    "\n",
    "#             success = (len(t_vals_b) > 0 and t_vals_b[-1] >= 0.999 * t_span[1])\n",
    "#             success_runs.append(success)\n",
    "#             runtimes_b.append(wall_b)\n",
    "\n",
    "#             n = grid['N_DOFS']\n",
    "#             s_final_block = y_vals_b[-1, 2*n:3*n]\n",
    "#             s_end_mean = float(np.mean(s_final_block))\n",
    "#             slip_end_list.append(s_end_mean)\n",
    "\n",
    "#             if run_idx == 0:\n",
    "#                 t_store = t_vals_b\n",
    "#                 y_store = y_vals_b\n",
    "\n",
    "#             print(f\"  PI baseline run {run_idx}: {wall_b:.3f}s, success={success}\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"  PI baseline run {run_idx} failed: {e}\")\n",
    "#             success_runs.append(False)\n",
    "\n",
    "#     if not any(success_runs):\n",
    "#         print(\"  WARNING: PI baseline failed all runs\")\n",
    "#         return None\n",
    "\n",
    "#     runtimes_b = [r for r in runtimes_b if np.isfinite(r)]\n",
    "\n",
    "#     n = grid['N_DOFS']\n",
    "#     v_b = np.mean(y_store[:, :n], axis=1)\n",
    "#     u_b = np.mean(y_store[:, n:2*n], axis=1)\n",
    "\n",
    "#     return dict(\n",
    "#         times=np.asarray(t_store, dtype=float) if t_store is not None else np.array([], dtype=float),\n",
    "#         mean_v=np.asarray(v_b, dtype=float),\n",
    "#         mean_u=np.asarray(u_b, dtype=float),\n",
    "#         runtime_s=float(np.mean(runtimes_b)) if runtimes_b else float(\"nan\"),\n",
    "#         runtime_std=float(np.std(runtimes_b)) if len(runtimes_b) > 1 else 0.0,\n",
    "#         converged=True,\n",
    "#         success_rate=sum(success_runs) / len(success_runs),\n",
    "#         s_end_mean=float(np.mean([s for s in slip_end_list if np.isfinite(s)])) if slip_end_list else float(\"nan\"),\n",
    "#     )\n",
    "\n",
    "# # ============================\n",
    "# # RL evaluation (TD3 only)\n",
    "# # ============================\n",
    "# def make_obs_reward(alpha_value, grid, v_max_override):\n",
    "#     def build_reward_fn(alpha_value: float):\n",
    "#         alpha_value = float(alpha_value)\n",
    "#         def my_reward_fn(solver_perf, dt_attempt, xk, env):\n",
    "#             (runtime_inc, dts, err_LO, err_l1, err_HI, E, sLO, sl1, sHI, kLO, kL1, kHI) = solver_perf\n",
    "#             dt_range = env.dt_max - env.dt_min\n",
    "#             if dts == 0.0:\n",
    "#                 dt_norm_attempt = (dt_attempt - env.dt_min) / dt_range\n",
    "#                 return -dt_norm_attempt\n",
    "#             dt_norm = (dts - env.dt_min) / dt_range\n",
    "#             S1 = float(np.clip(dt_norm, 0.0, 1.0))\n",
    "#             if not hasattr(env, 'runtime_history'): env.runtime_history = []\n",
    "#             env.runtime_history.append(float(runtime_inc))\n",
    "#             if not hasattr(env, 'max_runtime_seen'): env.max_runtime_seen = float(runtime_inc)\n",
    "#             else: env.max_runtime_seen = max(env.max_runtime_seen, float(runtime_inc))\n",
    "#             if not hasattr(env, 'min_runtime_seen'): env.min_runtime_seen = float(runtime_inc)\n",
    "#             else: env.min_runtime_seen = min(env.min_runtime_seen, float(runtime_inc))\n",
    "#             rt_min = float(env.min_runtime_seen); rt_max = max(rt_min, float(env.max_runtime_seen))\n",
    "#             denom = max(1e-8, rt_max - rt_min)\n",
    "#             S2 = 1 - (float(runtime_inc) - rt_min) / denom\n",
    "#             A = float(np.exp(-alpha_value * E))\n",
    "#             return S1 * S2 * A\n",
    "#         return my_reward_fn\n",
    "\n",
    "#     reward_fn = build_reward_fn(alpha_value)\n",
    "\n",
    "#     def obs_fn(dt_attempt, converged, xk, solver_perf, fk=None, env=None):\n",
    "#         assert env is not None\n",
    "#         n = grid['N_DOFS']\n",
    "#         v = xk[:n]\n",
    "#         u = xk[n:2*n]\n",
    "#         avg_v = float(np.mean(v))\n",
    "#         ks_u = grid['KS'] @ u\n",
    "#         E_int = 0.5 * float(np.dot(u, ks_u))\n",
    "#         E_norm = float(E_int / grid['Uintc']) if grid['Uintc'] > 0 else float(E_int)\n",
    "#         if solver_perf is not None:\n",
    "#             dts = solver_perf[1]\n",
    "#             conv_flag_bipolar = 1.0 if dts > 0.0 else -1.0\n",
    "#         else:\n",
    "#             conv_flag_bipolar = -1.0\n",
    "\n",
    "#         if dt_attempt is None or solver_perf is None:\n",
    "#             dt_norm_default = 0.5\n",
    "#             conv_flag = 0.0 if converged is None else float(converged)\n",
    "#             return np.array([\n",
    "#                 E_norm,\n",
    "#                 avg_v / (v_max_override / grid['fault'].Vscale),\n",
    "#                 (env.iter_error / 4.0) if hasattr(env, 'iter_error') else 0.0,\n",
    "#                 conv_flag, 0.0,\n",
    "#                 conv_flag_bipolar * dt_norm_default,\n",
    "#             ], dtype=np.float64)\n",
    "\n",
    "#         (runtime_inc, dts, err_LO, err_l1, err_HI, E_global, sLO, sl1, sHI, kLO, kL1, kHI) = solver_perf\n",
    "#         dt_range = env.dt_max - env.dt_min\n",
    "#         dt_norm = (dt_attempt - env.dt_min) / dt_range if dt_range > 0 else 0.0\n",
    "#         dt_norm = float(np.clip(dt_norm, 0.0, 1.0))\n",
    "#         conv_flag = float(converged) if converged is not None else 0.0\n",
    "#         acc = 1.0 / (1.0 + E_global) if conv_flag else 0.0\n",
    "\n",
    "#         return np.array([\n",
    "#             E_norm,\n",
    "#             avg_v / (v_max_override / grid['fault'].Vscale),\n",
    "#             (env.iter_error / 4.0) if hasattr(env, 'iter_error') else 0.0,\n",
    "#             conv_flag, acc,\n",
    "#             conv_flag_bipolar * dt_norm,\n",
    "#         ], dtype=np.float64)\n",
    "\n",
    "#     return reward_fn, obs_fn\n",
    "\n",
    "# def rollout_policy_on_vecenv(model, vec_env, run_name=\"(unnamed)\"):\n",
    "#     vec_env.training = False\n",
    "#     vec_env.norm_obs = False\n",
    "#     vec_env.norm_reward = False\n",
    "#     base_env = vec_env.venv.envs[0] if hasattr(vec_env, \"venv\") else vec_env.envs[0]\n",
    "\n",
    "#     obs = vec_env.reset()\n",
    "#     times, mean_vel, mean_slip = [], [], []\n",
    "#     last_t, repeat_count, broke_stuck = None, 0, False\n",
    "#     last_converged, last_xk = None, None\n",
    "#     start = time.time(); done = False\n",
    "\n",
    "#     while not done:\n",
    "#         action, _ = model.predict(obs, deterministic=True)\n",
    "#         obs, rewards, dones, infos = vec_env.step(action)\n",
    "#         done = bool(dones[0]); info = infos[0]\n",
    "#         t_now = info.get(\"t_k1\", np.nan)\n",
    "#         xk = info.get(\"xk\", None)\n",
    "#         if xk is not None: last_xk = xk\n",
    "#         if \"converged\" in info: last_converged = info.get(\"converged\")\n",
    "\n",
    "#         current_t = None if t_now is None or np.isnan(t_now) else float(t_now)\n",
    "#         if current_t is not None:\n",
    "#             if last_t is not None and np.isclose(current_t, last_t, rtol=0.0, atol=1e-12):\n",
    "#                 repeat_count += 1\n",
    "#                 if repeat_count >= 10:\n",
    "#                     print(f\"[{run_name}] WARNING: time stuck at {current_t}\")\n",
    "#                     broke_stuck = True; break\n",
    "#             else:\n",
    "#                 repeat_count = 0\n",
    "#             last_t = current_t\n",
    "\n",
    "#         if xk is not None and current_t is not None:\n",
    "#             n = xk.shape[0] // 3\n",
    "#             v_block = xk[:n]; u_block = xk[n:2*n]\n",
    "#             times.append(current_t)\n",
    "#             mean_vel.append(float(np.mean(v_block)))\n",
    "#             mean_slip.append(float(np.mean(u_block)))\n",
    "\n",
    "#     wall_time = time.time() - start\n",
    "#     if last_converged is not None:\n",
    "#         success = bool(last_converged)\n",
    "#     else:\n",
    "#         success = (len(times) > 0 and hasattr(base_env, 'tnmax') and times[-1] >= 0.999 * float(base_env.tnmax)) and not broke_stuck\n",
    "\n",
    "#     if last_xk is not None:\n",
    "#         n = last_xk.shape[0] // 3\n",
    "#         s_block = last_xk[2*n:3*n]\n",
    "#         s_end_mean = float(np.mean(s_block))\n",
    "#     else:\n",
    "#         s_end_mean = np.nan\n",
    "\n",
    "#     print(f\"[{run_name}] Done: wall_time={wall_time:.3f}s, steps={len(times)}, final_t={(times[-1] if len(times)>0 else float('nan')):.3f}, s_end={s_end_mean:.6e}\")\n",
    "#     return (np.asarray(times, float), np.asarray(mean_vel, float), np.asarray(mean_slip, float),\n",
    "#             float(wall_time), bool(success), s_end_mean)\n",
    "\n",
    "# def evaluate_td3_for_grid(grid, spec, alpha_dirs):\n",
    "#     v_max_override = spec['v_max']; grid_label = spec['label']\n",
    "#     local_dt_min = 1e-6\n",
    "#     local_dt_max = 30 / 5 * grid['fault'].second / grid['fault'].Tscale\n",
    "#     series = {}\n",
    "\n",
    "#     for alpha_dir in alpha_dirs:\n",
    "#         algo_dirs = sorted([p for p in alpha_dir.iterdir() if p.is_dir()])\n",
    "#         for algo_dir in algo_dirs:\n",
    "#             algo_name = algo_dir.name\n",
    "#             if algo_name != \"TD3\":\n",
    "#                 continue  # skip everything except TD3\n",
    "\n",
    "#             meta_path = algo_dir / \"metadata.json\"\n",
    "#             model_path = algo_dir / \"model.zip\"\n",
    "#             if not model_path.exists():\n",
    "#                 model_path = algo_dir / \"model\"\n",
    "#             vecnorm_path = algo_dir / \"vec_norm.pkl\"\n",
    "#             if not meta_path.exists() or not model_path.exists():\n",
    "#                 continue\n",
    "\n",
    "#             with open(meta_path, \"r\", encoding=\"utf-8\") as fh:\n",
    "#                 meta = json.load(fh)\n",
    "#             alpha_value = float(meta.get(\"alpha\", np.nan))\n",
    "#             if not np.isfinite(alpha_value):\n",
    "#                 continue\n",
    "\n",
    "#             reward_fn, obs_fn = make_obs_reward(alpha_value, grid, v_max_override)\n",
    "\n",
    "#             def make_env():\n",
    "#                 obs_space = spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float64)\n",
    "#                 return AdaptiveStepperEnv(\n",
    "#                     system=grid['rhs'], dt0=local_dt_min, t0=t_span[0], x0=grid['y0'], tnmax=t_span[1],\n",
    "#                     dt_min=local_dt_min, dt_max=local_dt_max, nparams=(1e-6, 100),\n",
    "#                     integrator=grid['method_mp'], component_slices=grid['component_slices'],\n",
    "#                     reward_fn=reward_fn, obs_fn=obs_fn, obs_space=obs_space, verbose=False, alpha=alpha_value,\n",
    "#                 )\n",
    "\n",
    "#             model = TD3.load(str(model_path), device=\"cpu\")\n",
    "\n",
    "#             runtimes, success_runs, slip_end_list = [], [], []\n",
    "#             t_store = v_store = u_store = None\n",
    "#             key = f\"{algo_name} alpha={alpha_value:g}\"\n",
    "#             print(f\"\\nEvaluating {key} on grid {grid_label}...\")\n",
    "\n",
    "#             for run_idx in range(N_EVAL_RUNS):\n",
    "#                 base_vec_env = DummyVecEnv([make_env])\n",
    "#                 if vecnorm_path.exists():\n",
    "#                     vec_env = VecNormalize.load(str(vecnorm_path), base_vec_env)\n",
    "#                     vec_env.training = False; vec_env.norm_obs = False; vec_env.norm_reward = False\n",
    "#                 else:\n",
    "#                     vec_env = base_vec_env\n",
    "#                 model.set_env(vec_env)\n",
    "\n",
    "#                 t_arr, v_mean, u_mean, wall, success, s_end_mean = rollout_policy_on_vecenv(\n",
    "#                     model, vec_env, run_name=f\"{key} run{run_idx}\"\n",
    "#                 )\n",
    "#                 runtimes.append(wall); success_runs.append(success); slip_end_list.append(s_end_mean)\n",
    "#                 if run_idx == 0: t_store, v_store, u_store = t_arr, v_mean, u_mean\n",
    "#                 vec_env.close()\n",
    "#                 if run_idx == 0 and not success: break\n",
    "\n",
    "#             if any(success_runs):\n",
    "#                 times_arr = np.asarray(t_store, float) if t_store is not None else np.array([], float)\n",
    "#                 mean_v_arr = np.asarray(v_store, float) if v_store is not None else np.array([], float)\n",
    "#                 mean_u_arr = np.asarray(u_store, float) if u_store is not None else np.array([], float)\n",
    "#                 finite_slips = [s for s in slip_end_list if np.isfinite(s)]\n",
    "#                 s_end_mean = float(np.mean(finite_slips)) if finite_slips else float(\"nan\")\n",
    "#                 series[key] = dict(\n",
    "#                     times=times_arr, mean_v=mean_v_arr, mean_u=mean_u_arr,\n",
    "#                     runtime_s=float(np.mean(runtimes)), runtime_std=float(np.std(runtimes)) if len(runtimes) > 1 else 0.0,\n",
    "#                     converged=True, success_rate=sum(success_runs)/len(success_runs), s_end_mean=s_end_mean,\n",
    "#                 )\n",
    "#                 print(f\"  Success rate: {series[key]['success_rate']:.1%}, Runtime: {series[key]['runtime_s']:.3f}s Â± {series[key]['runtime_std']:.3f}s\")\n",
    "#     return series\n",
    "\n",
    "# # ============================\n",
    "# # Serialization helper (scales to physical units like your original)\n",
    "# # ============================\n",
    "# def save_series_json(series: dict, grid: dict, out_path: Path):\n",
    "#     scale_t = grid['fault'].Tscale / grid['fault'].second\n",
    "#     scale_v = grid['fault'].Vscale\n",
    "#     scale_u = grid['fault'].Dscale\n",
    "\n",
    "#     json_series = {}\n",
    "#     for key, data in series.items():\n",
    "#         times = np.asarray(data.get(\"times\", []), float)\n",
    "#         mean_v = np.asarray(data.get(\"mean_v\", []), float)\n",
    "#         mean_u = np.asarray(data.get(\"mean_u\", []), float)\n",
    "#         s_end_mean = data.get(\"s_end_mean\", None)\n",
    "\n",
    "#         json_series[key] = {\n",
    "#             \"times\": (times * scale_t).tolist() if times.size else [],\n",
    "#             \"mean_v\": (mean_v * scale_v).tolist() if mean_v.size else [],\n",
    "#             \"mean_u\": (mean_u * scale_u).tolist() if mean_u.size else [],\n",
    "#             \"runtime_s\": data.get(\"runtime_s\", float(\"nan\")),\n",
    "#             \"runtime_std\": data.get(\"runtime_std\", 0.0),\n",
    "#             \"converged\": data.get(\"converged\", False),\n",
    "#             \"success_rate\": data.get(\"success_rate\", 1.0),\n",
    "#             \"s_end_mean\": (float(s_end_mean) * scale_u) if (s_end_mean is not None and np.isfinite(s_end_mean)) else None,\n",
    "#         }\n",
    "\n",
    "#     with open(out_path, \"w\") as f:\n",
    "#         json.dump(json_series, f, indent=2)\n",
    "#     print(f\"Saved results to {out_path}\")\n",
    "\n",
    "# # ============================\n",
    "# # Main\n",
    "# # ============================\n",
    "# if __name__ == \"__main__\":\n",
    "#     assert MODE in {\"PI_ONLY\", \"RL_TD3_ONLY\"}, \"MODE must be 'PI_ONLY' or 'RL_TD3_ONLY'\"\n",
    "\n",
    "#     # PI controller settings (unchanged)\n",
    "#     adaptive_opts_pi = dict(\n",
    "#         h0=5e-3, h_min=1e-7, h_down=0.6, h_up=1.8, method_order=1,mode=\"ratio\",\n",
    "#         controller=\"H211b\", b_param=4.0, skip_error_indices=[],\n",
    "#     )\n",
    "\n",
    "#     # If RL mode: find alpha_* dirs\n",
    "#     alpha_dirs = []\n",
    "#     if MODE == \"RL_TD3_ONLY\":\n",
    "#         alpha_dirs = sorted([p for p in RUN_ROOT.glob(\"alpha_*\") if p.is_dir()])\n",
    "#         if not alpha_dirs:\n",
    "#             raise FileNotFoundError(f\"No trained runs found under {RUN_ROOT}\")\n",
    "\n",
    "#     # Per-grid evaluation\n",
    "#     for spec in GRID_SPECS:\n",
    "#         print(f\"\\n{'='*60}\\nEvaluating on {spec['label']} grid (v_max={spec['v_max']})\\n{'='*60}\")\n",
    "#         grid = build_fault_and_solver(spec[\"Nz\"], spec[\"Nx\"])\n",
    "\n",
    "#         series = {}\n",
    "\n",
    "#         if MODE == \"PI_ONLY\":\n",
    "#             print(f\"\\nRunning PI baseline on {spec['label']}...\")\n",
    "#             baseline = adaptive_baseline_for_grid(grid, adaptive_opts_pi)\n",
    "#             if baseline is not None:\n",
    "#                 # save only PI under PI_baseline key so downstream code remains compatible\n",
    "#                 series[\"PI_baseline\"] = baseline\n",
    "#             out_file = Path(f\"eval_results_PI_{spec['label']}.json\")\n",
    "#             save_series_json(series, grid, out_file)\n",
    "\n",
    "#         elif MODE == \"RL_TD3_ONLY\":\n",
    "#             # Evaluate TD3 only (no PI)\n",
    "#             series = evaluate_td3_for_grid(grid, spec, alpha_dirs)\n",
    "#             out_file = Path(f\"eval_results_TD3_{spec['label']}.json\")\n",
    "#             save_series_json(series, grid, out_file)\n",
    "\n",
    "#     print(\"\\n\" + \"=\"*60 + f\"\\nEVALUATION COMPLETE ({MODE})\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c4373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import time\n",
    "# import json\n",
    "# from pathlib import Path\n",
    "# import matplotlib.pyplot as plt\n",
    "# import scipy.sparse as sp\n",
    "\n",
    "# from gymnasium import spaces\n",
    "# from stable_baselines3 import TD3\n",
    "# from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "# from sb3_contrib import TQC\n",
    "\n",
    "# from solve_nivp.projections import CoulombProjection\n",
    "# from solve_nivp.nonlinear_solvers import ImplicitEquationSolver\n",
    "# from solve_nivp.integrations import CompositeMethod\n",
    "# from solve_nivp.rl.env import AdaptiveStepperEnv\n",
    "# import solve_nivp\n",
    "# from plants.faults import strikeslip\n",
    "\n",
    "# # ============================\n",
    "# # Select what to run\n",
    "# # ============================\n",
    "# MODE = \"RESUMED_ONLY\"   # options: \"PI_ONLY\", \"RL_TD3_ONLY\", \"RESUMED_ONLY\"\n",
    "# RUN_ROOT = Path(\"TEST_RL_ALPHA_SWEEP\").resolve()\n",
    "# N_EVAL_RUNS = 5\n",
    "\n",
    "# # ----------------------------\n",
    "# # Time span for simulations\n",
    "# # ----------------------------\n",
    "# # t_span = [0.0, 30.0]\n",
    "\n",
    "# # ============================\n",
    "# # Grids and model physics\n",
    "# # ============================\n",
    "# GRID_SPECS = [\n",
    "#     {\"Nz\": 25, \"Nx\": 25, \"label\": \"25x25\", \"v_max\": 0.07},\n",
    "#     {\"Nz\": 50, \"Nx\": 50, \"label\": \"50x50\", \"v_max\": 0.07},\n",
    "#     {\"Nz\": 75, \"Nx\": 75, \"label\": \"75x75\", \"v_max\": 0.07},\n",
    "#     # {\"Nz\": 100, \"Nx\": 100, \"label\": \"100x100\", \"v_max\": 0.07},\n",
    "# ]\n",
    "\n",
    "# DMU = -0.1\n",
    "# DC = 100.0\n",
    "# MU_RES = 0.5\n",
    "\n",
    "# MODEL_CLASS_MAP = {\n",
    "#     \"TQC\": TQC,\n",
    "#     \"TD3\": TD3,\n",
    "# }\n",
    "\n",
    "# # ============================\n",
    "# # Build fault and solver per grid\n",
    "# # ============================\n",
    "# def build_fault_and_solver(Nz, Nx):\n",
    "#     fault_local = strikeslip.qs_strikeslip_fault(\n",
    "#         zdepth=3, xlength=3, Nz=Nz, Nx=Nx,\n",
    "#         G=30000., rho=2.5e-3, zeta=0.8/3,\n",
    "#         Ks_path=\"./Data/\", gamma_s=25., gamma_w=10.,\n",
    "#         sigma_ref=100., depth_ini=0., vinf=3.171e-10,\n",
    "#         Dmu_estimate=.5,\n",
    "#     )\n",
    "#     MA_l, KS_l, ES_l, SIGMA_N_l, VINF_raw_l = fault_local.get_plant()\n",
    "#     N_DOFS_l = fault_local.N\n",
    "#     VINF_l = VINF_raw_l * np.ones(N_DOFS_l) * 0\n",
    "\n",
    "#     I_N = sp.eye(N_DOFS_l, format='csr')\n",
    "#     A_l = sp.block_diag([sp.csr_matrix(MA_l), I_N, I_N], format='csr')\n",
    "\n",
    "#     component_slices_l = [\n",
    "#         slice(0, N_DOFS_l),\n",
    "#         slice(N_DOFS_l, 2 * N_DOFS_l),\n",
    "#         slice(2 * N_DOFS_l, 3 * N_DOFS_l),\n",
    "#     ]\n",
    "\n",
    "#     DC_scaled = DC / fault_local.Dscale\n",
    "\n",
    "#     def con_force_l(state, fk=None):\n",
    "#         n = N_DOFS_l\n",
    "#         slip_hist = state[2*n:3*n]\n",
    "#         mu_vals = MU_RES * (1 - DMU / MU_RES * np.exp(-slip_hist / DC_scaled))\n",
    "#         out = np.zeros_like(state)\n",
    "#         out[:n] = mu_vals * SIGMA_N_l\n",
    "#         return out\n",
    "\n",
    "#     y0_l = np.zeros(3 * N_DOFS_l)\n",
    "#     friction_force0 = con_force_l(y0_l)\n",
    "#     uc = -np.linalg.solve(KS_l, friction_force0[:N_DOFS_l])\n",
    "#     y0_l[N_DOFS_l:2*N_DOFS_l] = uc * (1.0 + 1e-5)\n",
    "#     Uintc_l = 0.5 * float(uc @ (KS_l @ uc))\n",
    "\n",
    "#     projection_l = CoulombProjection(\n",
    "#         con_force_func=con_force_l,\n",
    "#         rhok=np.ones(N_DOFS_l, dtype=float),\n",
    "#         component_slices=component_slices_l,\n",
    "#         constraint_indices=list(range(N_DOFS_l)),\n",
    "#         use_numba=True,\n",
    "#     )\n",
    "\n",
    "#     solver_opts_ssn = dict(\n",
    "#         tol=1e-8,\n",
    "#         max_iter=200,\n",
    "#         vi_strict_block_lipschitz=False,\n",
    "#         vi_max_block_adjust_iters=5,\n",
    "#         globalization='line_search',\n",
    "#     )\n",
    "\n",
    "#     solver_mp_l = ImplicitEquationSolver(\n",
    "#         method='VI',\n",
    "#         proj=projection_l,\n",
    "#         component_slices=component_slices_l,\n",
    "#         tol=solver_opts_ssn.get('tol', 1e-6),\n",
    "#         max_iter=solver_opts_ssn.get('max_iter', 50),\n",
    "#         vi_strict_block_lipschitz=solver_opts_ssn.get('vi_strict_block_lipschitz', False),\n",
    "#         vi_max_block_adjust_iters=solver_opts_ssn.get('vi_max_block_adjust_iters', 10),\n",
    "#     )\n",
    "#     method_mp_l = CompositeMethod(solver=solver_mp_l, A=A_l)\n",
    "\n",
    "#     def rhs_l(t, y):\n",
    "#         n = N_DOFS_l\n",
    "#         v = y[:n]\n",
    "#         u = y[n:2*n]\n",
    "#         vdot = -(KS_l @ u) - (ES_l @ (v - VINF_l))\n",
    "#         udot = v - VINF_l\n",
    "#         sdot = np.abs(v)\n",
    "#         return np.concatenate((vdot, udot, sdot))\n",
    "\n",
    "#     return dict(\n",
    "#         fault=fault_local, KS=KS_l, ES=ES_l, SIGMA_N=SIGMA_N_l, VINF=VINF_l,\n",
    "#         N_DOFS=N_DOFS_l, A=A_l, component_slices=component_slices_l,\n",
    "#         con_force=con_force_l, y0=y0_l, Uintc=Uintc_l, method_mp=method_mp_l,\n",
    "#         rhs=rhs_l, DC_scaled=DC_scaled, solver_opts_ssn=solver_opts_ssn\n",
    "#     )\n",
    "\n",
    "# # ============================\n",
    "# # PI baseline runner\n",
    "# # ============================\n",
    "# def adaptive_baseline_for_grid(grid, adaptive_opts, n_runs=N_EVAL_RUNS):\n",
    "#     projection_opts_nb = dict(\n",
    "#         con_force_func=grid['con_force'],\n",
    "#         rhok=np.ones(grid['N_DOFS'], dtype=float),\n",
    "#         component_slices=grid['component_slices'],\n",
    "#         constraint_indices=list(range(grid['N_DOFS'])),\n",
    "#         use_numba=True,\n",
    "#     )\n",
    "#     solver_opts = grid.get('solver_opts_ssn')\n",
    "\n",
    "#     local_adapt = dict(adaptive_opts)\n",
    "\n",
    "#     runtimes_b = []\n",
    "#     success_runs = []\n",
    "#     slip_end_list = []\n",
    "#     t_store = None\n",
    "#     y_store = None\n",
    "\n",
    "#     for run_idx in range(n_runs):\n",
    "#         start_b = time.time()\n",
    "#         try:\n",
    "#             t_vals_b, y_vals_b, h_vals_b, fk_vals_b, solver_info_b = solve_nivp.solve_ivp_ns(\n",
    "#                 fun=grid['rhs'], t_span=t_span, y0=grid['y0'],\n",
    "#                 method='composite', projection='coulomb', solver='VI',\n",
    "#                 projection_opts=projection_opts_nb, solver_opts=solver_opts,\n",
    "#                 adaptive=True, adaptive_opts=local_adapt,\n",
    "#                 h0=local_adapt.get('h0', 5e-3),\n",
    "#                 component_slices=grid['component_slices'],\n",
    "#                 verbose=False, A=grid['A'],\n",
    "#             )\n",
    "#             wall_b = time.time() - start_b\n",
    "\n",
    "#             success = (len(t_vals_b) > 0 and t_vals_b[-1] >= 0.999 * t_span[1])\n",
    "#             success_runs.append(success)\n",
    "#             runtimes_b.append(wall_b)\n",
    "\n",
    "#             n = grid['N_DOFS']\n",
    "#             s_final_block = y_vals_b[-1, 2*n:3*n]\n",
    "#             s_end_mean = float(np.mean(s_final_block))\n",
    "#             slip_end_list.append(s_end_mean)\n",
    "\n",
    "#             if run_idx == 0:\n",
    "#                 t_store = t_vals_b\n",
    "#                 y_store = y_vals_b\n",
    "\n",
    "#             print(f\"  PI baseline run {run_idx}: {wall_b:.3f}s, success={success}\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"  PI baseline run {run_idx} failed: {e}\")\n",
    "#             success_runs.append(False)\n",
    "\n",
    "#     if not any(success_runs):\n",
    "#         print(\"  WARNING: PI baseline failed all runs\")\n",
    "#         return None\n",
    "\n",
    "#     runtimes_b = [r for r in runtimes_b if np.isfinite(r)]\n",
    "\n",
    "#     n = grid['N_DOFS']\n",
    "#     v_b = np.mean(y_store[:, :n], axis=1)\n",
    "#     u_b = np.mean(y_store[:, n:2*n], axis=1)\n",
    "\n",
    "#     return dict(\n",
    "#         times=np.asarray(t_store, dtype=float) if t_store is not None else np.array([], dtype=float),\n",
    "#         mean_v=np.asarray(v_b, dtype=float),\n",
    "#         mean_u=np.asarray(u_b, dtype=float),\n",
    "#         runtime_s=float(np.mean(runtimes_b)) if runtimes_b else float(\"nan\"),\n",
    "#         runtime_std=float(np.std(runtimes_b)) if len(runtimes_b) > 1 else 0.0,\n",
    "#         converged=True,\n",
    "#         success_rate=sum(success_runs) / len(success_runs),\n",
    "#         s_end_mean=float(np.mean([s for s in slip_end_list if np.isfinite(s)])) if slip_end_list else float(\"nan\"),\n",
    "#     )\n",
    "\n",
    "# # ============================\n",
    "# # RL obs + reward builders\n",
    "# # ============================\n",
    "# def make_obs_reward(alpha_value, grid, v_max_override):\n",
    "#     def build_reward_fn(alpha_value: float):\n",
    "#         alpha_value = float(alpha_value)\n",
    "#         def my_reward_fn(solver_perf, dt_attempt, xk, env):\n",
    "#             (runtime_inc, dts, err_LO, err_l1, err_HI, E, sLO, sl1, sHI, kLO, kL1, kHI) = solver_perf\n",
    "#             dt_range = env.dt_max - env.dt_min\n",
    "#             if dts == 0.0:\n",
    "#                 dt_norm_attempt = (dt_attempt - env.dt_min) / dt_range\n",
    "#                 return -dt_norm_attempt\n",
    "#             dt_norm = (dts - env.dt_min) / dt_range\n",
    "#             S1 = float(np.clip(dt_norm, 0.0, 1.0))\n",
    "#             if not hasattr(env, 'runtime_history'):\n",
    "#                 env.runtime_history = []\n",
    "#             env.runtime_history.append(float(runtime_inc))\n",
    "#             if not hasattr(env, 'max_runtime_seen'):\n",
    "#                 env.max_runtime_seen = float(runtime_inc)\n",
    "#             else:\n",
    "#                 env.max_runtime_seen = max(env.max_runtime_seen, float(runtime_inc))\n",
    "#             if not hasattr(env, 'min_runtime_seen'):\n",
    "#                 env.min_runtime_seen = float(runtime_inc)\n",
    "#             else:\n",
    "#                 env.min_runtime_seen = min(env.min_runtime_seen, float(runtime_inc))\n",
    "#             rt_min = float(env.min_runtime_seen)\n",
    "#             rt_max = max(rt_min, float(env.max_runtime_seen))\n",
    "#             denom = max(1e-8, rt_max - rt_min)\n",
    "#             S2 = 1 - (float(runtime_inc) - rt_min) / denom\n",
    "#             A = float(np.exp(-alpha_value * E))\n",
    "#             return S1 * S2 * A\n",
    "#         return my_reward_fn\n",
    "\n",
    "#     reward_fn = build_reward_fn(alpha_value)\n",
    "\n",
    "#     def obs_fn(dt_attempt, converged, xk, solver_perf, fk=None, env=None):\n",
    "#         assert env is not None\n",
    "#         n = grid['N_DOFS']\n",
    "#         v = xk[:n]\n",
    "#         u = xk[n:2*n]\n",
    "#         avg_v = float(np.mean(v))\n",
    "#         ks_u = grid['KS'] @ u\n",
    "#         E_int = 0.5 * float(np.dot(u, ks_u))\n",
    "#         E_norm = float(E_int / grid['Uintc']) if grid['Uintc'] > 0 else float(E_int)\n",
    "#         if solver_perf is not None:\n",
    "#             dts = solver_perf[1]\n",
    "#             conv_flag_bipolar = 1.0 if dts > 0.0 else -1.0\n",
    "#         else:\n",
    "#             conv_flag_bipolar = -1.0\n",
    "\n",
    "#         if dt_attempt is None or solver_perf is None:\n",
    "#             dt_norm_default = 0.5\n",
    "#             conv_flag = 0.0 if converged is None else float(converged)\n",
    "#             return np.array([\n",
    "#                 E_norm,\n",
    "#                 avg_v / (v_max_override / grid['fault'].Vscale),\n",
    "#                 (env.iter_error / 4.0) if hasattr(env, 'iter_error') else 0.0,\n",
    "#                 conv_flag, 0.0,\n",
    "#                 conv_flag_bipolar * dt_norm_default,\n",
    "#             ], dtype=np.float64)\n",
    "\n",
    "#         (runtime_inc, dts, err_LO, err_l1, err_HI, E_global, sLO, sl1, sHI, kLO, kL1, kHI) = solver_perf\n",
    "#         dt_range = env.dt_max - env.dt_min\n",
    "#         dt_norm = (dt_attempt - env.dt_min) / dt_range if dt_range > 0 else 0.0\n",
    "#         dt_norm = float(np.clip(dt_norm, 0.0, 1.0))\n",
    "#         conv_flag = float(converged) if converged is not None else 0.0\n",
    "#         acc = 1.0 / (1.0 + E_global) if conv_flag else 0.0\n",
    "\n",
    "#         return np.array([\n",
    "#             E_norm,\n",
    "#             avg_v / (v_max_override / grid['fault'].Vscale),\n",
    "#             (env.iter_error / 4.0) if hasattr(env, 'iter_error') else 0.0,\n",
    "#             conv_flag, acc,\n",
    "#             conv_flag_bipolar * dt_norm,\n",
    "#         ], dtype=np.float64)\n",
    "\n",
    "#     return reward_fn, obs_fn\n",
    "\n",
    "# # ============================\n",
    "# # Rollout helper\n",
    "# # ============================\n",
    "# def rollout_policy_on_vecenv(model, vec_env, run_name=\"(unnamed)\"):\n",
    "#     vec_env.training = False\n",
    "#     vec_env.norm_obs = False\n",
    "#     vec_env.norm_reward = False\n",
    "#     base_env = vec_env.venv.envs[0] if hasattr(vec_env, \"venv\") else vec_env.envs[0]\n",
    "\n",
    "#     obs = vec_env.reset()\n",
    "#     times, mean_vel, mean_slip = [], [], []\n",
    "#     last_t, repeat_count, broke_stuck = None, 0, False\n",
    "#     last_converged, last_xk = None, None\n",
    "#     start = time.time()\n",
    "#     done = False\n",
    "\n",
    "#     while not done:\n",
    "#         action, _ = model.predict(obs, deterministic=True)\n",
    "#         obs, rewards, dones, infos = vec_env.step(action)\n",
    "#         done = bool(dones[0])\n",
    "#         info = infos[0]\n",
    "#         t_now = info.get(\"t_k1\", np.nan)\n",
    "#         xk = info.get(\"xk\", None)\n",
    "#         if xk is not None:\n",
    "#             last_xk = xk\n",
    "#         if \"converged\" in info:\n",
    "#             last_converged = info.get(\"converged\")\n",
    "\n",
    "#         current_t = None if t_now is None or np.isnan(t_now) else float(t_now)\n",
    "#         if current_t is not None:\n",
    "#             if last_t is not None and np.isclose(current_t, last_t, rtol=0.0, atol=1e-12):\n",
    "#                 repeat_count += 1\n",
    "#                 if repeat_count >= 10:\n",
    "#                     print(f\"[{run_name}] WARNING: time stuck at {current_t}\")\n",
    "#                     broke_stuck = True\n",
    "#                     break\n",
    "#             else:\n",
    "#                 repeat_count = 0\n",
    "#             last_t = current_t\n",
    "\n",
    "#         if xk is not None and current_t is not None:\n",
    "#             n = xk.shape[0] // 3\n",
    "#             v_block = xk[:n]\n",
    "#             u_block = xk[n:2*n]\n",
    "#             times.append(current_t)\n",
    "#             mean_vel.append(float(np.mean(v_block)))\n",
    "#             mean_slip.append(float(np.mean(u_block)))\n",
    "\n",
    "#     wall_time = time.time() - start\n",
    "#     if last_converged is not None:\n",
    "#         success = bool(last_converged)\n",
    "#     else:\n",
    "#         success = (len(times) > 0 and hasattr(base_env, 'tnmax') and\n",
    "#                    times[-1] >= 0.999 * float(base_env.tnmax)) and not broke_stuck\n",
    "\n",
    "#     if last_xk is not None:\n",
    "#         n = last_xk.shape[0] // 3\n",
    "#         s_block = last_xk[2*n:3*n]\n",
    "#         s_end_mean = float(np.mean(s_block))\n",
    "#     else:\n",
    "#         s_end_mean = np.nan\n",
    "\n",
    "#     print(f\"[{run_name}] Done: wall_time={wall_time:.3f}s, steps={len(times)}, \"\n",
    "#           f\"final_t={(times[-1] if len(times)>0 else float('nan')):.3f}, s_end={s_end_mean:.6e}\")\n",
    "#     return (np.asarray(times, float), np.asarray(mean_vel, float), np.asarray(mean_slip, float),\n",
    "#             float(wall_time), bool(success), s_end_mean)\n",
    "\n",
    "# # ============================\n",
    "# # TD3 evaluation (original model.zip)\n",
    "# # ============================\n",
    "# def evaluate_td3_for_grid(grid, spec, alpha_dirs):\n",
    "#     v_max_override = spec['v_max']\n",
    "#     grid_label = spec['label']\n",
    "#     local_dt_min = 1e-6\n",
    "#     local_dt_max = 30 / 5 * grid['fault'].second / grid['fault'].Tscale\n",
    "#     series = {}\n",
    "\n",
    "#     for alpha_dir in alpha_dirs:\n",
    "#         algo_dirs = sorted([p for p in alpha_dir.iterdir() if p.is_dir()])\n",
    "#         for algo_dir in algo_dirs:\n",
    "#             algo_name = algo_dir.name\n",
    "#             if algo_name != \"TD3\":\n",
    "#                 continue  # skip everything except TD3\n",
    "\n",
    "#             meta_path = algo_dir / \"metadata.json\"\n",
    "#             model_path = algo_dir / \"model.zip\"\n",
    "#             if not model_path.exists():\n",
    "#                 model_path = algo_dir / \"model\"\n",
    "#             vecnorm_path = algo_dir / \"vec_norm.pkl\"\n",
    "#             if not meta_path.exists() or not model_path.exists():\n",
    "#                 continue\n",
    "\n",
    "#             with open(meta_path, \"r\", encoding=\"utf-8\") as fh:\n",
    "#                 meta = json.load(fh)\n",
    "#             alpha_value = float(meta.get(\"alpha\", np.nan))\n",
    "#             if not np.isfinite(alpha_value):\n",
    "#                 continue\n",
    "\n",
    "#             reward_fn, obs_fn = make_obs_reward(alpha_value, grid, v_max_override)\n",
    "\n",
    "#             def make_env():\n",
    "#                 obs_space = spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float64)\n",
    "#                 return AdaptiveStepperEnv(\n",
    "#                     system=grid['rhs'], dt0=local_dt_min, t0=t_span[0], x0=grid['y0'], tnmax=t_span[1],\n",
    "#                     dt_min=local_dt_min, dt_max=local_dt_max, nparams=(1e-6, 100),\n",
    "#                     integrator=grid['method_mp'], component_slices=grid['component_slices'],\n",
    "#                     reward_fn=reward_fn, obs_fn=obs_fn, obs_space=obs_space, verbose=False, alpha=alpha_value,\n",
    "#                 )\n",
    "\n",
    "#             model = TD3.load(str(model_path), device=\"cpu\")\n",
    "\n",
    "#             runtimes, success_runs, slip_end_list = [], [], []\n",
    "#             t_store = v_store = u_store = None\n",
    "#             key = f\"{algo_name} alpha={alpha_value:g}\"\n",
    "#             print(f\"\\nEvaluating {key} on grid {grid_label}...\")\n",
    "\n",
    "#             for run_idx in range(N_EVAL_RUNS):\n",
    "#                 base_vec_env = DummyVecEnv([make_env])\n",
    "#                 if vecnorm_path.exists():\n",
    "#                     vec_env = VecNormalize.load(str(vecnorm_path), base_vec_env)\n",
    "#                     vec_env.training = False\n",
    "#                     vec_env.norm_obs = False\n",
    "#                     vec_env.norm_reward = False\n",
    "#                 else:\n",
    "#                     vec_env = base_vec_env\n",
    "#                 model.set_env(vec_env)\n",
    "\n",
    "#                 t_arr, v_mean, u_mean, wall, success, s_end_mean = rollout_policy_on_vecenv(\n",
    "#                     model, vec_env, run_name=f\"{key} run{run_idx}\"\n",
    "#                 )\n",
    "#                 runtimes.append(wall)\n",
    "#                 success_runs.append(success)\n",
    "#                 slip_end_list.append(s_end_mean)\n",
    "#                 if run_idx == 0:\n",
    "#                     t_store, v_store, u_store = t_arr, v_mean, u_mean\n",
    "#                 vec_env.close()\n",
    "#                 if run_idx == 0 and not success:\n",
    "#                     break\n",
    "\n",
    "#             if any(success_runs):\n",
    "#                 times_arr = np.asarray(t_store, float) if t_store is not None else np.array([], float)\n",
    "#                 mean_v_arr = np.asarray(v_store, float) if v_store is not None else np.array([], float)\n",
    "#                 mean_u_arr = np.asarray(u_store, float) if u_store is not None else np.array([], float)\n",
    "#                 finite_slips = [s for s in slip_end_list if np.isfinite(s)]\n",
    "#                 s_end_mean = float(np.mean(finite_slips)) if finite_slips else float(\"nan\")\n",
    "#                 series[key] = dict(\n",
    "#                     times=times_arr, mean_v=mean_v_arr, mean_u=mean_u_arr,\n",
    "#                     runtime_s=float(np.mean(runtimes)),\n",
    "#                     runtime_std=float(np.std(runtimes)) if len(runtimes) > 1 else 0.0,\n",
    "#                     converged=True,\n",
    "#                     success_rate=sum(success_runs)/len(success_runs),\n",
    "#                     s_end_mean=s_end_mean,\n",
    "#                 )\n",
    "#                 print(f\"  Success rate: {series[key]['success_rate']:.1%}, \"\n",
    "#                       f\"Runtime: {series[key]['runtime_s']:.3f}s Â± {series[key]['runtime_std']:.3f}s\")\n",
    "#     return series\n",
    "\n",
    "# # ============================\n",
    "# # RESUMED evaluation (uses only true continuations)\n",
    "# # ============================\n",
    "# def evaluate_resumed_for_grid(grid, spec, alpha_dirs):\n",
    "#     \"\"\"\n",
    "#     Evaluate only the *resumed* RL models, using last_model_path / last_vecnorm_path\n",
    "#     from metadata.json for each (alpha, algorithm).\n",
    "\n",
    "#     Models with no 'continuations' entry are skipped.\n",
    "#     \"\"\"\n",
    "#     v_max_override = spec['v_max']\n",
    "#     grid_label = spec['label']\n",
    "#     local_dt_min = 1e-6\n",
    "#     local_dt_max = 30 / 5 * grid['fault'].second / grid['fault'].Tscale\n",
    "#     series = {}\n",
    "\n",
    "#     for alpha_dir in alpha_dirs:\n",
    "#         algo_dirs = sorted([p for p in alpha_dir.iterdir() if p.is_dir()])\n",
    "\n",
    "#         for algo_dir in algo_dirs:\n",
    "#             algo_name = algo_dir.name\n",
    "#             if algo_name not in MODEL_CLASS_MAP:\n",
    "#                 continue  # ignore unknown algos\n",
    "\n",
    "#             meta_path = algo_dir / \"metadata.json\"\n",
    "#             if not meta_path.exists():\n",
    "#                 continue\n",
    "\n",
    "#             with open(meta_path, \"r\", encoding=\"utf-8\") as fh:\n",
    "#                 meta = json.load(fh)\n",
    "\n",
    "#             alpha_value_meta = meta.get(\"alpha\", \"unknown\")\n",
    "\n",
    "#             # --- only keep *truly resumed* models ---\n",
    "#             continuations = meta.get(\"continuations\", [])\n",
    "#             if not continuations:\n",
    "#                 print(f\"  Skipping {algo_name} alpha={alpha_value_meta} (no continuation recorded)\")\n",
    "#                 continue\n",
    "\n",
    "#             model_path_str = meta.get(\"last_model_path\")\n",
    "#             if model_path_str is None:\n",
    "#                 print(f\"  Skipping {algo_name} alpha={alpha_value_meta}: no last_model_path set\")\n",
    "#                 continue\n",
    "#             model_path = Path(model_path_str)\n",
    "\n",
    "#             if not model_path.exists():\n",
    "#                 print(f\"  Skipping {algo_name} alpha={alpha_value_meta}: model '{model_path}' not found\")\n",
    "#                 continue\n",
    "\n",
    "#             vecnorm_path_str = meta.get(\"last_vecnorm_path\")\n",
    "#             if vecnorm_path_str is not None:\n",
    "#                 vecnorm_path = Path(vecnorm_path_str)\n",
    "#             else:\n",
    "#                 vecnorm_path = algo_dir / \"vec_norm.pkl\"\n",
    "\n",
    "#             alpha_value = float(meta.get(\"alpha\", np.nan))\n",
    "#             if not np.isfinite(alpha_value):\n",
    "#                 print(f\"  Skipping {algo_name} in {alpha_dir}: invalid alpha in metadata\")\n",
    "#                 continue\n",
    "\n",
    "#             reward_fn, obs_fn = make_obs_reward(alpha_value, grid, v_max_override)\n",
    "\n",
    "#             def make_env():\n",
    "#                 obs_space = spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float64)\n",
    "#                 return AdaptiveStepperEnv(\n",
    "#                     system=grid['rhs'],\n",
    "#                     dt0=local_dt_min,\n",
    "#                     t0=t_span[0],\n",
    "#                     x0=grid['y0'],\n",
    "#                     tnmax=t_span[1],\n",
    "#                     dt_min=local_dt_min,\n",
    "#                     dt_max=local_dt_max,\n",
    "#                     nparams=(1e-6, 100),\n",
    "#                     integrator=grid['method_mp'],\n",
    "#                     component_slices=grid['component_slices'],\n",
    "#                     reward_fn=reward_fn,\n",
    "#                     obs_fn=obs_fn,\n",
    "#                     obs_space=obs_space,\n",
    "#                     verbose=False,\n",
    "#                     alpha=alpha_value,\n",
    "#                 )\n",
    "\n",
    "#             model_cls = MODEL_CLASS_MAP[algo_name]\n",
    "#             model = model_cls.load(str(model_path), device=\"cpu\")\n",
    "\n",
    "#             runtimes, success_runs, slip_end_list = [], [], []\n",
    "#             t_store = v_store = u_store = None\n",
    "#             key = f\"{algo_name}_resumed alpha={alpha_value:g}\"\n",
    "#             print(f\"\\nEvaluating RESUMED {key} on grid {grid_label}...\")\n",
    "\n",
    "#             for run_idx in range(N_EVAL_RUNS):\n",
    "#                 base_vec_env = DummyVecEnv([make_env])\n",
    "\n",
    "#                 if vecnorm_path.exists():\n",
    "#                     vec_env = VecNormalize.load(str(vecnorm_path), base_vec_env)\n",
    "#                     vec_env.training = False\n",
    "#                     vec_env.norm_obs = False\n",
    "#                     vec_env.norm_reward = False\n",
    "#                 else:\n",
    "#                     vec_env = base_vec_env\n",
    "\n",
    "#                 model.set_env(vec_env)\n",
    "\n",
    "#                 t_arr, v_mean, u_mean, wall, success, s_end_mean = rollout_policy_on_vecenv(\n",
    "#                     model, vec_env, run_name=f\"{key} run{run_idx}\"\n",
    "#                 )\n",
    "\n",
    "#                 runtimes.append(wall)\n",
    "#                 success_runs.append(success)\n",
    "#                 slip_end_list.append(s_end_mean)\n",
    "\n",
    "#                 if run_idx == 0:\n",
    "#                     t_store, v_store, u_store = t_arr, v_mean, u_mean\n",
    "\n",
    "#                 vec_env.close()\n",
    "\n",
    "#                 if run_idx == 0 and not success:\n",
    "#                     break\n",
    "\n",
    "#             if any(success_runs):\n",
    "#                 times_arr = np.asarray(t_store, float) if t_store is not None else np.array([], float)\n",
    "#                 mean_v_arr = np.asarray(v_store, float) if v_store is not None else np.array([], float)\n",
    "#                 mean_u_arr = np.asarray(u_store, float) if u_store is not None else np.array([], float)\n",
    "#                 finite_slips = [s for s in slip_end_list if np.isfinite(s)]\n",
    "#                 s_end_mean = float(np.mean(finite_slips)) if finite_slips else float(\"nan\")\n",
    "\n",
    "#                 series[key] = dict(\n",
    "#                     times=times_arr,\n",
    "#                     mean_v=mean_v_arr,\n",
    "#                     mean_u=mean_u_arr,\n",
    "#                     runtime_s=float(np.mean(runtimes)),\n",
    "#                     runtime_std=float(np.std(runtimes)) if len(runtimes) > 1 else 0.0,\n",
    "#                     converged=True,\n",
    "#                     success_rate=sum(success_runs) / len(success_runs),\n",
    "#                     s_end_mean=s_end_mean,\n",
    "#                 )\n",
    "#                 print(\n",
    "#                     f\"  Success rate: {series[key]['success_rate']:.1%}, \"\n",
    "#                     f\"Runtime: {series[key]['runtime_s']:.3f}s Â± {series[key]['runtime_std']:.3f}s\"\n",
    "#                 )\n",
    "\n",
    "#     return series\n",
    "\n",
    "# # ============================\n",
    "# # Serialization helper\n",
    "# # ============================\n",
    "# def save_series_json(series: dict, grid: dict, out_path: Path):\n",
    "#     scale_t = grid['fault'].Tscale / grid['fault'].second\n",
    "#     scale_v = grid['fault'].Vscale\n",
    "#     scale_u = grid['fault'].Dscale\n",
    "\n",
    "#     json_series = {}\n",
    "#     for key, data in series.items():\n",
    "#         times = np.asarray(data.get(\"times\", []), float)\n",
    "#         mean_v = np.asarray(data.get(\"mean_v\", []), float)\n",
    "#         mean_u = np.asarray(data.get(\"mean_u\", []), float)\n",
    "#         s_end_mean = data.get(\"s_end_mean\", None)\n",
    "\n",
    "#         json_series[key] = {\n",
    "#             \"times\": (times * scale_t).tolist() if times.size else [],\n",
    "#             \"mean_v\": (mean_v * scale_v).tolist() if mean_v.size else [],\n",
    "#             \"mean_u\": (mean_u * scale_u).tolist() if mean_u.size else [],\n",
    "#             \"runtime_s\": data.get(\"runtime_s\", float(\"nan\")),\n",
    "#             \"runtime_std\": data.get(\"runtime_std\", 0.0),\n",
    "#             \"converged\": data.get(\"converged\", False),\n",
    "#             \"success_rate\": data.get(\"success_rate\", 1.0),\n",
    "#             \"s_end_mean\": (float(s_end_mean) * scale_u) if (s_end_mean is not None and np.isfinite(s_end_mean)) else None,\n",
    "#         }\n",
    "\n",
    "#     with open(out_path, \"w\") as f:\n",
    "#         json.dump(json_series, f, indent=2)\n",
    "#     print(f\"Saved results to {out_path}\")\n",
    "\n",
    "# # ============================\n",
    "# # Main\n",
    "# # ============================\n",
    "# if __name__ == \"__main__\":\n",
    "#     assert MODE in {\"PI_ONLY\", \"RL_TD3_ONLY\", \"RESUMED_ONLY\"}, \\\n",
    "#         \"MODE must be 'PI_ONLY', 'RL_TD3_ONLY', or 'RESUMED_ONLY'\"\n",
    "\n",
    "#     # PI controller settings\n",
    "#     adaptive_opts_pi = dict(\n",
    "#         h0=5e-3, h_min=1e-7, h_down=0.6, h_up=1.8, method_order=1, mode=\"ratio\",\n",
    "#         controller=\"H211b\", b_param=2.0, skip_error_indices=[],\n",
    "#     )\n",
    "\n",
    "#     # If RL mode: find alpha_* dirs\n",
    "#     alpha_dirs = []\n",
    "#     if MODE in {\"RL_TD3_ONLY\", \"RESUMED_ONLY\"}:\n",
    "#         alpha_dirs = sorted([p for p in RUN_ROOT.glob(\"alpha_*\") if p.is_dir()])\n",
    "#         if not alpha_dirs:\n",
    "#             raise FileNotFoundError(f\"No trained runs found under {RUN_ROOT}\")\n",
    "\n",
    "#     # Per-grid evaluation\n",
    "#     for spec in GRID_SPECS:\n",
    "#         print(f\"\\n{'='*60}\\nEvaluating on {spec['label']} grid (v_max={spec['v_max']})\\n{'='*60}\")\n",
    "#         grid = build_fault_and_solver(spec[\"Nz\"], spec[\"Nx\"])\n",
    "\n",
    "#         series = {}\n",
    "\n",
    "#         if MODE == \"PI_ONLY\":\n",
    "#             print(f\"\\nRunning PI baseline on {spec['label']}...\")\n",
    "#             baseline = adaptive_baseline_for_grid(grid, adaptive_opts_pi)\n",
    "#             if baseline is not None:\n",
    "#                 series[\"PI_baseline\"] = baseline\n",
    "#             out_file = Path(f\"eval_results_5by5_PI_{spec['label']}.json\")\n",
    "#             save_series_json(series, grid, out_file)\n",
    "\n",
    "#         elif MODE == \"RL_TD3_ONLY\":\n",
    "#             series = evaluate_td3_for_grid(grid, spec, alpha_dirs)\n",
    "#             out_file = Path(f\"eval_results_TD3_{spec['label']}.json\")\n",
    "#             save_series_json(series, grid, out_file)\n",
    "\n",
    "#         elif MODE == \"RESUMED_ONLY\":\n",
    "#             series = evaluate_resumed_for_grid(grid, spec, alpha_dirs)\n",
    "#             out_file = Path(f\"eval_results_RESUMED_{spec['label']}.json\")\n",
    "#             save_series_json(series, grid, out_file)\n",
    "\n",
    "#     print(\"\\n\" + \"=\"*60 + f\"\\nEVALUATION COMPLETE ({MODE})\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104e45f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import TD3            # for RL_TD3_ONLY mode\n",
    "# from sb3_contrib import TQC               # not used here\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "from solve_nivp.projections import CoulombProjection\n",
    "from solve_nivp.nonlinear_solvers import ImplicitEquationSolver\n",
    "from solve_nivp.integrations import CompositeMethod\n",
    "from solve_nivp.rl.env import AdaptiveStepperEnv\n",
    "import solve_nivp\n",
    "from plants.faults import strikeslip\n",
    "\n",
    "# ============================\n",
    "# Select what to run\n",
    "# ============================\n",
    "MODE = \"PI_ONLY\"        # options: \"PI_ONLY\", \"RL_TD3_ONLY\"\n",
    "RUN_ROOT = Path(\"TEST_RL_ALPHA_SWEEP\").resolve()\n",
    "N_EVAL_RUNS = 2\n",
    "\n",
    "# ----------------------------\n",
    "# Required: provide your time span here\n",
    "# ----------------------------\n",
    "# Example (replace with yours):\n",
    "# t_span = [0.0, 40.0]   # nondimensional end time used in your runs4.\n",
    "# t_span = [0.0, 4.0]\n",
    "\n",
    "# ============================\n",
    "# Grids and model physics\n",
    "# ============================\n",
    "GRID_SPECS = [\n",
    "    {\"Nz\": 25, \"Nx\": 25, \"label\": \"25x25\", \"v_max\": 0.07},\n",
    "    {\"Nz\": 50, \"Nx\": 50, \"label\": \"50x50\", \"v_max\": 0.07},\n",
    "    {\"Nz\": 75, \"Nx\": 75, \"label\": \"75x75\", \"v_max\": 0.07},\n",
    "    # {\"Nz\": 100, \"Nx\": 100, \"label\": \"100x100\", \"v_max\": 0.18},\n",
    "]\n",
    "\n",
    "DMU = -0.1\n",
    "DC = 100.0\n",
    "MU_RES = 0.5\n",
    "\n",
    "def build_fault_and_solver(Nz, Nx):\n",
    "    fault_local = strikeslip.qs_strikeslip_fault(\n",
    "        zdepth=3, xlength=3, Nz=Nz, Nx=Nx,\n",
    "        G=30000., rho=2.5e-3, zeta=0.8/3,\n",
    "        Ks_path=\"./Data/\", gamma_s=25., gamma_w=10.,\n",
    "        sigma_ref=100., depth_ini=0., vinf=3.171e-10,\n",
    "        Dmu_estimate=.5,\n",
    "    )\n",
    "    MA_l, KS_l, ES_l, SIGMA_N_l, VINF_raw_l = fault_local.get_plant()\n",
    "    N_DOFS_l = fault_local.N\n",
    "    VINF_l = VINF_raw_l * np.ones(N_DOFS_l) * 0\n",
    "\n",
    "    I_N = sp.eye(N_DOFS_l, format='csr')\n",
    "    A_l = sp.block_diag([sp.csr_matrix(MA_l), I_N, I_N], format='csr')\n",
    "\n",
    "    component_slices_l = [\n",
    "        slice(0, N_DOFS_l),\n",
    "        slice(N_DOFS_l, 2 * N_DOFS_l),\n",
    "        slice(2 * N_DOFS_l, 3 * N_DOFS_l),\n",
    "    ]\n",
    "\n",
    "    DC_scaled = DC / fault_local.Dscale\n",
    "\n",
    "    def con_force_l(state, fk=None):\n",
    "        n = N_DOFS_l\n",
    "        slip_hist = state[2*n:3*n]\n",
    "        mu_vals = MU_RES * (1 - DMU / MU_RES * np.exp(-slip_hist / DC_scaled))\n",
    "        out = np.zeros_like(state)\n",
    "        out[:n] = mu_vals * SIGMA_N_l\n",
    "        return out\n",
    "\n",
    "    y0_l = np.zeros(3 * N_DOFS_l)\n",
    "    friction_force0 = con_force_l(y0_l)\n",
    "    uc = -np.linalg.solve(KS_l, friction_force0[:N_DOFS_l])\n",
    "    y0_l[N_DOFS_l:2*N_DOFS_l] = uc * (1.0 + 1e-5)\n",
    "    Uintc_l = 0.5 * float(uc @ (KS_l @ uc))\n",
    "\n",
    "    projection_l = CoulombProjection(\n",
    "        con_force_func=con_force_l,\n",
    "        rhok=np.ones(N_DOFS_l, dtype=float),\n",
    "        component_slices=component_slices_l,\n",
    "        constraint_indices=list(range(N_DOFS_l)),\n",
    "        use_numba=True,\n",
    "    )\n",
    "\n",
    "    solver_opts_ssn = dict(\n",
    "        tol=1e-8,\n",
    "        max_iter=200,\n",
    "        vi_strict_block_lipschitz=False,\n",
    "        vi_max_block_adjust_iters=5,\n",
    "        globalization='line_search',\n",
    "    )\n",
    "\n",
    "    solver_mp_l = ImplicitEquationSolver(\n",
    "        method='VI',\n",
    "        proj=projection_l,\n",
    "        component_slices=component_slices_l,\n",
    "        tol=solver_opts_ssn.get('tol', 1e-6),\n",
    "        max_iter=solver_opts_ssn.get('max_iter', 50),\n",
    "        vi_strict_block_lipschitz=solver_opts_ssn.get('vi_strict_block_lipschitz', False),\n",
    "        vi_max_block_adjust_iters=solver_opts_ssn.get('vi_max_block_adjust_iters', 10),\n",
    "    )\n",
    "    method_mp_l = CompositeMethod(solver=solver_mp_l, A=A_l)\n",
    "\n",
    "    def rhs_l(t, y):\n",
    "        n = N_DOFS_l\n",
    "        v = y[:n]\n",
    "        u = y[n:2*n]\n",
    "        vdot = -(KS_l @ u) - (ES_l @ (v - VINF_l))\n",
    "        udot = v - VINF_l\n",
    "        sdot = np.abs(v)\n",
    "        return np.concatenate((vdot, udot, sdot))\n",
    "\n",
    "    return dict(\n",
    "        fault=fault_local, KS=KS_l, ES=ES_l, SIGMA_N=SIGMA_N_l, VINF=VINF_l,\n",
    "        N_DOFS=N_DOFS_l, A=A_l, component_slices=component_slices_l,\n",
    "        con_force=con_force_l, y0=y0_l, Uintc=Uintc_l, method_mp=method_mp_l,\n",
    "        rhs=rhs_l, DC_scaled=DC_scaled, solver_opts_ssn=solver_opts_ssn\n",
    "    )\n",
    "\n",
    "# ============================\n",
    "# PI baseline runner\n",
    "# ============================\n",
    "def adaptive_baseline_for_grid(grid, adaptive_opts, n_runs=N_EVAL_RUNS):\n",
    "    projection_opts_nb = dict(\n",
    "        con_force_func=grid['con_force'],\n",
    "        rhok=np.ones(grid['N_DOFS'], dtype=float),\n",
    "        component_slices=grid['component_slices'],\n",
    "        constraint_indices=list(range(grid['N_DOFS'])),\n",
    "        use_numba=True,\n",
    "    )\n",
    "    solver_opts = grid.get('solver_opts_ssn')\n",
    "\n",
    "    local_adapt = dict(adaptive_opts)\n",
    "\n",
    "    runtimes_b = []\n",
    "    success_runs = []\n",
    "    slip_end_list = []\n",
    "    t_store = None\n",
    "    y_store = None\n",
    "\n",
    "    for run_idx in range(n_runs):\n",
    "        start_b = time.time()\n",
    "        try:\n",
    "            t_vals_b, y_vals_b, h_vals_b, fk_vals_b, solver_info_b = solve_nivp.solve_ivp_ns(\n",
    "                fun=grid['rhs'], t_span=t_span, y0=grid['y0'],\n",
    "                method='composite', projection='coulomb', solver='VI',\n",
    "                projection_opts=projection_opts_nb, solver_opts=solver_opts,\n",
    "                adaptive=True, adaptive_opts=local_adapt,\n",
    "                h0=local_adapt.get('h0', 5e-3),\n",
    "                component_slices=grid['component_slices'],\n",
    "                verbose=False, A=grid['A'],\n",
    "            )\n",
    "            wall_b = time.time() - start_b\n",
    "\n",
    "            success = (len(t_vals_b) > 0 and t_vals_b[-1] >= 0.999 * t_span[1])\n",
    "            success_runs.append(success)\n",
    "            runtimes_b.append(wall_b)\n",
    "\n",
    "            n = grid['N_DOFS']\n",
    "            s_final_block = y_vals_b[-1, 2*n:3*n]\n",
    "            s_end_mean = float(np.mean(s_final_block))\n",
    "            slip_end_list.append(s_end_mean)\n",
    "\n",
    "            if run_idx == 0:\n",
    "                t_store = t_vals_b\n",
    "                y_store = y_vals_b\n",
    "\n",
    "            print(f\"  PI baseline run {run_idx}: {wall_b:.3f}s, success={success}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  PI baseline run {run_idx} failed: {e}\")\n",
    "            success_runs.append(False)\n",
    "\n",
    "    if not any(success_runs):\n",
    "        print(\"  WARNING: PI baseline failed all runs\")\n",
    "        return None\n",
    "\n",
    "    runtimes_b = [r for r in runtimes_b if np.isfinite(r)]\n",
    "\n",
    "    n = grid['N_DOFS']\n",
    "    v_b = np.mean(y_store[:, :n], axis=1)\n",
    "    u_b = np.mean(y_store[:, n:2*n], axis=1)\n",
    "\n",
    "    return dict(\n",
    "        times=np.asarray(t_store, dtype=float) if t_store is not None else np.array([], dtype=float),\n",
    "        mean_v=np.asarray(v_b, dtype=float),\n",
    "        mean_u=np.asarray(u_b, dtype=float),\n",
    "        runtime_s=float(np.mean(runtimes_b)) if runtimes_b else float(\"nan\"),\n",
    "        runtime_std=float(np.std(runtimes_b)) if len(runtimes_b) > 1 else 0.0,\n",
    "        converged=True,\n",
    "        success_rate=sum(success_runs) / len(success_runs),\n",
    "        s_end_mean=float(np.mean([s for s in slip_end_list if np.isfinite(s)])) if slip_end_list else float(\"nan\"),\n",
    "    )\n",
    "\n",
    "# ============================\n",
    "# RL evaluation (TD3 only)\n",
    "# ============================\n",
    "def make_obs_reward(alpha_value, grid, v_max_override):\n",
    "    def build_reward_fn(alpha_value: float):\n",
    "        alpha_value = float(alpha_value)\n",
    "        def my_reward_fn(solver_perf, dt_attempt, xk, env):\n",
    "            (runtime_inc, dts, err_LO, err_l1, err_HI, E, sLO, sl1, sHI, kLO, kL1, kHI) = solver_perf\n",
    "            dt_range = env.dt_max - env.dt_min\n",
    "            if dts == 0.0:\n",
    "                dt_norm_attempt = (dt_attempt - env.dt_min) / dt_range\n",
    "                return -dt_norm_attempt\n",
    "            dt_norm = (dts - env.dt_min) / dt_range\n",
    "            S1 = float(np.clip(dt_norm, 0.0, 1.0))\n",
    "            if not hasattr(env, 'runtime_history'): env.runtime_history = []\n",
    "            env.runtime_history.append(float(runtime_inc))\n",
    "            if not hasattr(env, 'max_runtime_seen'): env.max_runtime_seen = float(runtime_inc)\n",
    "            else: env.max_runtime_seen = max(env.max_runtime_seen, float(runtime_inc))\n",
    "            if not hasattr(env, 'min_runtime_seen'): env.min_runtime_seen = float(runtime_inc)\n",
    "            else: env.min_runtime_seen = min(env.min_runtime_seen, float(runtime_inc))\n",
    "            rt_min = float(env.min_runtime_seen); rt_max = max(rt_min, float(env.max_runtime_seen))\n",
    "            denom = max(1e-8, rt_max - rt_min)\n",
    "            S2 = 1 - (float(runtime_inc) - rt_min) / denom\n",
    "            A = float(np.exp(-alpha_value * E))\n",
    "            return S1 * S2 * A\n",
    "        return my_reward_fn\n",
    "\n",
    "    reward_fn = build_reward_fn(alpha_value)\n",
    "\n",
    "    def obs_fn(dt_attempt, converged, xk, solver_perf, fk=None, env=None):\n",
    "        assert env is not None\n",
    "        n = grid['N_DOFS']\n",
    "        v = xk[:n]\n",
    "        u = xk[n:2*n]\n",
    "        avg_v = float(np.mean(v))\n",
    "        ks_u = grid['KS'] @ u\n",
    "        E_int = 0.5 * float(np.dot(u, ks_u))\n",
    "        E_norm = float(E_int / grid['Uintc']) if grid['Uintc'] > 0 else float(E_int)\n",
    "        if solver_perf is not None:\n",
    "            dts = solver_perf[1]\n",
    "            conv_flag_bipolar = 1.0 if dts > 0.0 else -1.0\n",
    "        else:\n",
    "            conv_flag_bipolar = -1.0\n",
    "\n",
    "        if dt_attempt is None or solver_perf is None:\n",
    "            dt_norm_default = 0.5\n",
    "            conv_flag = 0.0 if converged is None else float(converged)\n",
    "            return np.array([\n",
    "                E_norm,\n",
    "                avg_v / (v_max_override / grid['fault'].Vscale),\n",
    "                (env.iter_error / 4.0) if hasattr(env, 'iter_error') else 0.0,\n",
    "                conv_flag, 0.0,\n",
    "                conv_flag_bipolar * dt_norm_default,\n",
    "            ], dtype=np.float64)\n",
    "\n",
    "        (runtime_inc, dts, err_LO, err_l1, err_HI, E_global, sLO, sl1, sHI, kLO, kL1, kHI) = solver_perf\n",
    "        dt_range = env.dt_max - env.dt_min\n",
    "        dt_norm = (dt_attempt - env.dt_min) / dt_range if dt_range > 0 else 0.0\n",
    "        dt_norm = float(np.clip(dt_norm, 0.0, 1.0))\n",
    "        conv_flag = float(converged) if converged is not None else 0.0\n",
    "        acc = 1.0 / (1.0 + E_global) if conv_flag else 0.0\n",
    "\n",
    "        return np.array([\n",
    "            E_norm,\n",
    "            avg_v / (v_max_override / grid['fault'].Vscale),\n",
    "            (env.iter_error / 4.0) if hasattr(env, 'iter_error') else 0.0,\n",
    "            conv_flag, acc,\n",
    "            conv_flag_bipolar * dt_norm,\n",
    "        ], dtype=np.float64)\n",
    "\n",
    "    return reward_fn, obs_fn\n",
    "\n",
    "def rollout_policy_on_vecenv(model, vec_env, run_name=\"(unnamed)\"):\n",
    "    vec_env.training = False\n",
    "    vec_env.norm_obs = False\n",
    "    vec_env.norm_reward = False\n",
    "    base_env = vec_env.venv.envs[0] if hasattr(vec_env, \"venv\") else vec_env.envs[0]\n",
    "\n",
    "    obs = vec_env.reset()\n",
    "    times, mean_vel, mean_slip = [], [], []\n",
    "    last_t, repeat_count, broke_stuck = None, 0, False\n",
    "    last_converged, last_xk = None, None\n",
    "    start = time.time(); done = False\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, dones, infos = vec_env.step(action)\n",
    "        done = bool(dones[0]); info = infos[0]\n",
    "        t_now = info.get(\"t_k1\", np.nan)\n",
    "        xk = info.get(\"xk\", None)\n",
    "        if xk is not None: last_xk = xk\n",
    "        if \"converged\" in info: last_converged = info.get(\"converged\")\n",
    "\n",
    "        current_t = None if t_now is None or np.isnan(t_now) else float(t_now)\n",
    "        if current_t is not None:\n",
    "            if last_t is not None and np.isclose(current_t, last_t, rtol=0.0, atol=1e-12):\n",
    "                repeat_count += 1\n",
    "                if repeat_count >= 10:\n",
    "                    print(f\"[{run_name}] WARNING: time stuck at {current_t}\")\n",
    "                    broke_stuck = True; break\n",
    "            else:\n",
    "                repeat_count = 0\n",
    "            last_t = current_t\n",
    "\n",
    "        if xk is not None and current_t is not None:\n",
    "            n = xk.shape[0] // 3\n",
    "            v_block = xk[:n]; u_block = xk[n:2*n]\n",
    "            times.append(current_t)\n",
    "            mean_vel.append(float(np.mean(v_block)))\n",
    "            mean_slip.append(float(np.mean(u_block)))\n",
    "\n",
    "    wall_time = time.time() - start\n",
    "    if last_converged is not None:\n",
    "        success = bool(last_converged)\n",
    "    else:\n",
    "        success = (len(times) > 0 and hasattr(base_env, 'tnmax') and times[-1] >= 0.999 * float(base_env.tnmax)) and not broke_stuck\n",
    "\n",
    "    if last_xk is not None:\n",
    "        n = last_xk.shape[0] // 3\n",
    "        s_block = last_xk[2*n:3*n]\n",
    "        s_end_mean = float(np.mean(s_block))\n",
    "    else:\n",
    "        s_end_mean = np.nan\n",
    "\n",
    "    print(f\"[{run_name}] Done: wall_time={wall_time:.3f}s, steps={len(times)}, final_t={(times[-1] if len(times)>0 else float('nan')):.3f}, s_end={s_end_mean:.6e}\")\n",
    "    return (np.asarray(times, float), np.asarray(mean_vel, float), np.asarray(mean_slip, float),\n",
    "            float(wall_time), bool(success), s_end_mean)\n",
    "\n",
    "def evaluate_td3_for_grid(grid, spec, alpha_dirs):\n",
    "    v_max_override = spec['v_max']; grid_label = spec['label']\n",
    "    local_dt_min = 1e-6\n",
    "    local_dt_max = 30 / 5 * grid['fault'].second / grid['fault'].Tscale\n",
    "    series = {}\n",
    "\n",
    "    for alpha_dir in alpha_dirs:\n",
    "        algo_dirs = sorted([p for p in alpha_dir.iterdir() if p.is_dir()])\n",
    "        for algo_dir in algo_dirs:\n",
    "            algo_name = algo_dir.name\n",
    "            if algo_name != \"TD3\":\n",
    "                continue  # skip everything except TD3\n",
    "\n",
    "            meta_path = algo_dir / \"metadata.json\"\n",
    "            model_path = algo_dir / \"model.zip\"\n",
    "            if not model_path.exists():\n",
    "                model_path = algo_dir / \"model\"\n",
    "            vecnorm_path = algo_dir / \"vec_norm.pkl\"\n",
    "            if not meta_path.exists() or not model_path.exists():\n",
    "                continue\n",
    "\n",
    "            with open(meta_path, \"r\", encoding=\"utf-8\") as fh:\n",
    "                meta = json.load(fh)\n",
    "            alpha_value = float(meta.get(\"alpha\", np.nan))\n",
    "            if not np.isfinite(alpha_value):\n",
    "                continue\n",
    "\n",
    "            reward_fn, obs_fn = make_obs_reward(alpha_value, grid, v_max_override)\n",
    "\n",
    "            def make_env():\n",
    "                obs_space = spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float64)\n",
    "                return AdaptiveStepperEnv(\n",
    "                    system=grid['rhs'], dt0=local_dt_min, t0=t_span[0], x0=grid['y0'], tnmax=t_span[1],\n",
    "                    dt_min=local_dt_min, dt_max=local_dt_max, nparams=(1e-6, 100),\n",
    "                    integrator=grid['method_mp'], component_slices=grid['component_slices'],\n",
    "                    reward_fn=reward_fn, obs_fn=obs_fn, obs_space=obs_space, verbose=False, alpha=alpha_value,\n",
    "                )\n",
    "\n",
    "            model = TD3.load(str(model_path), device=\"cpu\")\n",
    "\n",
    "            runtimes, success_runs, slip_end_list = [], [], []\n",
    "            t_store = v_store = u_store = None\n",
    "            key = f\"{algo_name} alpha={alpha_value:g}\"\n",
    "            print(f\"\\nEvaluating {key} on grid {grid_label}...\")\n",
    "\n",
    "            for run_idx in range(N_EVAL_RUNS):\n",
    "                base_vec_env = DummyVecEnv([make_env])\n",
    "                if vecnorm_path.exists():\n",
    "                    vec_env = VecNormalize.load(str(vecnorm_path), base_vec_env)\n",
    "                    vec_env.training = False; vec_env.norm_obs = False; vec_env.norm_reward = False\n",
    "                else:\n",
    "                    vec_env = base_vec_env\n",
    "                model.set_env(vec_env)\n",
    "\n",
    "                t_arr, v_mean, u_mean, wall, success, s_end_mean = rollout_policy_on_vecenv(\n",
    "                    model, vec_env, run_name=f\"{key} run{run_idx}\"\n",
    "                )\n",
    "                runtimes.append(wall); success_runs.append(success); slip_end_list.append(s_end_mean)\n",
    "                if run_idx == 0: t_store, v_store, u_store = t_arr, v_mean, u_mean\n",
    "                vec_env.close()\n",
    "                if run_idx == 0 and not success: break\n",
    "\n",
    "            if any(success_runs):\n",
    "                times_arr = np.asarray(t_store, float) if t_store is not None else np.array([], float)\n",
    "                mean_v_arr = np.asarray(v_store, float) if v_store is not None else np.array([], float)\n",
    "                mean_u_arr = np.asarray(u_store, float) if u_store is not None else np.array([], float)\n",
    "                finite_slips = [s for s in slip_end_list if np.isfinite(s)]\n",
    "                s_end_mean = float(np.mean(finite_slips)) if finite_slips else float(\"nan\")\n",
    "                series[key] = dict(\n",
    "                    times=times_arr, mean_v=mean_v_arr, mean_u=mean_u_arr,\n",
    "                    runtime_s=float(np.mean(runtimes)), runtime_std=float(np.std(runtimes)) if len(runtimes) > 1 else 0.0,\n",
    "                    converged=True, success_rate=sum(success_runs)/len(success_runs), s_end_mean=s_end_mean,\n",
    "                )\n",
    "                print(f\"  Success rate: {series[key]['success_rate']:.1%}, Runtime: {series[key]['runtime_s']:.3f}s Â± {series[key]['runtime_std']:.3f}s\")\n",
    "    return series\n",
    "\n",
    "# ============================\n",
    "# Serialization helper (scales to physical units like your original)\n",
    "# ============================\n",
    "def save_series_json(series: dict, grid: dict, out_path: Path):\n",
    "    scale_t = grid['fault'].Tscale / grid['fault'].second\n",
    "    scale_v = grid['fault'].Vscale\n",
    "    scale_u = grid['fault'].Dscale\n",
    "\n",
    "    json_series = {}\n",
    "    for key, data in series.items():\n",
    "        times = np.asarray(data.get(\"times\", []), float)\n",
    "        mean_v = np.asarray(data.get(\"mean_v\", []), float)\n",
    "        mean_u = np.asarray(data.get(\"mean_u\", []), float)\n",
    "        s_end_mean = data.get(\"s_end_mean\", None)\n",
    "\n",
    "        json_series[key] = {\n",
    "            \"times\": (times * scale_t).tolist() if times.size else [],\n",
    "            \"mean_v\": (mean_v * scale_v).tolist() if mean_v.size else [],\n",
    "            \"mean_u\": (mean_u * scale_u).tolist() if mean_u.size else [],\n",
    "            \"runtime_s\": data.get(\"runtime_s\", float(\"nan\")),\n",
    "            \"runtime_std\": data.get(\"runtime_std\", 0.0),\n",
    "            \"converged\": data.get(\"converged\", False),\n",
    "            \"success_rate\": data.get(\"success_rate\", 1.0),\n",
    "            \"s_end_mean\": (float(s_end_mean) * scale_u) if (s_end_mean is not None and np.isfinite(s_end_mean)) else None,\n",
    "        }\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(json_series, f, indent=2)\n",
    "    print(f\"Saved results to {out_path}\")\n",
    "\n",
    "# ============================\n",
    "# Main\n",
    "# ============================\n",
    "if __name__ == \"__main__\":\n",
    "    assert MODE in {\"PI_ONLY\", \"RL_TD3_ONLY\"}, \"MODE must be 'PI_ONLY' or 'RL_TD3_ONLY'\"\n",
    "\n",
    "    # PI controller settings (unchanged)\n",
    "    adaptive_opts_pi = dict(\n",
    "        h0=5e-3, h_min=1e-7, h_down=0.6, h_up=1.8, method_order=1,mode=\"ratio\",\n",
    "        controller=\"h211b\", b_param=2.0, skip_error_indices=[],\n",
    "    )\n",
    "\n",
    "    # If RL mode: find alpha_* dirs\n",
    "    alpha_dirs = []\n",
    "    if MODE == \"RL_TD3_ONLY\":\n",
    "        alpha_dirs = sorted([p for p in RUN_ROOT.glob(\"alpha_*\") if p.is_dir()])\n",
    "        if not alpha_dirs:\n",
    "            raise FileNotFoundError(f\"No trained runs found under {RUN_ROOT}\")\n",
    "\n",
    "    # Per-grid evaluation\n",
    "    for spec in GRID_SPECS:\n",
    "        print(f\"\\n{'='*60}\\nEvaluating on {spec['label']} grid (v_max={spec['v_max']})\\n{'='*60}\")\n",
    "        grid = build_fault_and_solver(spec[\"Nz\"], spec[\"Nx\"])\n",
    "\n",
    "        series = {}\n",
    "\n",
    "        if MODE == \"PI_ONLY\":\n",
    "            print(f\"\\nRunning PI baseline on {spec['label']}...\")\n",
    "            baseline = adaptive_baseline_for_grid(grid, adaptive_opts_pi)\n",
    "            if baseline is not None:\n",
    "                # save only PI under PI_baseline key so downstream code remains compatible\n",
    "                series[\"PI_baseline\"] = baseline\n",
    "            out_file = Path(f\"eval_results_{spec['label']}.json\")\n",
    "            save_series_json(series, grid, out_file)\n",
    "\n",
    "        elif MODE == \"RL_TD3_ONLY\":\n",
    "            # Evaluate TD3 only (no PI)\n",
    "            series = evaluate_td3_for_grid(grid, spec, alpha_dirs)\n",
    "            out_file = Path(f\"eval_results_TD3_3by3_{spec['label']}.json\")\n",
    "            save_series_json(series, grid, out_file)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60 + f\"\\nEVALUATION COMPLETE ({MODE})\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbb9d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import time\n",
    "# import json\n",
    "# from pathlib import Path\n",
    "# import matplotlib.pyplot as plt\n",
    "# import scipy.sparse as sp5\n",
    "\n",
    "# from gymnasium import spaces\n",
    "# from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "# from sb3_contrib import TQC\n",
    "# # from stable_baselines3 import TD3\n",
    "\n",
    "# from solve_nivp.projections import CoulombProjection\n",
    "# from solve_nivp.nonlinear_solvers import ImplicitEquationSolver\n",
    "# from solve_nivp.integrations import CompositeMethod\n",
    "# from solve_nivp.rl.env import AdaptiveStepperEnv\n",
    "# import solve_nivp\n",
    "# from plants.faults import strikeslip\n",
    "\n",
    "# ###############################################################################\n",
    "# # Configuration\n",
    "# ###############################################################################\n",
    "# ALGO_REGISTRY = {\n",
    "#     \"TQC\": TQC,\n",
    "#     # \"TD3\": TD3,\n",
    "# }\n",
    "\n",
    "# RUN_ROOT = Path(\"TEST_RL_ALPHA_SWEEP\").resolve()\n",
    "# N_EVAL_RUNS = 5  # Number of evaluation runs for runtime averaging\n",
    "\n",
    "# # Grid specifications with their specific v_max values\n",
    "# GRID_SPECS = [\n",
    "#     # {\"Nz\": 1, \"Nx\": 1, \"label\": \"1x1\", \"v_max\": 0.07},  # Training grid\n",
    "#     {\"Nz\": 25, \"Nx\": 25, \"label\": \"25x25\", \"v_max\": 0.18},  # Same v_max as 1x1\n",
    "#     {\"Nz\": 50, \"Nx\": 50, \"label\": \"50x50\", \"v_max\": 0.18},  # Same v_max as 1x1\n",
    "#     {\"Nz\": 75, \"Nx\": 75, \"label\": \"75x75\", \"v_max\": 0.18},  # Same v_max as 1x1\n",
    "#     {\"Nz\": 100, \"Nx\": 100, \"label\": \"100x100\", \"v_max\": 0.18},  # Same v_max as 1x1\n",
    "\n",
    "#     # {\"Nz\": 5, \"Nx\": 5, \"label\": \"5x5\", \"v_max\": 0.18},  # Different v_max\n",
    "# ]\n",
    "\n",
    "# # Time span (assuming you have this defined globally)\n",
    "# # t_span = [0.0, your_final_time]\n",
    "\n",
    "# # Friction parameters (must match training)\n",
    "# DMU = -0.1\n",
    "# DC = 100.0  # Will be scaled by fault.Dscale\n",
    "# MU_RES = 0.5\n",
    "\n",
    "# ###############################################################################\n",
    "# # Build fault and solver for a given grid\n",
    "# ###############################################################################\n",
    "# def build_fault_and_solver(Nz, Nx):\n",
    "#     \"\"\"\n",
    "#     Build a fault model and solver stack for a given grid size.\n",
    "#     Returns dict with all necessary components.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # 1. Create fault for this grid\n",
    "#     fault_local = strikeslip.qs_strikeslip_fault(\n",
    "#         zdepth=5, xlength=5, Nz=Nz, Nx=Nx,\n",
    "#         G=30000., rho=2.5e-3, zeta=0.8/3,\n",
    "#         Ks_path=\"./Data/\", gamma_s=25., gamma_w=10.,\n",
    "#         sigma_ref=100., depth_ini=0., vinf=3.171e-10,\n",
    "#         Dmu_estimate=.5,\n",
    "#     )\n",
    "\n",
    "#     # 2. Extract plant matrices\n",
    "#     MA_l, KS_l, ES_l, SIGMA_N_l, VINF_raw_l = fault_local.get_plant()\n",
    "#     N_DOFS_l = fault_local.N\n",
    "#     VINF_l = VINF_raw_l * np.ones(N_DOFS_l) * 0  # Set to zero as in your code\n",
    "\n",
    "#     # 3. Build block diagonal A matrix\n",
    "#     I_N = sp.eye(N_DOFS_l, format='csr')\n",
    "#     A_l = sp.block_diag([sp.csr_matrix(MA_l), I_N, I_N], format='csr')\n",
    "\n",
    "#     # 4. Component slices\n",
    "#     component_slices_l = [\n",
    "#         slice(0, N_DOFS_l),              # v\n",
    "#         slice(N_DOFS_l, 2 * N_DOFS_l),   # u\n",
    "#         slice(2 * N_DOFS_l, 3 * N_DOFS_l) # s\n",
    "#     ]\n",
    "\n",
    "#     # 5. Scaled DC for this grid\n",
    "#     DC_scaled = DC / fault_local.Dscale\n",
    "\n",
    "#     # 6. Contact force function\n",
    "#     def con_force_l(state, fk=None):\n",
    "#         n = N_DOFS_l\n",
    "#         slip_hist = state[2*n:3*n]\n",
    "#         mu_vals = MU_RES * (1 - DMU / MU_RES * np.exp(-slip_hist / DC_scaled))\n",
    "#         out = np.zeros_like(state)\n",
    "#         out[:n] = mu_vals * SIGMA_N_l\n",
    "#         return out\n",
    "\n",
    "#     # 7. Initial state\n",
    "#     y0_l = np.zeros(3 * N_DOFS_l)\n",
    "#     friction_force0 = con_force_l(y0_l)\n",
    "#     uc = -np.linalg.solve(KS_l, friction_force0[:N_DOFS_l])\n",
    "#     y0_l[N_DOFS_l:2*N_DOFS_l] = uc * (1.0 + 1e-5)\n",
    "\n",
    "#     # 8. Critical energy for normalization\n",
    "#     Uintc_l = 0.5 * float(uc @ (KS_l @ uc))\n",
    "\n",
    "#     # 9. Projection operator\n",
    "#     projection_l = CoulombProjection(\n",
    "#         con_force_func=con_force_l,\n",
    "#         rhok=np.ones(N_DOFS_l, dtype=float),\n",
    "#         component_slices=component_slices_l,\n",
    "#         constraint_indices=list(range(N_DOFS_l)),\n",
    "#         use_numba=True,\n",
    "#     )\n",
    "\n",
    "#     # 10. VI solver\n",
    "\n",
    "#     # Tuned nonlinear-solver options shared by VI and SSN\n",
    "#     solver_opts_common = dict(\n",
    "#         tol=1e-8,\n",
    "#         max_iter=200,\n",
    "#     )\n",
    "\n",
    "#     # Adaptive controller tuned for nonsmooth dynamics\n",
    "#     adaptive_opts = dict(\n",
    "#         h0=5e-2,\n",
    "#         h_min=1e-7,\n",
    "#         h_down=0.6,\n",
    "#         h_up=1.8,\n",
    "#         method_order=1,           # conservative order for nonsmooth dynamics\n",
    "#         skip_error_indices=[],\n",
    "#         controller='h211b',              # smoother steps with PI control\n",
    "#         b_param=4.0,         # only if controller == \"H211b\"\n",
    "#         mode = 'ratio'\n",
    "#     )\n",
    "\n",
    "#     # SSN-specific tweaks for robustness and speed\n",
    "#     solver_opts_ssn = dict(solver_opts_common)\n",
    "#     solver_opts_ssn.update({\n",
    "#         \"vi_strict_block_lipschitz\": False,   # was True by default\n",
    "#         \"vi_max_block_adjust_iters\": 5,       # smaller safety cap\n",
    "#         \"globalization\": 'line_search',    # more robust globalization\n",
    "#     })\n",
    "#     solver_mp_l = ImplicitEquationSolver(\n",
    "#         method='VI',\n",
    "#         proj=projection_l,\n",
    "#         component_slices=component_slices_l,\n",
    "#         tol=solver_opts_ssn.get('tol', 1e-6),\n",
    "#         max_iter=solver_opts_ssn.get('max_iter', 50),\n",
    "#         vi_strict_block_lipschitz=solver_opts_ssn.get('vi_strict_block_lipschitz', False),\n",
    "#         vi_max_block_adjust_iters=solver_opts_ssn.get('vi_max_block_adjust_iters', 10),\n",
    "#     )\n",
    "\n",
    "#     # 11. Composite method\n",
    "#     method_mp_l = CompositeMethod(solver=solver_mp_l, A=A_l)\n",
    "\n",
    "#     # 12. RHS function\n",
    "#     def rhs_l(t, y):\n",
    "#         n = N_DOFS_l\n",
    "#         v = y[:n]\n",
    "#         u = y[n:2*n]\n",
    "\n",
    "#         vdot = -(KS_l @ u) - (ES_l @ (v - VINF_l))\n",
    "#         udot = v - VINF_l\n",
    "#         sdot = np.abs(v)\n",
    "#         return np.concatenate((vdot, udot, sdot))\n",
    "\n",
    "#     return dict(\n",
    "#         fault=fault_local,\n",
    "#         KS=KS_l,\n",
    "#         ES=ES_l,\n",
    "#         SIGMA_N=SIGMA_N_l,\n",
    "#         VINF=VINF_l,\n",
    "#         N_DOFS=N_DOFS_l,\n",
    "#         A=A_l,\n",
    "#         component_slices=component_slices_l,\n",
    "#         con_force=con_force_l,\n",
    "#         y0=y0_l,\n",
    "#         Uintc=Uintc_l,\n",
    "#         method_mp=method_mp_l,\n",
    "#         rhs=rhs_l,\n",
    "#         DC_scaled=DC_scaled,\n",
    "#         solver_opts_ssn=solver_opts_ssn,\n",
    "#     )\n",
    "\n",
    "# ###############################################################################\n",
    "# # Build observation and reward functions (WITH SIGNED DT_NORM)\n",
    "# ###############################################################################\n",
    "# def build_reward_fn(alpha_value: float):\n",
    "#     \"\"\"Your existing reward function\"\"\"\n",
    "#     alpha_value = float(alpha_value)\n",
    "\n",
    "#     def my_reward_fn(solver_perf, dt_attempt, xk, env):\n",
    "#         (\n",
    "#             runtime_inc,\n",
    "#             dts,\n",
    "#             error_LO,\n",
    "#             error_lil1,\n",
    "#             error_HI,\n",
    "#             E,\n",
    "#             success_LO,\n",
    "#             success_lil1,\n",
    "#             success_HI,\n",
    "#             kiter_LO,\n",
    "#             iter_lil1,\n",
    "#             kiter_HI,\n",
    "#         ) = solver_perf\n",
    "\n",
    "#         dt_range = env.dt_max - env.dt_min\n",
    "\n",
    "#         if dts == 0.0:\n",
    "#             dt_norm_attempt = (dt_attempt - env.dt_min) / dt_range\n",
    "#             return -dt_norm_attempt\n",
    "\n",
    "#         # S1: step size score\n",
    "#         dt_norm = (dts - env.dt_min) / dt_range\n",
    "#         S1 = float(np.clip(dt_norm, 0.0, 1.0))\n",
    "\n",
    "#         # S2: runtime score (simplified version)\n",
    "#         if not hasattr(env, 'runtime_history'):\n",
    "#             env.runtime_history = []\n",
    "#         env.runtime_history.append(float(runtime_inc))\n",
    "\n",
    "#         if not hasattr(env, 'max_runtime_seen'):\n",
    "#             env.max_runtime_seen = float(runtime_inc)\n",
    "#         else:\n",
    "#             if runtime_inc > env.max_runtime_seen:\n",
    "#                 env.max_runtime_seen = float(runtime_inc)\n",
    "\n",
    "#         if not hasattr(env, 'min_runtime_seen'):\n",
    "#             env.min_runtime_seen = float(runtime_inc)\n",
    "#         else:\n",
    "#             if runtime_inc < env.min_runtime_seen:\n",
    "#                 env.min_runtime_seen = float(runtime_inc)\n",
    "\n",
    "#         rt_min = float(env.min_runtime_seen)\n",
    "#         rt_max = max(rt_min, float(env.max_runtime_seen))\n",
    "#         denom = max(1e-8, rt_max - rt_min)\n",
    "#         S2 = 1 - (float(runtime_inc) - rt_min) / denom\n",
    "\n",
    "#         # A: accuracy score\n",
    "#         A = float(np.exp(-alpha_value * E))\n",
    "\n",
    "#         reward = S1 * S2 * A\n",
    "#         return reward\n",
    "\n",
    "#     return my_reward_fn\n",
    "\n",
    "# def make_obs_reward(alpha_value, grid, v_max_override):\n",
    "#     \"\"\"\n",
    "#     Build observation and reward functions for evaluation.\n",
    "#     Uses v_max_override for velocity normalization.\n",
    "#     \"\"\"\n",
    "#     reward_fn = build_reward_fn(alpha_value)\n",
    "\n",
    "#     def obs_fn(dt_attempt, converged, xk, solver_perf, fk=None, env=None):\n",
    "#         assert env is not None, \"env must be provided\"\n",
    "\n",
    "#         # Extract physics state\n",
    "#         n = grid['N_DOFS']\n",
    "#         v = xk[:n]\n",
    "#         u = xk[n:2*n]\n",
    "\n",
    "#         avg_v = float(np.mean(v))\n",
    "\n",
    "#         # Elastic energy\n",
    "#         ks_u = grid['KS'] @ u\n",
    "#         E_int = 0.5 * float(np.dot(u, ks_u))\n",
    "#         E_norm = float(E_int / grid['Uintc']) if grid['Uintc'] > 0 else float(E_int)\n",
    "\n",
    "#         # Bipolar convergence flag for signed dt_norm\n",
    "#         if solver_perf is not None:\n",
    "#             dts = solver_perf[1]\n",
    "#             conv_flag_bipolar = 1.0 if dts > 0.0 else -1.0\n",
    "#         else:\n",
    "#             conv_flag_bipolar = -1.0\n",
    "\n",
    "#         # RESET BRANCH\n",
    "#         if dt_attempt is None or solver_perf is None:\n",
    "#             dt_norm_default = 0.5\n",
    "#             conv_flag = 0.0 if converged is None else float(converged)\n",
    "\n",
    "#             return np.array([\n",
    "#                 E_norm,\n",
    "#                 avg_v / (v_max_override / grid['fault'].Vscale),  # Use override\n",
    "#                 (env.iter_error / 4.0) if hasattr(env, 'iter_error') else 0.0,\n",
    "#                 conv_flag,\n",
    "#                 0.0,  # accuracy proxy neutral\n",
    "#                 conv_flag_bipolar * dt_norm_default,  # SIGNED!\n",
    "#             ], dtype=np.float64)\n",
    "\n",
    "#         # NORMAL STEP BRANCH\n",
    "#         (\n",
    "#             runtime_inc,\n",
    "#             dts,\n",
    "#             error_LO,\n",
    "#             error_lil1,\n",
    "#             error_HI,\n",
    "#             E_global,\n",
    "#             success_LO,\n",
    "#             success_lil1,\n",
    "#             success_HI,\n",
    "#             kiter_LO,\n",
    "#             iter_lil1,\n",
    "#             kiter_HI,\n",
    "#         ) = solver_perf\n",
    "\n",
    "#         # Normalize dt_attempt\n",
    "#         dt_range = env.dt_max - env.dt_min\n",
    "#         if dt_range <= 0.0:\n",
    "#             dt_norm = 0.0\n",
    "#         else:\n",
    "#             dt_norm = (dt_attempt - env.dt_min) / dt_range\n",
    "#             dt_norm = float(np.clip(dt_norm, 0.0, 1.0))\n",
    "\n",
    "#         # Convergence and accuracy\n",
    "#         conv_flag = float(converged) if converged is not None else 0.0\n",
    "#         if conv_flag:\n",
    "#             acc = 1.0 / (1.0 + E_global)\n",
    "#         else:\n",
    "#             acc = 0.0\n",
    "\n",
    "#         # Return observation with SIGNED dt_norm\n",
    "#         return np.array([\n",
    "#             E_norm,\n",
    "#             avg_v / (v_max_override / grid['fault'].Vscale),  # Use override\n",
    "#             (env.iter_error / 4.0) if hasattr(env, 'iter_error') else 0.0,\n",
    "#             conv_flag,\n",
    "#             acc,\n",
    "#             conv_flag_bipolar * dt_norm,  # SIGNED!\n",
    "#         ], dtype=np.float64)\n",
    "\n",
    "#     return reward_fn, obs_fn\n",
    "\n",
    "# ###############################################################################\n",
    "# # Rollout helper\n",
    "# ###############################################################################\n",
    "# def rollout_policy_on_vecenv(model, vec_env, run_name=\"(unnamed)\"):\n",
    "#     \"\"\"Roll out a policy on a VecEnv and collect trajectory data.\"\"\"\n",
    "#     vec_env.training = False\n",
    "#     vec_env.norm_obs = False\n",
    "#     vec_env.norm_reward = False\n",
    "\n",
    "#     base_env = vec_env.venv.envs[0] if hasattr(vec_env, \"venv\") else vec_env.envs[0]\n",
    "\n",
    "#     obs = vec_env.reset()\n",
    "#     times = []\n",
    "#     mean_vel = []\n",
    "#     mean_slip = []\n",
    "\n",
    "#     last_t = None\n",
    "#     repeat_count = 0\n",
    "#     broke_stuck = False\n",
    "#     last_converged = None\n",
    "#     last_xk = None\n",
    "\n",
    "#     start = time.time()\n",
    "#     done = False\n",
    "\n",
    "#     while not done:\n",
    "#         action, _ = model.predict(obs, deterministic=True)\n",
    "#         obs, rewards, dones, infos = vec_env.step(action)\n",
    "\n",
    "#         done = bool(dones[0])\n",
    "#         info = infos[0]\n",
    "\n",
    "#         t_now = info.get(\"t_k1\", np.nan)\n",
    "#         xk = info.get(\"xk\", None)\n",
    "#         if xk is not None:\n",
    "#             last_xk = xk\n",
    "\n",
    "#         if \"converged\" in info:\n",
    "#             last_converged = info.get(\"converged\")\n",
    "\n",
    "#         # Check for stuck time\n",
    "#         current_t = None if t_now is None or np.isnan(t_now) else float(t_now)\n",
    "#         if current_t is not None:\n",
    "#             if last_t is not None and np.isclose(current_t, last_t, rtol=0.0, atol=1e-12):\n",
    "#                 repeat_count += 1\n",
    "#                 if repeat_count >= 10:\n",
    "#                     print(f\"[{run_name}] WARNING: time stuck at {current_t}\")\n",
    "#                     broke_stuck = True\n",
    "#                     break\n",
    "#             else:\n",
    "#                 repeat_count = 0\n",
    "#             last_t = current_t\n",
    "\n",
    "#         # Store trajectory data\n",
    "#         if xk is not None and current_t is not None:\n",
    "#             n = xk.shape[0] // 3\n",
    "#             v_block = xk[:n]\n",
    "#             u_block = xk[n:2*n]\n",
    "#             times.append(current_t)\n",
    "#             mean_vel.append(float(np.mean(v_block)))\n",
    "#             mean_slip.append(float(np.mean(u_block)))\n",
    "\n",
    "#     wall_time = time.time() - start\n",
    "\n",
    "#     # Determine success\n",
    "#     if last_converged is not None:\n",
    "#         success = bool(last_converged)\n",
    "#     else:\n",
    "#         if len(times) > 0 and hasattr(base_env, 'tnmax'):\n",
    "#             success = (times[-1] >= 0.999 * float(base_env.tnmax)) and not broke_stuck\n",
    "#         else:\n",
    "#             success = not broke_stuck\n",
    "\n",
    "#     # Final slip mean\n",
    "#     if last_xk is not None:\n",
    "#         n = last_xk.shape[0] // 3\n",
    "#         s_block = last_xk[2*n:3*n]\n",
    "#         s_end_mean = float(np.mean(s_block))\n",
    "#     else:\n",
    "#         s_end_mean = np.nan\n",
    "\n",
    "#     print(f\"[{run_name}] Done: wall_time={wall_time:.3f}s, steps={len(times)}, \"\n",
    "#           f\"final_t={(times[-1] if len(times)>0 else 'NA'):.3f}, \"\n",
    "#           f\"s_end={s_end_mean:.6e}\")\n",
    "\n",
    "#     return (\n",
    "#         np.asarray(times, dtype=float),\n",
    "#         np.asarray(mean_vel, dtype=float),\n",
    "#         np.asarray(mean_slip, dtype=float),\n",
    "#         float(wall_time),\n",
    "#         bool(success),\n",
    "#         s_end_mean,\n",
    "#     )\n",
    "\n",
    "# ###############################################################################\n",
    "# # Evaluate policies on a specific grid\n",
    "# ###############################################################################\n",
    "# def evaluate_policies_for_grid(grid, spec, alpha_dirs):\n",
    "#     \"\"\"Evaluate all trained policies on a specific grid configuration.\"\"\"\n",
    "\n",
    "#     v_max_override = spec['v_max']\n",
    "#     grid_label = spec['label']\n",
    "\n",
    "#     # Time step bounds (adjust as needed)\n",
    "#     local_dt_min = 1e-6\n",
    "#     local_dt_max = 30 / 5 * grid['fault'].second / grid['fault'].Tscale\n",
    "\n",
    "#     series = {}\n",
    "\n",
    "#     for alpha_dir in alpha_dirs:\n",
    "#         algo_dirs = sorted([p for p in alpha_dir.iterdir() if p.is_dir()])\n",
    "\n",
    "#         for algo_dir in algo_dirs:\n",
    "#             algo_name = algo_dir.name\n",
    "#             ModelClass = ALGO_REGISTRY.get(algo_name)\n",
    "#             if ModelClass is None:\n",
    "#                 continue\n",
    "\n",
    "#             meta_path = algo_dir / \"metadata.json\"\n",
    "#             model_path = algo_dir / \"model.zip\"\n",
    "#             if not model_path.exists():\n",
    "#                 model_path = algo_dir / \"model\"\n",
    "#             vecnorm_path = algo_dir / \"vec_norm.pkl\"\n",
    "\n",
    "#             if not meta_path.exists() or not model_path.exists():\n",
    "#                 continue\n",
    "\n",
    "#             with open(meta_path, \"r\", encoding=\"utf-8\") as fh:\n",
    "#                 meta = json.load(fh)\n",
    "\n",
    "#             alpha_value = float(meta.get(\"alpha\", np.nan))\n",
    "#             if not np.isfinite(alpha_value):\n",
    "#                 continue\n",
    "\n",
    "#             # Build reward/obs functions for this grid\n",
    "#             reward_fn, obs_fn = make_obs_reward(alpha_value, grid, v_max_override)\n",
    "\n",
    "#             # Environment factory\n",
    "#             def make_env():\n",
    "#                 obs_space = spaces.Box(\n",
    "#                     low=-np.inf,\n",
    "#                     high=np.inf,\n",
    "#                     shape=(6,),\n",
    "#                     dtype=np.float64\n",
    "#                 )\n",
    "#                 return AdaptiveStepperEnv(\n",
    "#                     system=grid['rhs'],\n",
    "#                     dt0=local_dt_min,\n",
    "#                     t0=t_span[0],\n",
    "#                     x0=grid['y0'],\n",
    "#                     tnmax=t_span[1],\n",
    "#                     dt_min=local_dt_min,\n",
    "#                     dt_max=local_dt_max,\n",
    "#                     nparams=(1e-6, 100),\n",
    "#                     integrator=grid['method_mp'],\n",
    "#                     component_slices=grid['component_slices'],\n",
    "#                     reward_fn=reward_fn,\n",
    "#                     obs_fn=obs_fn,\n",
    "#                     obs_space=obs_space,\n",
    "#                     verbose=False,\n",
    "#                     alpha=alpha_value,\n",
    "#                 )\n",
    "\n",
    "#             # Load model\n",
    "#             model = ModelClass.load(str(model_path), device=\"cpu\")\n",
    "\n",
    "#             # Run evaluations\n",
    "#             runtimes = []\n",
    "#             success_runs = []\n",
    "#             slip_end_list = []\n",
    "#             t_store = None\n",
    "#             v_store = None\n",
    "#             u_store = None\n",
    "\n",
    "#             key = f\"{algo_name} alpha={alpha_value:g}\"\n",
    "#             print(f\"\\nEvaluating {key} on grid {grid_label}...\")\n",
    "\n",
    "#             for run_idx in range(N_EVAL_RUNS):\n",
    "#                 # Create fresh env for each run\n",
    "#                 base_vec_env = DummyVecEnv([make_env])\n",
    "\n",
    "#                 # Load VecNormalize if available\n",
    "#                 if vecnorm_path.exists():\n",
    "#                     vec_env = VecNormalize.load(str(vecnorm_path), base_vec_env)\n",
    "#                     vec_env.training = False\n",
    "#                     vec_env.norm_obs = False  # Disable since we handle normalization\n",
    "#                     vec_env.norm_reward = False\n",
    "#                 else:\n",
    "#                     vec_env = base_vec_env\n",
    "\n",
    "#                 # Attach env to model\n",
    "#                 model.set_env(vec_env)\n",
    "\n",
    "#                 # Rollout\n",
    "#                 t_arr, v_mean, u_mean, wall, success, s_end_mean = rollout_policy_on_vecenv(\n",
    "#                     model, vec_env, run_name=f\"{key} run{run_idx}\"\n",
    "#                 )\n",
    "\n",
    "#                 runtimes.append(wall)\n",
    "#                 success_runs.append(success)\n",
    "#                 slip_end_list.append(s_end_mean)\n",
    "\n",
    "#                 if run_idx == 0:\n",
    "#                     t_store = t_arr\n",
    "#                     v_store = v_mean\n",
    "#                     u_store = u_mean\n",
    "\n",
    "#                 vec_env.close()\n",
    "\n",
    "#                 # Stop if first run failed (deterministic policy)\n",
    "#                 if run_idx == 0 and not success:\n",
    "#                     break\n",
    "\n",
    "#             # Record results if any run succeeded\n",
    "#             if any(success_runs):\n",
    "#                 times_arr = np.asarray(t_store, dtype=float) if t_store is not None else np.array([], dtype=float)\n",
    "#                 mean_v_arr = np.asarray(v_store, dtype=float) if v_store is not None else np.array([], dtype=float)\n",
    "#                 mean_u_arr = np.asarray(u_store, dtype=float) if u_store is not None else np.array([], dtype=float)\n",
    "#                 finite_slips = [s for s in slip_end_list if np.isfinite(s)]\n",
    "#                 s_end_mean = float(np.mean(finite_slips)) if finite_slips else float(\"nan\")\n",
    "#                 series[key] = dict(\n",
    "#                     times=times_arr,\n",
    "#                     mean_v=mean_v_arr,\n",
    "#                     mean_u=mean_u_arr,\n",
    "#                     runtime_s=float(np.mean(runtimes)),\n",
    "#                     runtime_std=float(np.std(runtimes)) if len(runtimes) > 1 else 0.0,\n",
    "#                     converged=True,\n",
    "#                     success_rate=sum(success_runs) / len(success_runs),\n",
    "#                     s_end_mean=s_end_mean,\n",
    "#                 )\n",
    "#                 print(f\"  Success rate: {series[key]['success_rate']:.1%}, \"\n",
    "#                       f\"Runtime: {series[key]['runtime_s']:.3f}s Â± {series[key]['runtime_std']:.3f}s\")\n",
    "\n",
    "#     return series\n",
    "\n",
    "# ###############################################################################\n",
    "# # PI Baseline\n",
    "# ###############################################################################\n",
    "# def adaptive_baseline_for_grid(grid, adaptive_opts):\n",
    "#     \"\"\"Run classical PI adaptive controller as baseline.\"\"\"\n",
    "\n",
    "#     projection_opts_nb = dict(\n",
    "#         con_force_func=grid['con_force'],\n",
    "#         rhok=np.ones(grid['N_DOFS'], dtype=float),\n",
    "#         component_slices=grid['component_slices'],\n",
    "#         constraint_indices=list(range(grid['N_DOFS'])),\n",
    "#         use_numba=True,\n",
    "#     )\n",
    "\n",
    "#     solver_opts = grid.get('solver_opts_ssn')\n",
    "#     if solver_opts is None:\n",
    "#         raise KeyError(\"Grid dictionary missing 'solver_opts_ssn'. Rebuild grid with build_fault_and_solver().\")\n",
    "\n",
    "#     local_adapt = dict(adaptive_opts)\n",
    "#     # local_adapt[\"use_PI\"] = True\n",
    "\n",
    "#     runtimes_b = []\n",
    "#     success_runs = []\n",
    "#     slip_end_list = []\n",
    "#     t_store = None\n",
    "#     y_store = None\n",
    "\n",
    "#     for run_idx in range(N_EVAL_RUNS):\n",
    "#         start_b = time.time()\n",
    "#         try:\n",
    "#             (t_vals_b, y_vals_b, h_vals_b, fk_vals_b, solver_info_b) = solve_nivp.solve_ivp_ns(\n",
    "#                 fun=grid['rhs'],\n",
    "#                 t_span=t_span,\n",
    "#                 y0=grid['y0'],\n",
    "#                 method='composite',\n",
    "#                 projection='coulomb',\n",
    "#                 solver='VI',\n",
    "#                 projection_opts=projection_opts_nb,\n",
    "#                 solver_opts=solver_opts,\n",
    "#                 adaptive=True,\n",
    "#                 adaptive_opts=local_adapt,\n",
    "#                 h0=local_adapt.get('h0', 5e-3),\n",
    "#                 component_slices=grid['component_slices'],\n",
    "#                 verbose=False,\n",
    "#                 A=grid['A'],\n",
    "#             )\n",
    "#             wall_b = time.time() - start_b\n",
    "\n",
    "#             success = (len(t_vals_b) > 0 and t_vals_b[-1] >= 0.999 * t_span[1])\n",
    "#             success_runs.append(success)\n",
    "#             runtimes_b.append(wall_b)\n",
    "\n",
    "#             n = grid['N_DOFS']\n",
    "#             s_final_block = y_vals_b[-1, 2*n:3*n]\n",
    "#             s_end_mean = float(np.mean(s_final_block))\n",
    "#             slip_end_list.append(s_end_mean)\n",
    "\n",
    "#             if run_idx == 0:\n",
    "#                 t_store = t_vals_b\n",
    "#                 y_store = y_vals_b\n",
    "\n",
    "#             print(f\"  PI baseline run {run_idx}: {wall_b:.3f}s, success={success}\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"  PI baseline run {run_idx} failed: {e}\")\n",
    "#             success_runs.append(False)\n",
    "\n",
    "#     if not any(success_runs):\n",
    "#         print(\"  WARNING: PI baseline failed all runs\")\n",
    "#         return None\n",
    "\n",
    "#     runtimes_b = [r for r in runtimes_b if np.isfinite(r)]\n",
    "\n",
    "#     n = grid['N_DOFS']\n",
    "#     v_b = np.mean(y_store[:, :n], axis=1)\n",
    "#     u_b = np.mean(y_store[:, n:2*n], axis=1)\n",
    "\n",
    "#     return dict(\n",
    "#         times=np.asarray(t_store, dtype=float) if t_store is not None else np.array([], dtype=float),\n",
    "#         mean_v=np.asarray(v_b, dtype=float),\n",
    "#         mean_u=np.asarray(u_b, dtype=float),\n",
    "#         runtime_s=float(np.mean(runtimes_b)) if runtimes_b else float(\"nan\"),\n",
    "#         runtime_std=float(np.std(runtimes_b)) if len(runtimes_b) > 1 else 0.0,\n",
    "#         converged=True,\n",
    "#         success_rate=sum(success_runs) / len(success_runs),\n",
    "#         s_end_mean=float(np.mean([s for s in slip_end_list if np.isfinite(s)])) if slip_end_list else float(\"nan\"),\n",
    "#     )\n",
    "\n",
    "# ###############################################################################\n",
    "# # Main Execution\n",
    "# ###############################################################################\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Discover trained models\n",
    "#     alpha_dirs = sorted([p for p in RUN_ROOT.glob(\"alpha_*\") if p.is_dir()])\n",
    "#     if not alpha_dirs:\n",
    "#         raise FileNotFoundError(f\"No trained runs found under {RUN_ROOT}\")\n",
    "\n",
    "#     # PI controller settings\n",
    "#     adaptive_opts_pi = dict(\n",
    "#         h0=5e-3,\n",
    "#         h_min=1e-7,\n",
    "#         h_down=0.6,\n",
    "#         h_up=1.8,\n",
    "#         method_order=1,\n",
    "#         controller=\"H211b\",\n",
    "#         b_param=4.0,\n",
    "#         skip_error_indices=[],\n",
    "#     )\n",
    "\n",
    "#     # Results storage\n",
    "#     all_results = {}\n",
    "\n",
    "#     # Evaluate on each grid\n",
    "#     for spec in GRID_SPECS:\n",
    "#         print(f\"\\n{'='*60}\")\n",
    "#         print(f\"Evaluating on {spec['label']} grid (v_max={spec['v_max']})\")\n",
    "#         print('='*60)\n",
    "\n",
    "#         # Build grid\n",
    "#         grid = build_fault_and_solver(spec[\"Nz\"], spec[\"Nx\"])\n",
    "\n",
    "#         # Evaluate RL policies\n",
    "#         series = evaluate_policies_for_grid(grid, spec, alpha_dirs)\n",
    "\n",
    "#         # Evaluate PI baseline\n",
    "#         print(f\"\\nRunning PI baseline on {spec['label']}...\")\n",
    "#         baseline = adaptive_baseline_for_grid(grid, adaptive_opts_pi)\n",
    "#         if baseline is not None:\n",
    "#             series[\"PI_baseline\"] = baseline\n",
    "\n",
    "#         all_results[spec['label']] = series\n",
    "\n",
    "#         # Save results\n",
    "#         cache_path = Path(f\"eval_results_5by5_{spec['label']}.json\")\n",
    "\n",
    "#         scale_t = grid['fault'].Tscale / grid['fault'].second\n",
    "#         scale_v = grid['fault'].Vscale\n",
    "#         scale_u = grid['fault'].Dscale\n",
    "\n",
    "#         # Convert numpy arrays to lists for JSON serialization (scaled to physical units)\n",
    "#         json_series = {}\n",
    "#         for key, data in series.items():\n",
    "#             times = np.asarray(data.get(\"times\", []), dtype=float)\n",
    "#             mean_v = np.asarray(data.get(\"mean_v\", []), dtype=float)\n",
    "#             mean_u = np.asarray(data.get(\"mean_u\", []), dtype=float)\n",
    "#             s_end_mean = data[\"s_end_mean\"] if \"s_end_mean\" in data else None\n",
    "\n",
    "#             times_scaled = (times * scale_t).tolist() if times.size > 0 else []\n",
    "#             mean_v_scaled = (mean_v * scale_v).tolist() if mean_v.size > 0 else []\n",
    "#             mean_u_scaled = (mean_u * scale_u).tolist() if mean_u.size > 0 else []\n",
    "#             if s_end_mean is not None and np.isfinite(s_end_mean):\n",
    "#                 s_end_mean_scaled = float(s_end_mean) * scale_u\n",
    "#             else:\n",
    "#                 s_end_mean_scaled = None\n",
    "\n",
    "#             json_series[key] = {\n",
    "#                 \"times\": times_scaled,\n",
    "#                 \"mean_v\": mean_v_scaled,\n",
    "#                 \"mean_u\": mean_u_scaled,\n",
    "#                 \"runtime_s\": data.get(\"runtime_s\", float(\"nan\")),\n",
    "#                 \"runtime_std\": data.get(\"runtime_std\", 0.0),\n",
    "#                 \"converged\": data.get(\"converged\", False),\n",
    "#                 \"success_rate\": data.get(\"success_rate\", 1.0),\n",
    "#                 \"s_end_mean\": s_end_mean_scaled,\n",
    "#             }\n",
    "\n",
    "#         with open(cache_path, \"w\") as f:\n",
    "#             json.dump(json_series, f, indent=2)\n",
    "#         print(f\"Saved results to {cache_path}\")\n",
    "\n",
    "#     # Plot mean velocity trajectories for each grid (scaled to physical units)\n",
    "#     for spec in GRID_SPECS:\n",
    "#         series = all_results.get(spec['label'], {})\n",
    "#         if not series:\n",
    "#             continue\n",
    "\n",
    "#         fault_obj = build_fault_and_solver(spec[\"Nz\"], spec[\"Nx\"])['fault']\n",
    "#         scale_t = fault_obj.Tscale / fault_obj.second\n",
    "#         scale_v = fault_obj.Vscale\n",
    "\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plotted = False\n",
    "#         for name, data in series.items():\n",
    "#             times = np.asarray(data.get(\"times\", []), dtype=float)\n",
    "#             mean_v = np.asarray(data.get(\"mean_v\", []), dtype=float)\n",
    "#             if times.size == 0 or mean_v.size == 0:\n",
    "#                 continue\n",
    "\n",
    "#             plt.plot(times * scale_t, mean_v * scale_v, label=name)\n",
    "#             plotted = True\n",
    "\n",
    "#         if not plotted:\n",
    "#             plt.close()\n",
    "#             continue\n",
    "\n",
    "#         plt.xlabel(\"Time (s)\")\n",
    "#         plt.ylabel(\"Mean velocity (m/s)\")\n",
    "#         plt.title(f\"Mean velocity vs time ({spec['label']} grid)\")\n",
    "#         plt.grid(True, alpha=0.3)\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "\n",
    "#         out_path = Path(f\"mean_velocity_{spec['label']}.png\")\n",
    "#         plt.savefig(out_path, dpi=150, bbox_inches='tight')\n",
    "#         print(f\"Saved velocity plot to {out_path}\")\n",
    "#         plt.show()\n",
    "\n",
    "#     print(\"\\n\" + \"=\"*60)\n",
    "#     print(\"EVALUATION COMPLETE\")\n",
    "#     print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f71966b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b9401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plants.faults import strikeslip  # you already have this import\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Minimal helper: build fault & return Dscale and N_DOFS for a grid\n",
    "# ------------------------------------------------------------------\n",
    "def build_fault_and_solver(Nz: int, Nx: int):\n",
    "    \"\"\"\n",
    "    Build a quasi-static strike-slip fault for given grid size and\n",
    "    return the fault object and its number of DOFs.\n",
    "\n",
    "    This is a minimal version for post-processing/plotting:\n",
    "    we only need `fault.Dscale` and `fault.N` (N_DOFS).\n",
    "    \"\"\"\n",
    "\n",
    "    fault_local = strikeslip.qs_strikeslip_fault(\n",
    "        zdepth=3,\n",
    "        xlength=3,\n",
    "        Nz=Nz,\n",
    "        Nx=Nx,\n",
    "        G=30000.0,\n",
    "        rho=2.5e-3,\n",
    "        zeta=0.8 / 3.0,\n",
    "        Ks_path=\"./Data/\",\n",
    "        gamma_s=25.0,\n",
    "        gamma_w=10.0,\n",
    "        sigma_ref=100.0,\n",
    "        depth_ini=0.0,\n",
    "        vinf=3.171e-10,\n",
    "        Dmu_estimate=0.5,\n",
    "    )\n",
    "\n",
    "    # number of DOFs on the fault\n",
    "    N_DOFS_l = fault_local.N\n",
    "\n",
    "    # return in the same structure that your loader expects\n",
    "    return {\n",
    "        \"fault\": fault_local,\n",
    "        \"N_DOFS\": N_DOFS_l,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eaa38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Assumes build_fault_and_solver(Nz, Nx) is already defined\n",
    "# in this file or imported from your evaluation module.\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------- helpers to load PI JSON results ----------\n",
    "\n",
    "def load_pi_jsons(glob_pattern: str,\n",
    "                  stem_prefix_to_strip: str,\n",
    "                  ignore_5by5: bool = True,\n",
    "                  verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Load PI_baseline runtime and s_end_mean from all JSON files\n",
    "    matching glob_pattern.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[label] -> dict(runtime_s, s_end)\n",
    "        label is e.g. '25x25', '50x50', ...\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    files = sorted(Path(\".\").glob(glob_pattern))\n",
    "    if verbose:\n",
    "        print(f\"\\nLoading PI_baseline from {glob_pattern}:\")\n",
    "        print(\"  Found:\", [f.name for f in files])\n",
    "\n",
    "    for path in files:\n",
    "        stem = path.stem\n",
    "        if ignore_5by5 and \"5by5\" in stem:\n",
    "            if verbose:\n",
    "                print(f\"  Skipping {path.name} (5by5)\")\n",
    "            continue\n",
    "\n",
    "        label = stem.replace(stem_prefix_to_strip, \"\")\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as fh:\n",
    "                data = json.load(fh)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"  [WARN] Failed to read {path.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        pi_data = data.get(\"PI_baseline\", {})\n",
    "        runtime = float(pi_data.get(\"runtime_s\", np.nan))\n",
    "        s_end = pi_data.get(\"s_end_mean\", None)\n",
    "        if s_end is None:\n",
    "            s_end = np.nan\n",
    "        else:\n",
    "            s_end = float(s_end)  # already scaled in your eval code\n",
    "\n",
    "        if not np.isfinite(runtime):\n",
    "            if verbose:\n",
    "                print(f\"  [WARN] {path.name}: invalid runtime {runtime}\")\n",
    "            continue\n",
    "\n",
    "        results[label] = dict(runtime_s=runtime, s_end=s_end)\n",
    "        if verbose:\n",
    "            print(f\"  {path.name}: label={label}, runtime={runtime:.3f}s, s_end={s_end:.3e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ---------- helper to load constant stepper NPZ ----------\n",
    "\n",
    "def load_constant_stepper_results(output_dir: str,\n",
    "                                  verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Load constant-stepper results for all files like\n",
    "    'constant_stepper_50by50.npz' in output_dir.\n",
    "\n",
    "    Uses build_fault_and_solver(Nz, Nx) to get N_DOFS and Dscale,\n",
    "    then computes average slip at final time and scales it.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[label] -> dict(runtime_s, s_end)\n",
    "        label is e.g. '25x25', '50x50', ...\n",
    "        runtime_s may be NaN if not stored in npz.\n",
    "    \"\"\"\n",
    "    const_results = {}\n",
    "    out_path = Path(output_dir)\n",
    "\n",
    "    pattern = \"constant_stepper_*by*.npz\"\n",
    "    files = sorted(out_path.glob(pattern))\n",
    "    if verbose:\n",
    "        print(f\"\\nLoading constant-stepper files from {output_dir}:\")\n",
    "        print(\"  Found:\", [f.name for f in files])\n",
    "\n",
    "    for npz_path in files:\n",
    "        stem = npz_path.stem  # e.g. 'constant_stepper_50by50'\n",
    "        grid_part = stem.replace(\"constant_stepper_\", \"\")  # '50by50'\n",
    "\n",
    "        try:\n",
    "            Nz_str, Nx_str = grid_part.split(\"by\")\n",
    "            Nz, Nx = int(Nz_str), int(Nx_str)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"  [WARN] Could not parse Nz,Nx from {stem}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # build fault to get Dscale and N_DOFS\n",
    "        grid = build_fault_and_solver(Nz, Nx)\n",
    "        fault = grid[\"fault\"]\n",
    "        N = grid[\"N_DOFS\"]\n",
    "        Dscale = fault.Dscale\n",
    "\n",
    "        data = np.load(npz_path)\n",
    "\n",
    "        if \"ts1\" not in data.files or \"ys1\" not in data.files:\n",
    "            if verbose:\n",
    "                print(f\"  [WARN] {npz_path.name} missing ts1/ys1; skipping.\")\n",
    "            continue\n",
    "\n",
    "        ts = data[\"ts1\"]\n",
    "        ys = data[\"ys1\"]\n",
    "\n",
    "        if ys.shape[1] < 3 * N:\n",
    "            if verbose:\n",
    "                print(f\"  [WARN] {npz_path.name}: ys shape {ys.shape} < 3*N={3*N}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # slip block (dimensionless), average over DOFs, then scale to physical\n",
    "        s_block = ys[:, 2*N:3*N]\n",
    "        avg_s_dimless = np.mean(s_block, axis=1)\n",
    "        avg_s_phys = avg_s_dimless * Dscale\n",
    "        s_end = float(avg_s_phys[-1])\n",
    "\n",
    "        # runtime if stored inside npz\n",
    "        if \"runtime_s\" in data.files:\n",
    "            runtime_s = float(data[\"runtime_s\"])\n",
    "        elif \"wall_time\" in data.files:\n",
    "            runtime_s = float(data[\"wall_time\"])\n",
    "        else:\n",
    "            runtime_s = np.nan\n",
    "            if verbose:\n",
    "                print(f\"  [INFO] {npz_path.name}: no runtime in file, setting runtime_s=NaN.\")\n",
    "\n",
    "        label = f\"{Nz}x{Nx}\"\n",
    "        const_results[label] = dict(runtime_s=runtime_s, s_end=s_end)\n",
    "        if verbose:\n",
    "            print(f\"  {npz_path.name}: label={label}, runtime={runtime_s:.3f}s, s_end={s_end:.3e}\")\n",
    "\n",
    "    return const_results\n",
    "\n",
    "\n",
    "# ---------- util for sorting labels like '25x25' ----------\n",
    "\n",
    "def _grid_sort_key(label: str) -> int:\n",
    "    try:\n",
    "        return int(label.split(\"x\")[0])\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# ---------- main plotting function ----------\n",
    "\n",
    "def scatter_runtime_vs_error(\n",
    "    const_output_dir: str,\n",
    "    threeby3_glob: str = \"eval_results_3by3_*.json\",\n",
    "    pionly_glob: str = \"eval_results_PI_*.json\",\n",
    "    save_path: str = \"scatter_PI_vs_constant.png\",\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Make a scatter plot (runtime vs slip error) comparing:\n",
    "\n",
    "      - Adaptive PI from 3by3 eval JSONs\n",
    "      - Adaptive PI from PI-only eval JSONs\n",
    "      - Constant time-stepper (NPZ files in const_output_dir)\n",
    "\n",
    "    Uses PI-only s_end as the reference for the error.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) load PI results\n",
    "    pi_3by3 = load_pi_jsons(\n",
    "        threeby3_glob,\n",
    "        stem_prefix_to_strip=\"eval_results_3by3_\",\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    pi_only = load_pi_jsons(\n",
    "        pionly_glob,\n",
    "        stem_prefix_to_strip=\"eval_results_PI_\",\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    # 2) load constant stepper results\n",
    "    const_res = load_constant_stepper_results(const_output_dir, verbose=verbose)\n",
    "\n",
    "    # 3) build combined table\n",
    "    all_labels = sorted(\n",
    "        set(pi_3by3.keys()) | set(pi_only.keys()) | set(const_res.keys()),\n",
    "        key=_grid_sort_key,\n",
    "    )\n",
    "    if verbose:\n",
    "        print(\"\\nAll grid labels found:\", all_labels)\n",
    "\n",
    "    rows = []\n",
    "    for label in all_labels:\n",
    "        r_pi3 = pi_3by3.get(label, {})\n",
    "        r_pio = pi_only.get(label, {})\n",
    "        r_con = const_res.get(label, {})\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"grid\": label,\n",
    "                \"runtime_PI_3by3_s\": r_pi3.get(\"runtime_s\", np.nan),\n",
    "                \"s_PI_3by3\": r_pi3.get(\"s_end\", np.nan),\n",
    "                \"runtime_PI_only_s\": r_pio.get(\"runtime_s\", np.nan),\n",
    "                \"s_PI_only\": r_pio.get(\"s_end\", np.nan),\n",
    "                \"runtime_const_s\": r_con.get(\"runtime_s\", np.nan),\n",
    "                \"s_const\": r_con.get(\"s_end\", np.nan),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(rows).set_index(\"grid\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nCombined DataFrame:\")\n",
    "        print(df)\n",
    "\n",
    "    # 4) choose reference accuracy (here: PI-only slip)\n",
    "    df[\"s_ref\"] = df[\"s_PI_only\"]\n",
    "\n",
    "    # errors\n",
    "    df[\"err_PI_3by3\"] = np.abs(df[\"s_PI_3by3\"] - df[\"s_ref\"])\n",
    "    df[\"err_PI_only\"] = np.abs(df[\"s_PI_only\"] - df[\"s_ref\"])  # zero where ref exists\n",
    "    df[\"err_const\"] = np.abs(df[\"s_const\"] - df[\"s_ref\"])\n",
    "\n",
    "    # 5) scatter plot: runtime vs error\n",
    "    plt.figure(figsize=(7, 6))\n",
    "\n",
    "    def _scatter_method(rt_col, err_col, label, marker):\n",
    "        mask = np.isfinite(df[rt_col]) & np.isfinite(df[err_col])\n",
    "        if not mask.any():\n",
    "            return\n",
    "        plt.scatter(\n",
    "            df.loc[mask, rt_col],\n",
    "            df.loc[mask, err_col],\n",
    "            marker=marker,\n",
    "            s=80,\n",
    "            label=label,\n",
    "        )\n",
    "        # annotate each point with grid label\n",
    "        for g in df.index[mask]:\n",
    "            x = df.loc[g, rt_col]\n",
    "            y = df.loc[g, err_col]\n",
    "            plt.text(x * 1.01, y, g, fontsize=8, va=\"center\")\n",
    "\n",
    "    _scatter_method(\"runtime_PI_3by3_s\", \"err_PI_3by3\", \"Adaptive PI (3by3 JSON)\", \"o\")\n",
    "    _scatter_method(\"runtime_PI_only_s\", \"err_PI_only\", \"Adaptive PI (PI-only JSON)\", \"s\")\n",
    "    _scatter_method(\"runtime_const_s\", \"err_const\", \"Constant stepper\", \"^\")\n",
    "\n",
    "    plt.xlabel(\"Runtime [s]\")\n",
    "    plt.ylabel(r\"| final mean slip âˆ’ reference |\")\n",
    "    plt.title(\"Runtime vs slip error\\n(reference = PI-only s_end\", fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------- run if executed as script ----------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # set this to the directory where your constant_stepper_*.npz live\n",
    "    CONST_OUTPUT_DIR = \".\"  # <-- change to your folder\n",
    "\n",
    "    df_summary = scatter_runtime_vs_error(\n",
    "        const_output_dir=CONST_OUTPUT_DIR,\n",
    "        threeby3_glob=\"eval_results_3by3_*.json\",\n",
    "        pionly_glob=\"eval_results_PI_*.json\",\n",
    "        save_path=\"scatter_PI_vs_constant.png\",\n",
    "        verbose=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc93d105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib.colors import BoundaryNorm, ListedColormap\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "# ============================================================\n",
    "# Global style (your settings)\n",
    "# ============================================================\n",
    "sizes = 32\n",
    "plt.rcParams.update({\n",
    "    'font.size': sizes,\n",
    "    'axes.titlesize': sizes,\n",
    "    'axes.labelsize': sizes,\n",
    "    'xtick.labelsize': sizes,\n",
    "    'ytick.labelsize': sizes,\n",
    "    'legend.fontsize': sizes,\n",
    "    'figure.titlesize': sizes,\n",
    "    'text.usetex': True,\n",
    "    'font.family': 'Times',\n",
    "    'pdf.fonttype': 42,\n",
    "    'ps.fonttype': 42,\n",
    "    'text.latex.preamble': r'\\usepackage{amsmath}\\usepackage{mathptmx}',\n",
    "    'axes.grid': False,\n",
    "})\n",
    "\n",
    "# optional cmocean\n",
    "try:\n",
    "    import cmocean\n",
    "    _HAS_CMOCEAN = True\n",
    "except Exception:\n",
    "    _HAS_CMOCEAN = False\n",
    "    print(\"[INFO] cmocean not found. Falling back to 'coolwarm'.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helpers: loading + ratio computation\n",
    "# ============================================================\n",
    "def _parse_alpha(method_name: str) -> float | None:\n",
    "    m = re.search(r'alpha\\s*=\\s*([0-9.+-eE]+)', method_name)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return float(m.group(1))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _aggregate_values(vals: list[float], how: str) -> float:\n",
    "    arr = np.array([v for v in vals if np.isfinite(v) and v > 0.0], dtype=float)\n",
    "    if arr.size == 0:\n",
    "        return np.nan\n",
    "    how = how.lower()\n",
    "    if how == \"min\":\n",
    "        return float(np.min(arr))\n",
    "    if how == \"mean\":\n",
    "        return float(np.mean(arr))\n",
    "    if how == \"max\":\n",
    "        return float(np.max(arr))\n",
    "    return float(np.min(arr))\n",
    "\n",
    "\n",
    "def compute_runtime_ratio_df(\n",
    "    results_glob: str,\n",
    "    aggregate: str = \"min\",\n",
    "    method_filter_prefix: str | None = None,\n",
    "    exclude_substrings: list[str] | None = None,\n",
    "    verbose: bool = True,\n",
    ") -> pd.DataFrame | None:\n",
    "    \"\"\"Return DF: index=discretization (e.g. '25x25'), columns=alphas, entries=PI/RL.\"\"\"\n",
    "    eval_files = sorted(Path(\".\").glob(results_glob))\n",
    "    if exclude_substrings:\n",
    "        eval_files = [\n",
    "            f for f in eval_files\n",
    "            if not any(sub in f.name for sub in exclude_substrings)\n",
    "        ]\n",
    "    if not eval_files:\n",
    "        print(f\"[WARN] No files for glob '{results_glob}' after excludes={exclude_substrings}\")\n",
    "        return None\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nLoading files for glob '{results_glob}' (excludes={exclude_substrings}):\")\n",
    "        print(\"  \", [f.name for f in eval_files])\n",
    "\n",
    "    all_data: dict[str, dict] = {}\n",
    "    for f in eval_files:\n",
    "        grid_label_full = f.stem.replace(\"eval_results_\", \"\")\n",
    "        with open(f, \"r\", encoding=\"utf-8\") as fh:\n",
    "            data = json.load(fh)\n",
    "        all_data[grid_label_full] = data\n",
    "        if verbose:\n",
    "            print(f\"  Loaded {f.name}: keys={list(data.keys())}\")\n",
    "\n",
    "    # collect all alphas\n",
    "    all_alphas = set()\n",
    "    for grid_data in all_data.values():\n",
    "        for method_name in grid_data.keys():\n",
    "            if method_name == \"PI_baseline\":\n",
    "                continue\n",
    "            if method_filter_prefix and not method_name.startswith(method_filter_prefix):\n",
    "                continue\n",
    "            a = _parse_alpha(method_name)\n",
    "            if a is not None:\n",
    "                all_alphas.add(a)\n",
    "\n",
    "    if not all_alphas:\n",
    "        print(f\"[WARN] No RL methods with alpha=* in '{results_glob}'\")\n",
    "        return None\n",
    "\n",
    "    alphas_sorted = sorted(all_alphas)\n",
    "    alpha_labels = [f\"{a:g}\" for a in alphas_sorted]\n",
    "\n",
    "    # index labels: strip e.g. \"3by3_25x25\" -> \"25x25\"\n",
    "    full_labels = list(all_data.keys())\n",
    "    disc_labels = [lab.split(\"_\")[-1] for lab in full_labels]\n",
    "\n",
    "    ratio_mat = np.full((len(full_labels), len(alphas_sorted)), np.nan, dtype=float)\n",
    "\n",
    "    for i, full_label in enumerate(full_labels):\n",
    "        grid_data = all_data[full_label]\n",
    "        pi_runtime = grid_data.get(\"PI_baseline\", {}).get(\"runtime_s\", np.nan)\n",
    "        if not (np.isfinite(pi_runtime) and pi_runtime > 0.0):\n",
    "            if verbose:\n",
    "                print(f\"  [WARN] No valid PI baseline for '{full_label}'\")\n",
    "            continue\n",
    "\n",
    "        per_alpha_values: dict[float, list[float]] = {a: [] for a in alphas_sorted}\n",
    "\n",
    "        for method_name, method_data in grid_data.items():\n",
    "            if method_name == \"PI_baseline\":\n",
    "                continue\n",
    "            if method_filter_prefix and not method_name.startswith(method_filter_prefix):\n",
    "                continue\n",
    "\n",
    "            a = _parse_alpha(method_name)\n",
    "            if a is None:\n",
    "                continue\n",
    "\n",
    "            converged = bool(method_data.get(\"converged\", False))\n",
    "            success_rate = float(method_data.get(\"success_rate\", 0.0))\n",
    "            if not (converged or success_rate > 0.0):\n",
    "                continue\n",
    "\n",
    "            rl_runtime = method_data.get(\"runtime_s\", np.nan)\n",
    "            if not (np.isfinite(rl_runtime) and rl_runtime > 0.0):\n",
    "                continue\n",
    "\n",
    "            for a_target in per_alpha_values.keys():\n",
    "                if abs(a - a_target) < 1e-12:\n",
    "                    per_alpha_values[a_target].append(float(rl_runtime))\n",
    "                    break\n",
    "\n",
    "        for j, a in enumerate(alphas_sorted):\n",
    "            rl_agg = _aggregate_values(per_alpha_values[a], aggregate)\n",
    "            if np.isfinite(rl_agg) and rl_agg > 0.0:\n",
    "                ratio_mat[i, j] = pi_runtime / rl_agg\n",
    "                if verbose:\n",
    "                    print(f\"  Grid {full_label:>12s}, alpha={a:g}: \"\n",
    "                          f\"PI={pi_runtime:.3f}, RL({aggregate})={rl_agg:.3f}, \"\n",
    "                          f\"ratio={ratio_mat[i,j]:.2f}\")\n",
    "\n",
    "    ratio_df = pd.DataFrame(ratio_mat, index=disc_labels, columns=alpha_labels)\n",
    "\n",
    "    # sort: discretization ascending, alpha ascending\n",
    "    def _disc_key(lbl: str):\n",
    "        try:\n",
    "            return int(lbl.split('x')[0])\n",
    "        except Exception:\n",
    "            return lbl\n",
    "\n",
    "    ratio_df = ratio_df.sort_index(key=lambda idx: [_disc_key(s) for s in idx])\n",
    "    ratio_df = ratio_df.reindex(sorted(ratio_df.columns, key=lambda s: float(s)), axis=1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nRatio DataFrame (index=disc, columns=alpha):\")\n",
    "        print(ratio_df)\n",
    "\n",
    "    return ratio_df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Plot helpers\n",
    "# ============================================================\n",
    "def _annotations(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if hasattr(df, \"map\"):\n",
    "        return df.map(lambda x: f\"{x:.2f}\" if pd.notna(x) else \"\")\n",
    "    return df.applymap(lambda x: f\"{x:.2f}\" if pd.notna(x) else \"\")\n",
    "\n",
    "\n",
    "def _overlay_na(ax, df: pd.DataFrame, fontsize=12, color=\"black\"):\n",
    "    for y in range(df.shape[0]):\n",
    "        for x in range(df.shape[1]):\n",
    "            if pd.isna(df.iat[y, x]):\n",
    "                ax.text(x + 0.5, y + 0.5, \"N/A\",\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        fontsize=fontsize, color=color)\n",
    "\n",
    "\n",
    "def _symmetric_limits_around_one(vals_all: np.ndarray):\n",
    "    vals_all = vals_all[np.isfinite(vals_all)]\n",
    "    if vals_all.size == 0:\n",
    "        return 0.5, 1.5\n",
    "    vmin_real = float(np.min(vals_all))\n",
    "    vmax_real = float(np.max(vals_all))\n",
    "    delta_low = max(0.0, 1.0 - vmin_real)\n",
    "    delta_high = max(0.0, vmax_real - 1.0)\n",
    "    half_span = max(delta_low, delta_high)\n",
    "    vmin_sym = max(1.0 - half_span, 1e-12)\n",
    "    vmax_sym = 1.0 + half_span\n",
    "    return vmin_sym, vmax_sym\n",
    "\n",
    "\n",
    "def _diverging_levels(vmin_sym: float, vmax_sym: float, n_bins_total: int):\n",
    "    if n_bins_total % 2 == 1:\n",
    "        n_bins_total += 1\n",
    "    n_half = n_bins_total // 2\n",
    "    lower = np.linspace(vmin_sym, 1.0, n_half + 1)[:-1]\n",
    "    upper = np.linspace(1.0, vmax_sym, n_half + 1)\n",
    "    return np.concatenate([lower, upper])  # len = n_bins_total + 1\n",
    "\n",
    "from matplotlib.colors import BoundaryNorm, ListedColormap\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "def create_runtime_ratio_dual_heatmap(\n",
    "    glob_a: str = \"eval_results_3by3_*.json\",\n",
    "    glob_b: str = \"eval_results_5by5_*.json\",\n",
    "    aggregate: str = \"min\",\n",
    "    method_filter_prefix: str | None = None,\n",
    "    exclude_substrings_a: list[str] | None = None,\n",
    "    exclude_substrings_b: list[str] | None = [\"_PI_\"],  # exclude 5by5_PI by default\n",
    "    save_path: str = \"runtime_ratio_3by3_vs_5by5_xalpha.pdf\",\n",
    "    n_bins_diverging: int = 10,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    # --------------------------------------------------------\n",
    "    # 1) Build the two ratio DataFrames\n",
    "    # --------------------------------------------------------\n",
    "    df_a = compute_runtime_ratio_df(\n",
    "        results_glob=glob_a,\n",
    "        aggregate=aggregate,\n",
    "        method_filter_prefix=method_filter_prefix,\n",
    "        exclude_substrings=exclude_substrings_a,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    df_b = compute_runtime_ratio_df(\n",
    "        results_glob=glob_b,\n",
    "        aggregate=aggregate,\n",
    "        method_filter_prefix=method_filter_prefix,\n",
    "        exclude_substrings=exclude_substrings_b,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    if df_a is None or df_b is None:\n",
    "        print(\"[ERROR] One of the DataFrames is None; aborting.\")\n",
    "        return None, None\n",
    "\n",
    "    vals_a = df_a.values[np.isfinite(df_a.values)]\n",
    "    vals_b = df_b.values[np.isfinite(df_b.values)]\n",
    "    if vals_a.size == 0 and vals_b.size == 0:\n",
    "        print(\"[ERROR] No finite ratios.\")\n",
    "        return df_a, df_b\n",
    "\n",
    "    if vals_a.size == 0:\n",
    "        all_vals = vals_b\n",
    "    elif vals_b.size == 0:\n",
    "        all_vals = vals_a\n",
    "    else:\n",
    "        all_vals = np.concatenate([vals_a, vals_b])\n",
    "\n",
    "    # symmetric limits around 1 so colour scale is centred\n",
    "    vmin_sym, vmax_sym = _symmetric_limits_around_one(all_vals)\n",
    "\n",
    "    # discrete bin boundaries\n",
    "    if n_bins_diverging % 2 == 1:\n",
    "        n_bins_diverging += 1\n",
    "    levels = _diverging_levels(vmin_sym, vmax_sym, n_bins_total=n_bins_diverging)\n",
    "    ncolors = len(levels) - 1\n",
    "\n",
    "    # base cmap: cmocean.balance if available, otherwise coolwarm\n",
    "    if _HAS_CMOCEAN:\n",
    "        base_cmap = cmocean.cm.balance\n",
    "    else:\n",
    "        base_cmap = plt.get_cmap(\"coolwarm\")\n",
    "\n",
    "    cmap_disc = ListedColormap(base_cmap(np.linspace(0, 1, ncolors)))\n",
    "    cmap_disc.set_bad(color=\"white\")\n",
    "    norm = BoundaryNorm(boundaries=levels, ncolors=ncolors)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2) Figure layout (more space between panels)\n",
    "    # --------------------------------------------------------\n",
    "    fig = plt.figure(figsize=(24, 8))\n",
    "    gs = fig.add_gridspec(\n",
    "        nrows=1, ncols=3,\n",
    "        width_ratios=[1, 1, 0.05],\n",
    "        wspace=0.4  # increase spacing between (a) and (b)\n",
    "    )\n",
    "\n",
    "    ax_a = fig.add_subplot(gs[0, 0])\n",
    "    ax_b = fig.add_subplot(gs[0, 1])\n",
    "    cax  = fig.add_subplot(gs[0, 2])\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 3) Panel (a): 3by3\n",
    "    # --------------------------------------------------------\n",
    "    ann_a = _annotations(df_a)\n",
    "    sns.heatmap(\n",
    "        df_a,\n",
    "        annot=ann_a.values,\n",
    "        fmt=\"\",\n",
    "        cmap=cmap_disc,\n",
    "        norm=norm,\n",
    "        mask=df_a.isna(),\n",
    "        cbar=False,\n",
    "        annot_kws={\"size\": sizes - 8},\n",
    "        linewidths=0.5,\n",
    "        linecolor=\"gray\",\n",
    "        square=True,\n",
    "        ax=ax_a,\n",
    "    )\n",
    "    _overlay_na(ax_a, df_a, fontsize=sizes - 6)\n",
    "    ax_a.set_xlabel(r'$\\alpha$', fontsize=sizes)\n",
    "    ax_a.set_ylabel(r'Discretization ($N_x$ by $N_z$)', fontsize=sizes)\n",
    "    ax_a.tick_params(axis='both', labelsize=sizes - 4)\n",
    "    ax_a.text(-0.07, 1.05, r'(a)',\n",
    "              transform=ax_a.transAxes,\n",
    "              fontsize=sizes,\n",
    "              va='bottom', ha='right')\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 4) Panel (b): 5by5\n",
    "    # --------------------------------------------------------\n",
    "    ann_b = _annotations(df_b)\n",
    "    sns.heatmap(\n",
    "        df_b,\n",
    "        annot=ann_b.values,\n",
    "        fmt=\"\",\n",
    "        cmap=cmap_disc,\n",
    "        norm=norm,\n",
    "        mask=df_b.isna(),\n",
    "        cbar=False,\n",
    "        annot_kws={\"size\": sizes - 8},\n",
    "        linewidths=0.5,\n",
    "        linecolor=\"gray\",\n",
    "        square=True,\n",
    "        ax=ax_b,\n",
    "    )\n",
    "    _overlay_na(ax_b, df_b, fontsize=sizes - 6)\n",
    "    ax_b.set_xlabel(r'$\\alpha$', fontsize=sizes)\n",
    "    ax_b.set_ylabel(r'Discretization ($N_x$ by $N_z$)', fontsize=sizes)\n",
    "    ax_b.tick_params(axis='both', labelsize=sizes - 4)\n",
    "    ax_b.text(-0.07, 1.2, r'(b)',\n",
    "              transform=ax_b.transAxes,\n",
    "              fontsize=sizes,\n",
    "              va='bottom', ha='right')\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 5) Shared colourbar â€“ always show 1 and some values below/above\n",
    "    # --------------------------------------------------------\n",
    "    sm = ScalarMappable(norm=norm, cmap=cmap_disc)\n",
    "    sm.set_array([])\n",
    "\n",
    "    cbar = fig.colorbar(sm, cax=cax)\n",
    "\n",
    "    # Build ticks: start from a coarse linspace, then inject 1.0 explicitly\n",
    "    n_ticks_base = min(5, ncolors + 1)\n",
    "    base_ticks = np.linspace(0, vmax_sym, n_ticks_base)\n",
    "    ticks = np.unique(np.concatenate([base_ticks, np.array([1.0])]))\n",
    "    # clamp to [vmin_sym, vmax_sym]\n",
    "    ticks = ticks[(ticks >= vmin_sym - 1e-9) & (ticks <= vmax_sym + 1e-9)]\n",
    "    ticks = np.sort(ticks)\n",
    "\n",
    "    # cbar.set_ticks(ticks)\n",
    "    # cbar.set_ticklabels([f\"{t:.2f}\" for t in ticks])\n",
    "    cbar.set_label(r'Runtime Ratio (PI / RL)', fontsize=sizes)\n",
    "    cbar.ax.tick_params(labelsize=sizes - 4)\n",
    "\n",
    "    fig.tight_layout(rect=[0.03, 0.05, 0.97, 0.95])\n",
    "    fig.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"Saved dual heatmap to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "    return df_a, df_b\n",
    "\n",
    "def create_runtime_ratio_dual_heatmap(\n",
    "    glob_a: str = \"eval_results_3by3_*.json\",\n",
    "    glob_b: str = \"eval_results_5by5_*.json\",\n",
    "    aggregate: str = \"min\",\n",
    "    method_filter_prefix: str | None = None,\n",
    "    exclude_substrings_a: list[str] | None = None,\n",
    "    exclude_substrings_b: list[str] | None = [\"_PI_\"],\n",
    "    save_path: str = \"runtime_ratio_3by3_vs_5by5_xalpha.pdf\",\n",
    "    n_bins_diverging: int = 10,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    # --------------------------------------------------------\n",
    "    # 1) Build the two ratio DataFrames\n",
    "    # --------------------------------------------------------\n",
    "    df_a = compute_runtime_ratio_df(\n",
    "        results_glob=glob_a,\n",
    "        aggregate=aggregate,\n",
    "        method_filter_prefix=method_filter_prefix,\n",
    "        exclude_substrings=exclude_substrings_a,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    df_b = compute_runtime_ratio_df(\n",
    "        results_glob=glob_b,\n",
    "        aggregate=aggregate,\n",
    "        method_filter_prefix=method_filter_prefix,\n",
    "        exclude_substrings=exclude_substrings_b,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    if df_a is None or df_b is None:\n",
    "        print(\"[ERROR] One of the DataFrames is None; aborting.\")\n",
    "        return None, None\n",
    "\n",
    "    vals_a = df_a.values[np.isfinite(df_a.values)]\n",
    "    vals_b = df_b.values[np.isfinite(df_b.values)]\n",
    "    if vals_a.size == 0 and vals_b.size == 0:\n",
    "        print(\"[ERROR] No finite ratios.\")\n",
    "        return df_a, df_b\n",
    "\n",
    "    if vals_a.size == 0:\n",
    "        all_vals = vals_b\n",
    "    elif vals_b.size == 0:\n",
    "        all_vals = vals_a\n",
    "    else:\n",
    "        all_vals = np.concatenate([vals_a, vals_b])\n",
    "\n",
    "    # symmetric limits around 1 so colour scale is centred\n",
    "    vmin_sym, vmax_sym = _symmetric_limits_around_one(all_vals)\n",
    "\n",
    "    # discrete bin boundaries\n",
    "    if n_bins_diverging % 2 == 1:\n",
    "        n_bins_diverging += 1\n",
    "    levels = _diverging_levels(vmin_sym, vmax_sym, n_bins_total=n_bins_diverging)\n",
    "    ncolors = len(levels) - 1\n",
    "\n",
    "    # base cmap: cmocean.balance if available, otherwise coolwarm\n",
    "    if _HAS_CMOCEAN:\n",
    "        base_cmap = cmocean.cm.balance\n",
    "    else:\n",
    "        base_cmap = plt.get_cmap(\"coolwarm\")\n",
    "\n",
    "    cmap_disc = ListedColormap(base_cmap(np.linspace(0, 1, ncolors)))\n",
    "    cmap_disc.set_bad(color=\"white\")\n",
    "    norm = BoundaryNorm(boundaries=levels, ncolors=ncolors)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2) Figure layout - only 2 columns for main plots\n",
    "    # --------------------------------------------------------\n",
    "    fig = plt.figure(figsize=(24, 8))\n",
    "    gs = fig.add_gridspec(\n",
    "        nrows=1, ncols=2,\n",
    "        width_ratios=[1, 1],\n",
    "        wspace=0.25  # Keep good spacing between (a) and (b)\n",
    "    )\n",
    "\n",
    "    ax_a = fig.add_subplot(gs[0, 0])\n",
    "    ax_b = fig.add_subplot(gs[0, 1])\n",
    "    \n",
    "    # Manually create colorbar axis close to plot (b)\n",
    "    # [left, bottom, width, height] in figure coordinates\n",
    "    cax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # Adjust 'left' to control distance\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 3) Panel (a): 3by3\n",
    "    # --------------------------------------------------------\n",
    "    ann_a = _annotations(df_a)\n",
    "    sns.heatmap(\n",
    "        df_a,\n",
    "        annot=ann_a.values,\n",
    "        fmt=\"\",\n",
    "        cmap=cmap_disc,\n",
    "        norm=norm,\n",
    "        mask=df_a.isna(),\n",
    "        cbar=False,\n",
    "        annot_kws={\"size\": sizes - 8},\n",
    "        linewidths=0.5,\n",
    "        linecolor=\"gray\",\n",
    "        square=True,\n",
    "        ax=ax_a,\n",
    "    )\n",
    "    _overlay_na(ax_a, df_a, fontsize=sizes - 6)\n",
    "    ax_a.set_xlabel(r'$\\alpha$', fontsize=sizes)\n",
    "    ax_a.set_ylabel(r'Discretization ($N_x$ by $N_z$)', fontsize=sizes)\n",
    "    ax_a.tick_params(axis='both', labelsize=sizes - 4)\n",
    "    ax_a.text(-0.07, 1.05, r'(a)',\n",
    "              transform=ax_a.transAxes,\n",
    "              fontsize=sizes,\n",
    "              va='bottom', ha='right')\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 4) Panel (b): 5by5\n",
    "    # --------------------------------------------------------\n",
    "    ann_b = _annotations(df_b)\n",
    "    sns.heatmap(\n",
    "        df_b,\n",
    "        annot=ann_b.values,\n",
    "        fmt=\"\",\n",
    "        cmap=cmap_disc,\n",
    "        norm=norm,\n",
    "        mask=df_b.isna(),\n",
    "        cbar=False,\n",
    "        annot_kws={\"size\": sizes - 8},\n",
    "        linewidths=0.5,\n",
    "        linecolor=\"gray\",\n",
    "        square=True,\n",
    "        ax=ax_b,\n",
    "    )\n",
    "    _overlay_na(ax_b, df_b, fontsize=sizes - 6)\n",
    "    ax_b.set_xlabel(r'$\\alpha$', fontsize=sizes)\n",
    "    ax_b.set_ylabel(r'Discretization ($N_x$ by $N_z$)', fontsize=sizes)\n",
    "    ax_b.tick_params(axis='both', labelsize=sizes - 4)\n",
    "    ax_b.text(-0.07, 1.05, r'(b)',\n",
    "              transform=ax_b.transAxes,\n",
    "              fontsize=sizes,\n",
    "              va='bottom', ha='right')\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 5) Shared colourbar with 1.0 explicitly shown\n",
    "    # --------------------------------------------------------\n",
    "    sm = ScalarMappable(norm=norm, cmap=cmap_disc)\n",
    "    sm.set_array([])\n",
    "\n",
    "    cbar = fig.colorbar(sm, cax=cax)\n",
    "\n",
    "    # Build ticks ensuring 1.0 is included\n",
    "    if vmax_sym > 10:\n",
    "        # For large ranges, select key values including 1.0\n",
    "        tick_candidates = [vmin_sym, 0.5, 0.8, 1.0, 5, 10, 15, 20, vmax_sym]\n",
    "        ticks = [t for t in tick_candidates if vmin_sym <= t <= vmax_sym]\n",
    "    else:\n",
    "        # For smaller ranges, linear spacing but force 1.0\n",
    "        n_ticks = 7\n",
    "        base_ticks = np.linspace(vmin_sym, vmax_sym, n_ticks)\n",
    "        # Replace closest value to 1.0 with exactly 1.0\n",
    "        if len(base_ticks) > 0:\n",
    "            idx_closest = np.argmin(np.abs(base_ticks - 1.0))\n",
    "            base_ticks[idx_closest] = 1.0\n",
    "        ticks = base_ticks\n",
    "    ticks = np.array([ 0.5, 1,  16, 32])\n",
    "\n",
    "    # Ensure ticks are sorted and unique\n",
    "    ticks = np.sort(np.unique(ticks))\n",
    "    \n",
    "    # Set the ticks and labels (with 1.00 explicitly formatted)\n",
    "    cbar.set_ticks(ticks)\n",
    "    tick_labels = [f\"{t:.2f}\" if abs(t - 1.0) > 0.01 else \"1.00\" for t in ticks]\n",
    "    cbar.set_ticklabels(tick_labels)\n",
    "    \n",
    "    cbar.set_label(r'Runtime Ratio (Heuristic / RL)', fontsize=sizes)\n",
    "    cbar.ax.tick_params(labelsize=sizes - 4)\n",
    "\n",
    "    fig.tight_layout(rect=[0.03, 0.05, 0.97, 0.95])\n",
    "    fig.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"Saved dual heatmap to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "    return df_a, df_b\n",
    "\n",
    "# ============================================================\n",
    "# Example call\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    df_3by3, df_5by5 = create_runtime_ratio_dual_heatmap(\n",
    "        glob_a=\"eval_results_3by3_*.json\",\n",
    "        glob_b=\"eval_results_5by5_*.json\",\n",
    "        aggregate=\"min\",\n",
    "        method_filter_prefix=None,       # or \"TQC \" if you only want TQC\n",
    "        exclude_substrings_a=None,\n",
    "        exclude_substrings_b=[\"_PI_\"],   # exclude eval_results_5by5_PI_*.json\n",
    "        save_path=\"figure5.pdf\",\n",
    "        n_bins_diverging=10,\n",
    "        verbose=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b29d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib.colors import BoundaryNorm, ListedColormap\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "# ============================================================\n",
    "# Global style (your settings)\n",
    "# ============================================================\n",
    "sizes = 32\n",
    "plt.rcParams.update({\n",
    "    'font.size': sizes,\n",
    "    'axes.titlesize': sizes,\n",
    "    'axes.labelsize': sizes,\n",
    "    'xtick.labelsize': sizes,\n",
    "    'ytick.labelsize': sizes,\n",
    "    'legend.fontsize': sizes,\n",
    "    'figure.titlesize': sizes,\n",
    "    'text.usetex': True,\n",
    "    'font.family': 'Times',\n",
    "    'pdf.fonttype': 42,\n",
    "    'ps.fonttype': 42,\n",
    "    'text.latex.preamble': r'\\usepackage{amsmath}\\usepackage{mathptmx}',\n",
    "    'axes.grid': False,\n",
    "})\n",
    "\n",
    "# optional cmocean\n",
    "try:\n",
    "    import cmocean\n",
    "    _HAS_CMOCEAN = True\n",
    "except Exception:\n",
    "    _HAS_CMOCEAN = False\n",
    "    print(\"[INFO] cmocean not found. Falling back to 'coolwarm'.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helpers: parsing + utilities\n",
    "# ============================================================\n",
    "def _parse_alpha(method_name: str) -> float | None:\n",
    "    \"\"\"Extract numeric alpha from method name like 'TQC alpha=4'.\"\"\"\n",
    "    m = re.search(r'alpha\\s*=\\s*([0-9.+-eE]+)', method_name)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return float(m.group(1))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _aggregate_values(vals: list[float], how: str) -> float:\n",
    "    \"\"\"Aggregate a list of runtimes with min/mean/max, skipping non-finite.\"\"\"\n",
    "    arr = np.array([v for v in vals if np.isfinite(v) and v > 0.0], dtype=float)\n",
    "    if arr.size == 0:\n",
    "        return np.nan\n",
    "    how = how.lower()\n",
    "    if how == \"min\":\n",
    "        return float(np.min(arr))\n",
    "    if how == \"mean\":\n",
    "        return float(np.mean(arr))\n",
    "    if how == \"max\":\n",
    "        return float(np.max(arr))\n",
    "    return float(np.min(arr))\n",
    "\n",
    "\n",
    "def _disc_key(lbl: str):\n",
    "    \"\"\"Sorting key for discretization strings like '25x25', '100x100'.\"\"\"\n",
    "    try:\n",
    "        return int(lbl.split('x')[0])\n",
    "    except Exception:\n",
    "        return lbl\n",
    "\n",
    "\n",
    "def _annotations(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Nicely formatted text annotations.\"\"\"\n",
    "    if hasattr(df, \"map\"):\n",
    "        return df.map(lambda x: f\"{x:.2f}\" if pd.notna(x) else \"\")\n",
    "    return df.applymap(lambda x: f\"{x:.2f}\" if pd.notna(x) else \"\")\n",
    "\n",
    "\n",
    "def _overlay_na(ax, df: pd.DataFrame, fontsize=12, color=\"black\"):\n",
    "    \"\"\"Write 'N/A' on NaN cells.\"\"\"\n",
    "    for y in range(df.shape[0]):\n",
    "        for x in range(df.shape[1]):\n",
    "            if pd.isna(df.iat[y, x]):\n",
    "                ax.text(x + 0.5, y + 0.5, \"N/A\",\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        fontsize=fontsize, color=color)\n",
    "\n",
    "\n",
    "def _symmetric_limits_around_one(vals_all: np.ndarray):\n",
    "    \"\"\"Symmetric colour limits around 1.0.\"\"\"\n",
    "    vals_all = vals_all[np.isfinite(vals_all)]\n",
    "    if vals_all.size == 0:\n",
    "        return 0.5, 1.5\n",
    "    vmin_real = float(np.min(vals_all))\n",
    "    vmax_real = float(np.max(vals_all))\n",
    "    delta_low = max(0.0, 1.0 - vmin_real)\n",
    "    delta_high = max(0.0, vmax_real - 1.0)\n",
    "    half_span = max(delta_low, delta_high)\n",
    "    vmin_sym = max(1.0 - half_span, 1e-12)\n",
    "    vmax_sym = 1.0 + half_span\n",
    "    return vmin_sym, vmax_sym\n",
    "\n",
    "\n",
    "def _diverging_levels(vmin_sym: float, vmax_sym: float, n_bins_total: int):\n",
    "    \"\"\"Discrete bin boundaries for diverging colormap centred at 1.\"\"\"\n",
    "    if n_bins_total % 2 == 1:\n",
    "        n_bins_total += 1\n",
    "    n_half = n_bins_total // 2\n",
    "    lower = np.linspace(vmin_sym, 1.0, n_half + 1)[:-1]  # exclude duplicate 1\n",
    "    upper = np.linspace(1.0, vmax_sym, n_half + 1)\n",
    "    return np.concatenate([lower, upper])  # length = n_bins_total + 1\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Load PI baselines from separate eval_results_*_PI_*.json\n",
    "# ============================================================\n",
    "def load_external_pi_baselines(\n",
    "    pi_glob: str,\n",
    "    exclude_substrings: list[str] | None = None,\n",
    "    verbose: bool = True,\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Load PI baseline runtimes from separate files.\n",
    "\n",
    "    Returns dict: { '25x25': runtime_s, ... } in *dimensionless* units\n",
    "    (exactly as stored in the JSON).\n",
    "    \"\"\"\n",
    "    pi_files = sorted(Path(\".\").glob(pi_glob))\n",
    "    if exclude_substrings:\n",
    "        pi_files = [\n",
    "            f for f in pi_files\n",
    "            if not any(sub in f.name for sub in exclude_substrings)\n",
    "        ]\n",
    "\n",
    "    pi_map: dict[str, float] = {}\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nLoading external PI baselines for glob '{pi_glob}':\")\n",
    "        print(\"  files:\", [f.name for f in pi_files])\n",
    "\n",
    "    for f in pi_files:\n",
    "        with open(f, \"r\", encoding=\"utf-8\") as fh:\n",
    "            data = json.load(fh)\n",
    "\n",
    "        # discretization label is always the last '_' chunk, e.g.\n",
    "        #   eval_results_PI_25x25      -> '25x25'\n",
    "        #   eval_results_5by5_PI_25x25 -> '25x25'\n",
    "        disc_label = f.stem.split(\"_\")[-1]\n",
    "\n",
    "        pi_runtime = data.get(\"PI_baseline\", {}).get(\"runtime_s\", np.nan)\n",
    "        if np.isfinite(pi_runtime) and pi_runtime > 0.0:\n",
    "            pi_map[disc_label] = float(pi_runtime)\n",
    "            if verbose:\n",
    "                print(f\"  {f.name}: disc={disc_label}, PI_runtime={pi_runtime:.3f}s\")\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"  [WARN] {f.name}: invalid PI runtime {pi_runtime}\")\n",
    "\n",
    "    return pi_map\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Compute PI/RL ratio using EXTERNAL PI baselines\n",
    "# ============================================================\n",
    "def compute_runtime_ratio_df_external_pi(\n",
    "    rl_glob: str,\n",
    "    pi_glob: str,\n",
    "    aggregate: str = \"min\",\n",
    "    method_filter_prefix: str | None = None,\n",
    "    rl_exclude_substrings: list[str] | None = None,\n",
    "    pi_exclude_substrings: list[str] | None = None,\n",
    "    verbose: bool = True,\n",
    ") -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Build a DataFrame of runtime ratios PI/RL for one family (3by3 or 5by5),\n",
    "    using *external* PI files for the baseline.\n",
    "\n",
    "    - rl_glob:      pattern for RL eval JSONs      (e.g. 'eval_results_3by3_*.json')\n",
    "    - pi_glob:      pattern for external PI JSONs (e.g. 'eval_results_PI_*.json')\n",
    "    - rl_exclude_substrings: optional list of substrings to skip in RL filenames\n",
    "                             (e.g. ['_PI_'] so you don't pick 5by5_PI* files)\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------- RL files ----------\n",
    "    rl_files = sorted(Path(\".\").glob(rl_glob))\n",
    "    if rl_exclude_substrings:\n",
    "        rl_files = [\n",
    "            f for f in rl_files\n",
    "            if not any(sub in f.name for sub in rl_exclude_substrings)\n",
    "        ]\n",
    "    if not rl_files:\n",
    "        print(f\"[WARN] No RL files for glob '{rl_glob}' after excludes={rl_exclude_substrings}\")\n",
    "        return None\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nLoading RL files for glob '{rl_glob}' (excludes={rl_exclude_substrings}):\")\n",
    "        print(\"  files:\", [f.name for f in rl_files])\n",
    "\n",
    "    rl_data: dict[str, dict] = {}\n",
    "    for f in rl_files:\n",
    "        with open(f, \"r\", encoding=\"utf-8\") as fh:\n",
    "            data = json.load(fh)\n",
    "        # discretization label: last '_' chunk, e.g. '3by3_25x25' -> '25x25'\n",
    "        disc_label = f.stem.split(\"_\")[-1]\n",
    "        rl_data[disc_label] = data\n",
    "        if verbose:\n",
    "            print(f\"  Loaded {f.name}: disc={disc_label}, keys={list(data.keys())}\")\n",
    "\n",
    "    # ---------- external PI runtimes ----------\n",
    "    pi_map = load_external_pi_baselines(\n",
    "        pi_glob=pi_glob,\n",
    "        exclude_substrings=pi_exclude_substrings,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    if not pi_map:\n",
    "        print(f\"[ERROR] No valid external PI baselines found for glob '{pi_glob}'\")\n",
    "        return None\n",
    "\n",
    "    # ---------- collect all alpha values from RL ----------\n",
    "    all_alphas = set()\n",
    "    for grid_data in rl_data.values():\n",
    "        for method_name in grid_data.keys():\n",
    "            if method_name == \"PI_baseline\":\n",
    "                continue  # ignore internal PI\n",
    "            if method_filter_prefix and not method_name.startswith(method_filter_prefix):\n",
    "                continue\n",
    "            a = _parse_alpha(method_name)\n",
    "            if a is not None:\n",
    "                all_alphas.add(a)\n",
    "\n",
    "    if not all_alphas:\n",
    "        print(f\"[WARN] No RL methods with alpha=* in RL glob '{rl_glob}'\")\n",
    "        return None\n",
    "\n",
    "    alphas_sorted = sorted(all_alphas)\n",
    "    alpha_labels = [f\"{a:g}\" for a in alphas_sorted]\n",
    "\n",
    "    # ---------- build ratio matrix ----------\n",
    "    disc_labels_sorted = sorted(rl_data.keys(), key=_disc_key)\n",
    "    ratio_mat = np.full((len(disc_labels_sorted), len(alphas_sorted)), np.nan, dtype=float)\n",
    "\n",
    "    for i, disc_label in enumerate(disc_labels_sorted):\n",
    "        grid_data = rl_data[disc_label]\n",
    "\n",
    "        pi_runtime = pi_map.get(disc_label, np.nan)\n",
    "        if not (np.isfinite(pi_runtime) and pi_runtime > 0.0):\n",
    "            if verbose:\n",
    "                print(f\"  [WARN] no external PI runtime for disc '{disc_label}'\")\n",
    "            continue\n",
    "\n",
    "        # collect RL runtimes per alpha\n",
    "        per_alpha_values: dict[float, list[float]] = {a: [] for a in alphas_sorted}\n",
    "\n",
    "        for method_name, method_data in grid_data.items():\n",
    "            if method_name == \"PI_baseline\":\n",
    "                continue  # ignore embedded PI\n",
    "            if method_filter_prefix and not method_name.startswith(method_filter_prefix):\n",
    "                continue\n",
    "\n",
    "            a = _parse_alpha(method_name)\n",
    "            if a is None:\n",
    "                continue\n",
    "\n",
    "            converged = bool(method_data.get(\"converged\", False))\n",
    "            success_rate = float(method_data.get(\"success_rate\", 0.0))\n",
    "            if not (converged or success_rate > 0.0):\n",
    "                continue\n",
    "\n",
    "            rl_runtime = method_data.get(\"runtime_s\", np.nan)\n",
    "            if not (np.isfinite(rl_runtime) and rl_runtime > 0.0):\n",
    "                continue\n",
    "\n",
    "            for a_target in per_alpha_values.keys():\n",
    "                if abs(a - a_target) < 1e-12:\n",
    "                    per_alpha_values[a_target].append(float(rl_runtime))\n",
    "                    break\n",
    "\n",
    "        for j, a in enumerate(alphas_sorted):\n",
    "            rl_agg = _aggregate_values(per_alpha_values[a], aggregate)\n",
    "            if np.isfinite(rl_agg) and rl_agg > 0.0:\n",
    "                ratio = pi_runtime / rl_agg\n",
    "                ratio_mat[i, j] = ratio\n",
    "                if verbose:\n",
    "                    print(f\"  disc={disc_label:>6s}, alpha={a:g}: \"\n",
    "                          f\"PI_ext={pi_runtime:.3f}, RL({aggregate})={rl_agg:.3f}, \"\n",
    "                          f\"ratio={ratio:.2f}\")\n",
    "\n",
    "    ratio_df = pd.DataFrame(ratio_mat, index=disc_labels_sorted, columns=alpha_labels)\n",
    "\n",
    "    # ensure alphas sorted numerically, rows by discretization\n",
    "    ratio_df = ratio_df.reindex(\n",
    "        sorted(ratio_df.columns, key=lambda s: float(s)),\n",
    "        axis=1\n",
    "    )\n",
    "    ratio_df = ratio_df.sort_index(key=lambda idx: [_disc_key(s) for s in idx])\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nRatio DataFrame (index=disc, columns=alpha):\")\n",
    "        print(ratio_df)\n",
    "\n",
    "    return ratio_df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Plot dual heatmap with shared discrete diverging colourbar\n",
    "# ============================================================\n",
    "def create_runtime_ratio_dual_heatmap_external_pi(\n",
    "    rl_glob_a: str = \"eval_results_3by3_*.json\",\n",
    "    pi_glob_a: str = \"eval_results_PI_*.json\",\n",
    "    rl_glob_b: str = \"eval_results_5by5_*.json\",\n",
    "    pi_glob_b: str = \"eval_results_5by5_PI_*.json\",\n",
    "    aggregate: str = \"min\",\n",
    "    method_filter_prefix: str | None = None,\n",
    "    rl_exclude_a: list[str] | None = None,\n",
    "    rl_exclude_b: list[str] | None = [\"_PI_\"],  # exclude 5by5_PI_* from RL glob\n",
    "    pi_exclude_a: list[str] | None = None,\n",
    "    pi_exclude_b: list[str] | None = None,\n",
    "    save_path: str = \"runtime_ratio_externalPI_3by3_vs_5by5.pdf\",\n",
    "    n_bins_diverging: int = 10,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    # --- build the two dataframes ---\n",
    "    df_a = compute_runtime_ratio_df_external_pi(\n",
    "        rl_glob=rl_glob_a,\n",
    "        pi_glob=pi_glob_a,\n",
    "        aggregate=aggregate,\n",
    "        method_filter_prefix=method_filter_prefix,\n",
    "        rl_exclude_substrings=rl_exclude_a,\n",
    "        pi_exclude_substrings=pi_exclude_a,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    df_b = compute_runtime_ratio_df_external_pi(\n",
    "        rl_glob=rl_glob_b,\n",
    "        pi_glob=pi_glob_b,\n",
    "        aggregate=aggregate,\n",
    "        method_filter_prefix=method_filter_prefix,\n",
    "        rl_exclude_substrings=rl_exclude_b,\n",
    "        pi_exclude_substrings=pi_exclude_b,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    if df_a is None or df_b is None:\n",
    "        print(\"[ERROR] One of the DataFrames is None; aborting.\")\n",
    "        return None, None\n",
    "\n",
    "    vals_a = df_a.values[np.isfinite(df_a.values)]\n",
    "    vals_b = df_b.values[np.isfinite(df_b.values)]\n",
    "\n",
    "    if vals_a.size == 0 and vals_b.size == 0:\n",
    "        print(\"[ERROR] No finite ratios to plot.\")\n",
    "        return df_a, df_b\n",
    "\n",
    "    if vals_a.size == 0:\n",
    "        all_vals = vals_b\n",
    "    elif vals_b.size == 0:\n",
    "        all_vals = vals_a\n",
    "    else:\n",
    "        all_vals = np.concatenate([vals_a, vals_b])\n",
    "\n",
    "    # symmetric limits around 1\n",
    "    vmin_sym, vmax_sym = _symmetric_limits_around_one(all_vals)\n",
    "\n",
    "    # discrete bins\n",
    "    if n_bins_diverging % 2 == 1:\n",
    "        n_bins_diverging += 1\n",
    "    levels = _diverging_levels(vmin_sym, vmax_sym, n_bins_total=n_bins_diverging)\n",
    "    ncolors = len(levels) - 1\n",
    "\n",
    "    # base colormap\n",
    "    if _HAS_CMOCEAN:\n",
    "        base_cmap = cmocean.cm.balance\n",
    "    else:\n",
    "        base_cmap = plt.get_cmap(\"coolwarm\")\n",
    "\n",
    "    cmap_disc = ListedColormap(base_cmap(np.linspace(0, 1, ncolors)))\n",
    "    cmap_disc.set_bad(color=\"white\")\n",
    "    norm = BoundaryNorm(boundaries=levels, ncolors=ncolors)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2) Figure layout - only 2 columns for main plots\n",
    "    # --------------------------------------------------------\n",
    "    fig = plt.figure(figsize=(24, 8))\n",
    "    gs = fig.add_gridspec(\n",
    "        nrows=1, ncols=2,\n",
    "        width_ratios=[1, 1],\n",
    "        wspace=0.25  # Keep good spacing between (a) and (b)\n",
    "    )\n",
    "\n",
    "    ax_a = fig.add_subplot(gs[0, 0])\n",
    "    ax_b = fig.add_subplot(gs[0, 1])\n",
    "    \n",
    "    # Manually create colorbar axis close to plot (b)\n",
    "    # [left, bottom, width, height] in figure coordinates\n",
    "    cax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # Adjust 'left' to control distance\n",
    "\n",
    "\n",
    "    # ---- Panel (a): 3by3 vs external PI ----\n",
    "    ann_a = _annotations(df_a)\n",
    "    sns.heatmap(\n",
    "        df_a,\n",
    "        annot=ann_a.values,\n",
    "        fmt=\"\",\n",
    "        cmap=cmap_disc,\n",
    "        norm=norm,\n",
    "        mask=df_a.isna(),\n",
    "        cbar=False,\n",
    "        annot_kws={\"size\": sizes - 8},\n",
    "        linewidths=0.5,\n",
    "        linecolor=\"gray\",\n",
    "        square=True,\n",
    "        ax=ax_a,\n",
    "    )\n",
    "    _overlay_na(ax_a, df_a, fontsize=sizes - 6)\n",
    "    ax_a.set_xlabel(r'$\\alpha$', fontsize=sizes)\n",
    "    ax_a.set_ylabel(r'Discretization ($N_x$ by $N_z$)', fontsize=sizes)\n",
    "    ax_a.tick_params(axis='both', labelsize=sizes - 4)\n",
    "    ax_a.text(-0.12, 1.05, r'(a)',\n",
    "              transform=ax_a.transAxes,\n",
    "              fontsize=sizes,\n",
    "              va='bottom', ha='left')\n",
    "\n",
    "    # ---- Panel (b): 5by5 vs external PI ----\n",
    "    ann_b = _annotations(df_b)\n",
    "    sns.heatmap(\n",
    "        df_b,\n",
    "        annot=ann_b.values,\n",
    "        fmt=\"\",\n",
    "        cmap=cmap_disc,\n",
    "        norm=norm,\n",
    "        mask=df_b.isna(),\n",
    "        cbar=False,\n",
    "        annot_kws={\"size\": sizes - 8},\n",
    "        linewidths=0.5,\n",
    "        linecolor=\"gray\",\n",
    "        square=True,\n",
    "        ax=ax_b,\n",
    "    )\n",
    "    _overlay_na(ax_b, df_b, fontsize=sizes - 6)\n",
    "    ax_b.set_xlabel(r'$\\alpha$', fontsize=sizes)\n",
    "    ax_b.set_ylabel(r'Discretization ($N_x$ by $N_z$)', fontsize=sizes)\n",
    "    ax_b.tick_params(axis='both', labelsize=sizes - 4)\n",
    "    ax_b.text(-0.12, 1.05, r'(b)',\n",
    "              transform=ax_b.transAxes,\n",
    "              fontsize=sizes,\n",
    "              va='bottom', ha='left')\n",
    "\n",
    "    # ---- Shared colourbar ----\n",
    "    sm = ScalarMappable(norm=norm, cmap=cmap_disc)\n",
    "    sm.set_array([])\n",
    "\n",
    "    cbar = fig.colorbar(sm, cax=cax)\n",
    "\n",
    "    ticks = [0.5,  1.0,  35,  vmax_sym]\n",
    "    # Filter to valid range\n",
    "    ticks = [t for t in ticks if vmin_sym <= t <= vmax_sym]\n",
    "    ticks = np.array([ 0.5, 1,   35,70])\n",
    "\n",
    "    ticks = np.sort(np.unique(ticks))\n",
    "\n",
    "    cbar.set_ticks(ticks)\n",
    "    tick_labels = []\n",
    "    for t in ticks:\n",
    "        if abs(t - 1.0) < 0.01:\n",
    "            tick_labels.append(\"1.00\")\n",
    "        elif t == int(t) and t >= 10:\n",
    "            tick_labels.append(f\"{int(t)}\")\n",
    "        else:\n",
    "            tick_labels.append(f\"{t:.1f}\")\n",
    "    cbar.set_ticklabels(tick_labels)\n",
    "    tick_labels = [f\"{t:.2f}\" if abs(t - 1.0) > 0.01 else \"1.00\" for t in ticks]\n",
    "    # tick_labels = [f\"{t:.2f}\" if abs(t - 1.0) > 0.01 else \"1.00\" for t in ticks]\n",
    "    cbar.set_ticklabels(tick_labels)\n",
    "    \n",
    "    cbar.set_label(r'Runtime Ratio (PI / RL)', fontsize=sizes)\n",
    "    cbar.ax.tick_params(labelsize=sizes - 4)\n",
    "    fig.tight_layout(rect=[0.03, 0.06, 0.97, 0.95])\n",
    "    fig.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"Saved dual heatmap (external PI) to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "    return df_a, df_b\n",
    "\n",
    "def create_runtime_ratio_heatmap_external_pi(\n",
    "    rl_glob: str = \"eval_results_3by3_*.json\",\n",
    "    pi_glob: str = \"eval_results_PI_*.json\",\n",
    "    aggregate: str = \"min\",\n",
    "    method_filter_prefix: str | None = None,\n",
    "    rl_exclude: list[str] | None = None,\n",
    "    pi_exclude: list[str] | None = None,\n",
    "    save_path: str = \"figure5a.pdf\",\n",
    "    n_bins_diverging: int = 10,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Single-panel version: only plot (a) = 3by3 vs external PI baselines.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- build the dataframe for 3by3 family ---\n",
    "    df = compute_runtime_ratio_df_external_pi(\n",
    "        rl_glob=rl_glob,\n",
    "        pi_glob=pi_glob,\n",
    "        aggregate=aggregate,\n",
    "        method_filter_prefix=method_filter_prefix,\n",
    "        rl_exclude_substrings=rl_exclude,\n",
    "        pi_exclude_substrings=pi_exclude,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    if df is None:\n",
    "        print(\"[ERROR] DataFrame is None; aborting.\")\n",
    "        return None\n",
    "\n",
    "    vals = df.values[np.isfinite(df.values)]\n",
    "    if vals.size == 0:\n",
    "        print(\"[ERROR] No finite ratios to plot.\")\n",
    "        return df\n",
    "\n",
    "    # symmetric limits around 1\n",
    "    vmin_sym, vmax_sym = _symmetric_limits_around_one(vals)\n",
    "\n",
    "    # discrete bins\n",
    "    if n_bins_diverging % 2 == 1:\n",
    "        n_bins_diverging += 1\n",
    "    levels = _diverging_levels(vmin_sym, vmax_sym, n_bins_total=n_bins_diverging)\n",
    "    ncolors = len(levels) - 1\n",
    "\n",
    "    # base colormap\n",
    "    if _HAS_CMOCEAN:\n",
    "        base_cmap = cmocean.cm.balance\n",
    "    else:\n",
    "        base_cmap = plt.get_cmap(\"coolwarm\")\n",
    "\n",
    "    cmap_disc = ListedColormap(base_cmap(np.linspace(0, 1, ncolors)))\n",
    "    cmap_disc.set_bad(color=\"white\")\n",
    "    norm = BoundaryNorm(boundaries=levels, ncolors=ncolors)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Figure layout: 1 panel + colourbar on the right\n",
    "    # --------------------------------------------------------\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    gs = fig.add_gridspec(\n",
    "        nrows=1, ncols=1,\n",
    "        width_ratios=[1],\n",
    "        wspace=0.2\n",
    "    )\n",
    "\n",
    "    ax = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "    # Manual colorbar axis\n",
    "    # [left, bottom, width, height] in figure coordinates\n",
    "    cax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "\n",
    "    # ---- Panel (a): 3by3 vs external PI ----\n",
    "    ann = _annotations(df)\n",
    "    sns.heatmap(\n",
    "        df,\n",
    "        annot=ann.values,\n",
    "        fmt=\"\",\n",
    "        cmap=cmap_disc,\n",
    "        norm=norm,\n",
    "        mask=df.isna(),\n",
    "        cbar=False,\n",
    "        annot_kws={\"size\": sizes - 8},\n",
    "        linewidths=0.5,\n",
    "        linecolor=\"gray\",\n",
    "        square=True,\n",
    "        ax=ax,\n",
    "    )\n",
    "    _overlay_na(ax, df, fontsize=sizes - 6)\n",
    "    ax.set_xlabel(r'$\\alpha$', fontsize=sizes)\n",
    "    ax.set_ylabel(r'Discretization ($N_x$ by $N_z$)', fontsize=sizes)\n",
    "    ax.tick_params(axis='both', labelsize=sizes - 4)\n",
    "    # ax.text(-0.12, 1.05, r'(a)',\n",
    "    #         transform=ax.transAxes,\n",
    "    #         fontsize=sizes,\n",
    "    #         va='bottom', ha='left')\n",
    "\n",
    "    # ---- Shared colourbar ----\n",
    "    sm = ScalarMappable(norm=norm, cmap=cmap_disc)\n",
    "    sm.set_array([])\n",
    "\n",
    "    cbar = fig.colorbar(sm, cax=cax)\n",
    "\n",
    "    # Keep your explicit tick choices (0.5, 1, 35, 70)\n",
    "    ticks = [0.5, 1.0, 15, 30]\n",
    "    ticks = [t for t in ticks if vmin_sym <= t <= vmax_sym]\n",
    "    # ticks = np.array([0.5, 1, 35, 70])  # as in your original code\n",
    "    ticks = np.sort(np.unique(ticks))\n",
    "\n",
    "    cbar.set_ticks(ticks)\n",
    "\n",
    "    tick_labels = [f\"{t:.2f}\" if abs(t - 1.0) > 0.01 else \"1.00\" for t in ticks]\n",
    "    cbar.set_ticklabels(tick_labels)\n",
    "\n",
    "    cbar.set_label(r'Runtime Ratio (PI / RL)', fontsize=sizes)\n",
    "    cbar.ax.tick_params(labelsize=sizes - 4)\n",
    "\n",
    "    fig.tight_layout(rect=[0.03, 0.06, 0.97, 0.95])\n",
    "    fig.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"Saved single-panel heatmap (external PI) to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "    return df\n",
    "\n",
    "# # ============================================================\n",
    "# # Example call\n",
    "# # ============================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     df_3by3, df_5by5 = create_runtime_ratio_heatmap_external_pi(\n",
    "#         rl_glob_a=\"eval_results_3by3_*.json\",     # RL on 3by3 grids\n",
    "#         pi_glob_a=\"eval_results_*.json\",       # external PI baselines (no 5by5)\n",
    "#         rl_glob_b=\"eval_results_5by5_*.json\",     # RL on 5by5 grids\n",
    "#         pi_glob_b=\"eval_results_5by5_PI_*.json\",  # external PI baselines for 5by5\n",
    "#         aggregate=\"min\",                          # fastest RL per alpha\n",
    "#         method_filter_prefix=None,                # or \"TQC \" if you only want TQC\n",
    "#         rl_exclude_a=None,                        # nothing special to exclude\n",
    "#         rl_exclude_b=[\"_PI_\"],                    # don't treat *_PI_* as RL\n",
    "#         pi_exclude_a=[\"_PI_\"],\n",
    "#         pi_exclude_b=None,\n",
    "#         save_path=\"figure5a.pdf\",\n",
    "#         n_bins_diverging=10,\n",
    "#         verbose=True,\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8513b6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_3by3 = create_runtime_ratio_heatmap_external_pi(\n",
    "              rl_glob=\"eval_results_3by3_*.json\",   # RL on 3by3 grids\n",
    "              # PI baselines: use the dedicated PI files, not the general ones\n",
    "              pi_glob=\"eval_results_*.json\",     # external PI baselines\n",
    "        aggregate=\"min\",                      # fastest RL per alpha\n",
    "             method_filter_prefix=None,            # or \"TQC \" if you only want TQC\n",
    "              rl_exclude=None,                      # nothing special to exclude\n",
    "              pi_exclude=[\"_PI_\"],                      # or e.g. [\"5by5\"] if needed\n",
    "             save_path=\"figure5a.pdf\",\n",
    "              n_bins_diverging=10,\n",
    "              verbose=True,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d221225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# from pathlib import Path\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # ============================================================\n",
    "# # Helpers to load PI JSON results\n",
    "# # ============================================================\n",
    "\n",
    "# def load_pi_jsons(glob_pattern: str,\n",
    "#                   stem_prefix_to_strip: str,\n",
    "#                   ignore_5by5: bool = True,\n",
    "#                   verbose: bool = True):\n",
    "#     \"\"\"\n",
    "#     Load PI_baseline runtime and s_end_mean from all JSON files\n",
    "#     matching glob_pattern.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     dict[label] -> dict(runtime_s, s_end)\n",
    "#         label is e.g. '25x25', '50x50', ...\n",
    "#     \"\"\"\n",
    "#     results = {}\n",
    "#     files = sorted(Path(\".\").glob(glob_pattern))\n",
    "#     if verbose:\n",
    "#         print(f\"\\nLoading PI_baseline from {glob_pattern}:\")\n",
    "#         print(\"  Found:\", [f.name for f in files])\n",
    "\n",
    "#     for path in files:\n",
    "#         stem = path.stem\n",
    "#         if ignore_5by5 and \"5by5\" in stem:\n",
    "#             if verbose:\n",
    "#                 print(f\"  Skipping {path.name} (5by5)\")\n",
    "#             continue\n",
    "\n",
    "#         label = stem.replace(stem_prefix_to_strip, \"\")\n",
    "#         try:\n",
    "#             with open(path, \"r\", encoding=\"utf-8\") as fh:\n",
    "#                 data = json.load(fh)\n",
    "#         except Exception as e:\n",
    "#             if verbose:\n",
    "#                 print(f\"  [WARN] Failed to read {path.name}: {e}\")\n",
    "#             continue\n",
    "\n",
    "#         pi_data = data.get(\"PI_baseline\", {})\n",
    "#         runtime = float(pi_data.get(\"runtime_s\", np.nan))\n",
    "#         s_end = pi_data.get(\"s_end_mean\", None)\n",
    "#         if s_end is None:\n",
    "#             s_end = np.nan\n",
    "#         else:\n",
    "#             # already scaled to physical units in your evaluation code\n",
    "#             s_end = float(s_end)\n",
    "\n",
    "#         if not np.isfinite(runtime):\n",
    "#             if verbose:\n",
    "#                 print(f\"  [WARN] {path.name}: invalid runtime {runtime}\")\n",
    "#             continue\n",
    "\n",
    "#         results[label] = dict(runtime_s=runtime, s_end=s_end)\n",
    "#         if verbose:\n",
    "#             print(f\"  {path.name}: label={label}, runtime={runtime:.3f}s, s_end={s_end:.3e}\")\n",
    "\n",
    "#     return results\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # Helper to load constant stepper NPZ results\n",
    "# # ============================================================\n",
    "\n",
    "# def load_constant_stepper_results(output_dir: str,\n",
    "#                                   verbose: bool = True):\n",
    "#     \"\"\"\n",
    "#     Load constant-stepper results for all files like\n",
    "#     'constant_stepper_50by50.npz' in output_dir.\n",
    "\n",
    "#     Uses build_fault_and_solver(Nz, Nx) to get N_DOFS and Dscale,\n",
    "#     then computes average slip at final time and scales it.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     dict[label] -> dict(runtime_s, s_end)\n",
    "#         label is e.g. '25x25', '50x50', ...\n",
    "#         runtime_s may be NaN if not stored in npz.\n",
    "#     \"\"\"\n",
    "#     # sanity check: build_fault_and_solver must exist\n",
    "#     if \"build_fault_and_solver\" not in globals():\n",
    "#         raise NameError(\n",
    "#             \"build_fault_and_solver(Nz, Nx) must be defined before \"\n",
    "#             \"calling load_constant_stepper_results.\"\n",
    "#         )\n",
    "\n",
    "#     const_results = {}\n",
    "#     out_path = Path(output_dir)\n",
    "\n",
    "#     pattern = \"constant_stepper_*by*.npz\"\n",
    "#     files = sorted(out_path.glob(pattern))\n",
    "#     if verbose:\n",
    "#         print(f\"\\nLoading constant-stepper files from {output_dir}:\")\n",
    "#         print(\"  Found:\", [f.name for f in files])\n",
    "\n",
    "#     for npz_path in files:\n",
    "#         stem = npz_path.stem  # e.g. 'constant_stepper_50by50'\n",
    "#         grid_part = stem.replace(\"constant_stepper_\", \"\")  # '50by50'\n",
    "\n",
    "#         try:\n",
    "#             Nz_str, Nx_str = grid_part.split(\"by\")\n",
    "#             Nz, Nx = int(Nz_str), int(Nx_str)\n",
    "#         except Exception as e:\n",
    "#             if verbose:\n",
    "#                 print(f\"  [WARN] Could not parse Nz,Nx from {stem}: {e}\")\n",
    "#             continue\n",
    "\n",
    "#         # build fault to get Dscale and N_DOFS\n",
    "#         grid = build_fault_and_solver(Nz, Nx)\n",
    "#         fault = grid[\"fault\"]\n",
    "#         N = grid[\"N_DOFS\"]\n",
    "#         Dscale = fault.Dscale\n",
    "\n",
    "#         data = np.load(npz_path)\n",
    "\n",
    "#         if \"ts1\" not in data.files or \"ys1\" not in data.files:\n",
    "#             if verbose:\n",
    "#                 print(f\"  [WARN] {npz_path.name} missing ts1/ys1; skipping.\")\n",
    "#             continue\n",
    "\n",
    "#         ts = data[\"ts1\"]\n",
    "#         ys = data[\"ys1\"]\n",
    "\n",
    "#         if ys.shape[1] < 3 * N:\n",
    "#             if verbose:\n",
    "#                 print(f\"  [WARN] {npz_path.name}: ys shape {ys.shape} < 3*N={3*N}; skipping.\")\n",
    "#             continue\n",
    "\n",
    "#         # slip block (dimensionless), average over DOFs, then scale to physical\n",
    "#         s_block = ys[:, 2*N:3*N]\n",
    "#         avg_s_dimless = np.mean(s_block, axis=1)\n",
    "#         avg_s_phys = avg_s_dimless * Dscale\n",
    "#         s_end = float(avg_s_phys[-1])\n",
    "\n",
    "#         # runtime if stored inside npz\n",
    "#         if \"runtime_s\" in data.files:\n",
    "#             runtime_s = float(data[\"runtime_s\"])\n",
    "#         elif \"wall_time\" in data.files:\n",
    "#             runtime_s = float(data[\"wall_time\"])\n",
    "#         else:\n",
    "#             runtime_s = np.nan\n",
    "#             if verbose:\n",
    "#                 print(f\"  [INFO] {npz_path.name}: no runtime in file, setting runtime_s=NaN.\")\n",
    "\n",
    "#         label = f\"{Nz}x{Nx}\"\n",
    "#         const_results[label] = dict(runtime_s=runtime_s, s_end=s_end)\n",
    "#         if verbose:\n",
    "#             print(f\"  {npz_path.name}: label={label}, runtime={runtime_s:.3f}s, s_end={s_end:.3e}\")\n",
    "\n",
    "#     return const_results\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # Sorting helper for labels like '25x25'\n",
    "# # ============================================================\n",
    "\n",
    "# def _grid_sort_key(label: str) -> int:\n",
    "#     try:\n",
    "#         return int(label.split(\"x\")[0])\n",
    "#     except Exception:\n",
    "#         return 0\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # Main plotting: ratios vs discretization, constant stepper reference\n",
    "# # ============================================================\n",
    "\n",
    "# def scatter_runtime_vs_error(\n",
    "#     const_output_dir: str,\n",
    "#     threeby3_glob: str = \"eval_results_3by3_*.json\",\n",
    "#     pionly_glob: str = \"eval_results_PI_*.json\",\n",
    "#     save_path: str = \"ratios_vs_discretization_const_ref.png\",\n",
    "#     verbose: bool = True,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Plot, versus discretization size, for each grid:\n",
    "\n",
    "#       Reference method: CONSTANT STEPPER (s_const, runtime_const_s)\n",
    "\n",
    "#       Panel (a): Runtime ratios (method / constant-stepper)\n",
    "#           - Adaptive PI from 3by3 JSON:    runtime_PI_3by3_s / runtime_const_s\n",
    "#           - Adaptive PI from PI-only JSON: runtime_PI_only_s / runtime_const_s\n",
    "\n",
    "#       Panel (b): Relative slip error vs constant-stepper slip\n",
    "#           - |s_PI_3by3 - s_const| / |s_const|\n",
    "#           - |s_PI_only - s_const| / |s_const|\n",
    "#     \"\"\"\n",
    "\n",
    "#     # 1) load PI results (3by3 + PI-only)\n",
    "#     pi_3by3 = load_pi_jsons(\n",
    "#         threeby3_glob,\n",
    "#         stem_prefix_to_strip=\"eval_results_3by3_\",\n",
    "#         verbose=verbose,\n",
    "#     )\n",
    "#     pi_only = load_pi_jsons(\n",
    "#         pionly_glob,\n",
    "#         stem_prefix_to_strip=\"eval_results_PI_\",\n",
    "#         verbose=verbose,\n",
    "#     )\n",
    "\n",
    "#     # 2) load constant stepper results (REFERENCE)\n",
    "#     const_res = load_constant_stepper_results(const_output_dir, verbose=verbose)\n",
    "\n",
    "#     # 3) build combined table\n",
    "#     all_labels = sorted(\n",
    "#         set(pi_3by3.keys()) | set(pi_only.keys()) | set(const_res.keys()),\n",
    "#         key=_grid_sort_key,\n",
    "#     )\n",
    "#     if verbose:\n",
    "#         print(\"\\nAll grid labels found:\", all_labels)\n",
    "\n",
    "#     rows = []\n",
    "#     for label in all_labels:\n",
    "#         r_pi3 = pi_3by3.get(label, {})\n",
    "#         r_pio = pi_only.get(label, {})\n",
    "#         r_con = const_res.get(label, {})\n",
    "\n",
    "#         rows.append(\n",
    "#             {\n",
    "#                 \"grid\": label,\n",
    "#                 \"runtime_PI_3by3_s\": r_pi3.get(\"runtime_s\", np.nan),\n",
    "#                 \"s_PI_3by3\": r_pi3.get(\"s_end\", np.nan),\n",
    "#                 \"runtime_PI_only_s\": r_pio.get(\"runtime_s\", np.nan),\n",
    "#                 \"s_PI_only\": r_pio.get(\"s_end\", np.nan),\n",
    "#                 \"runtime_const_s\": r_con.get(\"runtime_s\", np.nan),\n",
    "#                 \"s_const\": r_con.get(\"s_end\", np.nan),\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "#     df = pd.DataFrame(rows).set_index(\"grid\")\n",
    "\n",
    "#     if verbose:\n",
    "#         print(\"\\nCombined DataFrame (raw):\")\n",
    "#         print(df)\n",
    "\n",
    "#     # 4) add discretization size Nx for x-axis\n",
    "#     def _nx_from_label(lbl: str) -> float:\n",
    "#         try:\n",
    "#             return float(lbl.split(\"x\")[0])\n",
    "#         except Exception:\n",
    "#             return np.nan\n",
    "\n",
    "#     df[\"Nx\"] = [_nx_from_label(g) for g in df.index]\n",
    "#     df = df.sort_values(\"Nx\")\n",
    "\n",
    "#     # 5) Reference = CONSTANT STEPPER\n",
    "#     df[\"runtime_ref\"] = df[\"runtime_const_s\"]\n",
    "#     df[\"s_ref\"] = df[\"s_const\"]\n",
    "\n",
    "#     # 6) safe operations\n",
    "#     def safe_ratio(num, den):\n",
    "#         num = np.asarray(num, dtype=float)\n",
    "#         den = np.asarray(den, dtype=float)\n",
    "#         out = np.full_like(num, np.nan, dtype=float)\n",
    "#         mask = np.isfinite(num) & np.isfinite(den) & (den != 0.0)\n",
    "#         out[mask] = num[mask] / den[mask]\n",
    "#         return out\n",
    "\n",
    "#     def safe_rel_err(val, ref):\n",
    "#         val = np.asarray(val, dtype=float)\n",
    "#         ref = np.asarray(ref, dtype=float)\n",
    "#         out = np.full_like(val, np.nan, dtype=float)\n",
    "#         mask = np.isfinite(val) & np.isfinite(ref) & (ref != 0.0)\n",
    "#         out[mask] = np.abs(val[mask] - ref[mask]) / np.abs(ref[mask])\n",
    "#         return out\n",
    "\n",
    "#     # 7) Runtime ratios wrt constant stepper\n",
    "#     df[\"rt_ratio_PI_3by3\"] = safe_ratio(df[\"runtime_PI_3by3_s\"], df[\"runtime_ref\"])\n",
    "#     df[\"rt_ratio_PI_only\"] = safe_ratio(df[\"runtime_PI_only_s\"], df[\"runtime_ref\"])\n",
    "\n",
    "#     # 8) Relative slip errors wrt constant stepper slip\n",
    "#     df[\"rel_err_PI_3by3\"] = safe_rel_err(df[\"s_PI_3by3\"], df[\"s_ref\"])\n",
    "#     df[\"rel_err_PI_only\"] = safe_rel_err(df[\"s_PI_only\"], df[\"s_ref\"])\n",
    "\n",
    "#     if verbose:\n",
    "#         print(\"\\nDataFrame with constant-stepper reference:\")\n",
    "#         print(df[[\n",
    "#             \"Nx\",\n",
    "#             \"runtime_ref\",\n",
    "#             \"rt_ratio_PI_3by3\", \"rt_ratio_PI_only\",\n",
    "#             \"s_ref\",\n",
    "#             \"rel_err_PI_3by3\", \"rel_err_PI_only\",\n",
    "#         ]])\n",
    "\n",
    "#     # 9) Plot: two panels vs discretization size\n",
    "#     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), sharex=True)\n",
    "\n",
    "#     x = df[\"Nx\"].values\n",
    "#     xticklabels = df.index.tolist()\n",
    "\n",
    "#     # ---- Panel (a): runtime ratios ----\n",
    "#     mask_pi3 = np.isfinite(df[\"rt_ratio_PI_3by3\"])\n",
    "#     mask_pio = np.isfinite(df[\"rt_ratio_PI_only\"])\n",
    "\n",
    "#     if mask_pi3.any():\n",
    "#         ax1.plot(\n",
    "#             x[mask_pi3],\n",
    "#             df.loc[mask_pi3, \"rt_ratio_PI_3by3\"],\n",
    "#             \"o-\",\n",
    "#             label=\"Adaptive PI (3by3 JSON)\",\n",
    "#         )\n",
    "#     if mask_pio.any():\n",
    "#         ax1.plot(\n",
    "#             x[mask_pio],\n",
    "#             df.loc[mask_pio, \"rt_ratio_PI_only\"],\n",
    "#             \"s-\",\n",
    "#             label=\"Adaptive PI (PI-only JSON)\",\n",
    "#         )\n",
    "\n",
    "#     ax1.axhline(1.0, color=\"k\", linestyle=\"--\", linewidth=1)\n",
    "#     ax1.set_ylabel(\"Runtime ratio\\n(method / constant stepper)\")\n",
    "#     ax1.set_xlabel(r\"Discretization $N_x$ (from $N_x \\times N_z$)\")\n",
    "#     ax1.set_xticks(x)\n",
    "#     ax1.set_xticklabels(xticklabels, rotation=0)\n",
    "#     ax1.grid(True, alpha=0.3)\n",
    "#     ax1.legend()\n",
    "\n",
    "#     # ---- Panel (b): relative slip errors ----\n",
    "#     mask_pi3_e = np.isfinite(df[\"rel_err_PI_3by3\"])\n",
    "#     mask_pio_e = np.isfinite(df[\"rel_err_PI_only\"])\n",
    "\n",
    "#     if mask_pi3_e.any():\n",
    "#         ax2.plot(\n",
    "#             x[mask_pi3_e],\n",
    "#             df.loc[mask_pi3_e, \"rel_err_PI_3by3\"],\n",
    "#             \"o-\",\n",
    "#             label=\"Adaptive PI (3by3 JSON)\",\n",
    "#         )\n",
    "#     if mask_pio_e.any():\n",
    "#         ax2.plot(\n",
    "#             x[mask_pio_e],\n",
    "#             df.loc[mask_pio_e, \"rel_err_PI_only\"],\n",
    "#             \"s-\",\n",
    "#             label=\"Adaptive PI (PI-only JSON)\",\n",
    "#         )\n",
    "\n",
    "#     ax2.axhline(0.0, color=\"k\", linestyle=\"--\", linewidth=1)\n",
    "#     ax2.set_ylabel(\n",
    "#         r\"Relative slip error w.r.t constant stepper\"\n",
    "#         + \"\\n\"\n",
    "#         + r\"$|s - s_{\\rm const}|/|s_{\\rm const}|$\"\n",
    "#     )\n",
    "#     ax2.set_xlabel(r\"Discretization $N_x$ (from $N_x \\times N_z$)\")\n",
    "#     ax2.set_xticks(x)\n",
    "#     ax2.set_xticklabels(xticklabels, rotation=0)\n",
    "#     ax2.grid(True, alpha=0.3)\n",
    "#     ax2.legend()\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     fig.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "#     print(f\"\\nSaved ratios figure (constant-stepper reference) to {save_path}\")\n",
    "#     plt.show()\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # Run if executed as a script / from this cell\n",
    "# # ============================================================\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # directory where your constant_stepper_*.npz live\n",
    "#     CONST_OUTPUT_DIR = \".\"  # adjust if needed\n",
    "\n",
    "#     df_summary = scatter_runtime_vs_error(\n",
    "#         const_output_dir=CONST_OUTPUT_DIR,\n",
    "#         threeby3_glob=\"eval_results_3by3_*.json\",\n",
    "#         pionly_glob=\"eval_results_PI_*.json\",\n",
    "#         save_path=\"ratios_vs_discretization_const_ref.png\",\n",
    "#         verbose=True,\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffee245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib.colors import BoundaryNorm, ListedColormap\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "from plants.faults import strikeslip  # you already have this in your project\n",
    "\n",
    "# ============================================================\n",
    "# Global style (your settings)\n",
    "# ============================================================\n",
    "sizes = 30\n",
    "plt.rcParams.update({\n",
    "    'font.size': sizes,\n",
    "    'axes.titlesize': sizes,\n",
    "    'axes.labelsize': sizes,\n",
    "    'xtick.labelsize': sizes,\n",
    "    'ytick.labelsize': sizes,\n",
    "    'legend.fontsize': sizes,\n",
    "    'figure.titlesize': sizes,\n",
    "    'text.usetex': True,\n",
    "    'font.family': 'Times',\n",
    "    'pdf.fonttype': 42,\n",
    "    'ps.fonttype': 42,\n",
    "    'text.latex.preamble': r'\\usepackage{amsmath}\\usepackage{mathptmx}',\n",
    "    'axes.grid': False,\n",
    "})\n",
    "\n",
    "# optional cmocean\n",
    "try:\n",
    "    import cmocean\n",
    "    _HAS_CMOCEAN = True\n",
    "except Exception:\n",
    "    _HAS_CMOCEAN = False\n",
    "    print(\"[INFO] cmocean not found. Falling back to 'coolwarm' for diverging maps.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Minimal fault builder (patch-size aware)\n",
    "# ============================================================\n",
    "def build_fault_and_solver(Nz: int, Nx: int, patch_size: int = 3):\n",
    "    \"\"\"\n",
    "    Minimal helper for post-processing: build a quasi-static strike-slip fault\n",
    "    for a given grid and patch size; return fault and N_DOFS.\n",
    "\n",
    "    patch_size = zdepth = xlength = 3 (for 3x3 case) or 5 (for 5x5 case).\n",
    "    \"\"\"\n",
    "    fault_local = strikeslip.qs_strikeslip_fault(\n",
    "        zdepth=patch_size,\n",
    "        xlength=patch_size,\n",
    "        Nz=Nz,\n",
    "        Nx=Nx,\n",
    "        G=30000.0,\n",
    "        rho=2.5e-3,\n",
    "        zeta=0.8 / patch_size,\n",
    "        Ks_path=\"./Data/\",\n",
    "        gamma_s=25.0,\n",
    "        gamma_w=10.0,\n",
    "        sigma_ref=100.0,\n",
    "        depth_ini=0.0,\n",
    "        vinf=3.171e-10,\n",
    "        Dmu_estimate=0.5,\n",
    "    )\n",
    "\n",
    "    N_DOFS_l = fault_local.N\n",
    "    return {\n",
    "        \"fault\": fault_local,\n",
    "        \"N_DOFS\": N_DOFS_l,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Low-level helpers\n",
    "# ============================================================\n",
    "def _parse_alpha(method_name: str) -> float | None:\n",
    "    m = re.search(r'alpha\\s*=\\s*([0-9.+-eE]+)', method_name)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return float(m.group(1))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _aggregate_values(vals: list[float], how: str) -> float:\n",
    "    arr = np.array([v for v in vals if np.isfinite(v) and v >= 0.0], dtype=float)\n",
    "    if arr.size == 0:\n",
    "        return np.nan\n",
    "    how = how.lower()\n",
    "    if how == \"min\":\n",
    "        return float(np.min(arr))\n",
    "    if how == \"mean\":\n",
    "        return float(np.mean(arr))\n",
    "    if how == \"max\":\n",
    "        return float(np.max(arr))\n",
    "    return float(np.min(arr))\n",
    "\n",
    "\n",
    "def _annotations(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if hasattr(df, \"map\"):\n",
    "        return df.map(lambda x: f\"{x:.2f}\" if pd.notna(x) else \"\")\n",
    "    return df.applymap(lambda x: f\"{x:.2f}\" if pd.notna(x) else \"\")\n",
    "\n",
    "\n",
    "def _overlay_na(ax, df: pd.DataFrame, fontsize=12, color=\"black\"):\n",
    "    for y in range(df.shape[0]):\n",
    "        for x in range(df.shape[1]):\n",
    "            if pd.isna(df.iat[y, x]):\n",
    "                ax.text(x + 0.5, y + 0.5, \"N/A\",\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        fontsize=fontsize, color=color)\n",
    "\n",
    "\n",
    "def _symmetric_limits_around_one(vals_all: np.ndarray):\n",
    "    vals_all = vals_all[np.isfinite(vals_all)]\n",
    "    if vals_all.size == 0:\n",
    "        return 0.5, 1.5\n",
    "    vmin_real = float(np.min(vals_all))\n",
    "    vmax_real = float(np.max(vals_all))\n",
    "    delta_low = max(0.0, 1.0 - vmin_real)\n",
    "    delta_high = max(0.0, vmax_real - 1.0)\n",
    "    half_span = max(delta_low, delta_high)\n",
    "    vmin_sym = max(1.0 - half_span, 1e-12)\n",
    "    vmax_sym = 1.0 + half_span\n",
    "    return vmin_sym, vmax_sym\n",
    "\n",
    "\n",
    "def _diverging_levels(vmin_sym: float, vmax_sym: float, n_bins_total: int):\n",
    "    if n_bins_total % 2 == 1:\n",
    "        n_bins_total += 1\n",
    "    n_half = n_bins_total // 2\n",
    "    lower = np.linspace(vmin_sym, 1.0, n_half + 1)[:-1]\n",
    "    upper = np.linspace(1.0, vmax_sym, n_half + 1)\n",
    "    return np.concatenate([lower, upper])  # len = n_bins_total + 1\n",
    "\n",
    "\n",
    "def _sequential_levels(vals_all: np.ndarray, n_bins: int):\n",
    "    vals_all = vals_all[np.isfinite(vals_all)]\n",
    "    if vals_all.size == 0:\n",
    "        vmin, vmax = 0.0, 1.0\n",
    "    else:\n",
    "        vmin = float(np.min(vals_all))\n",
    "        vmax = float(np.max(vals_all))\n",
    "        if vmin == vmax:\n",
    "            vmin = max(0.0, vmin - 1e-6)\n",
    "            vmax = vmin + 1e-6\n",
    "    return np.linspace(vmin, vmax, int(n_bins) + 1)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Constant-stepper loader (scenario-aware)\n",
    "# ============================================================\n",
    "def load_constant_stepper_results(output_dir: str,\n",
    "                                  file_pattern: str,\n",
    "                                  patch_size: int,\n",
    "                                  exclude_substrings: list[str] | None = None,\n",
    "                                  verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Load constant-stepper results for files like:\n",
    "\n",
    "      - \"constant_stepper_25by25.npz\"          (for 3x3 scenario)\n",
    "      - \"constant_stepper_25by25_5by5.npz\"    (for 5x5 scenario)\n",
    "\n",
    "    `patch_size` controls zdepth=xlength used when building the fault\n",
    "    (3 for 3x3, 5 for 5x5).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[label] -> dict(runtime_s, s_end)\n",
    "        label is e.g. '25x25', '50x50', ...\n",
    "        runtime_s may be NaN if not stored in npz.\n",
    "    \"\"\"\n",
    "    const_results: dict[str, dict] = {}\n",
    "    out_path = Path(output_dir)\n",
    "\n",
    "    files = sorted(out_path.glob(file_pattern))\n",
    "    if exclude_substrings:\n",
    "        files = [\n",
    "            f for f in files\n",
    "            if not any(sub in f.name for sub in exclude_substrings)\n",
    "        ]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nLoading constant-stepper files from {output_dir}, pattern='{file_pattern}':\")\n",
    "        print(\"  Found:\", [f.name for f in files])\n",
    "\n",
    "    for npz_path in files:\n",
    "        stem = npz_path.stem  # e.g. 'constant_stepper_25by25' or 'constant_stepper_25by25_5by5'\n",
    "        rest = stem.replace(\"constant_stepper_\", \"\")  # '25by25' or '25by25_5by5'\n",
    "        grid_token = rest.split(\"_\")[0]              # always '25by25'\n",
    "\n",
    "        try:\n",
    "            Nz_str, Nx_str = grid_token.split(\"by\")\n",
    "            Nz, Nx = int(Nz_str), int(Nx_str)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"  [WARN] Could not parse Nz,Nx from '{rest}' in {stem}: {e}\")\n",
    "            continue\n",
    "\n",
    "        grid = build_fault_and_solver(Nz, Nx, patch_size=patch_size)\n",
    "        fault = grid[\"fault\"]\n",
    "        N = grid[\"N_DOFS\"]\n",
    "        Dscale = fault.Dscale\n",
    "\n",
    "        data = np.load(npz_path)\n",
    "\n",
    "        if \"ts1\" not in data.files or \"ys1\" not in data.files:\n",
    "            if verbose:\n",
    "                print(f\"  [WARN] {npz_path.name} missing ts1/ys1; skipping.\")\n",
    "            continue\n",
    "\n",
    "        ys = data[\"ys1\"]\n",
    "\n",
    "        if ys.shape[1] < 3 * N:\n",
    "            if verbose:\n",
    "                print(f\"  [WARN] {npz_path.name}: ys shape {ys.shape} < 3*N={3*N}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # slip block (dimensionless) -> average over DOFs -> scale to physical\n",
    "        s_block = ys[:, 2*N:3*N]\n",
    "        avg_s_dimless = np.mean(s_block, axis=1)\n",
    "        avg_s_phys = avg_s_dimless * Dscale\n",
    "        s_end = float(avg_s_phys[-1])\n",
    "\n",
    "        # runtime if stored inside npz\n",
    "        if \"runtime_s\" in data.files:\n",
    "            runtime_s = float(data[\"runtime_s\"])\n",
    "        elif \"wall_time\" in data.files:\n",
    "            runtime_s = float(data[\"wall_time\"])\n",
    "        else:\n",
    "            runtime_s = np.nan\n",
    "            if verbose:\n",
    "                print(f\"  [INFO] {npz_path.name}: no runtime in file, setting runtime_s=NaN.\")\n",
    "\n",
    "        label = f\"{Nz}x{Nx}\"\n",
    "        const_results[label] = dict(runtime_s=runtime_s, s_end=s_end)\n",
    "        if verbose:\n",
    "            print(f\"  {npz_path.name}: label={label}, runtime={runtime_s:.3f}s, s_end={s_end:.3e}\")\n",
    "\n",
    "    return const_results\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Error matrices (RL relative error, PI/RL error ratio)\n",
    "# ============================================================\n",
    "def compute_error_dfs_for_dataset(\n",
    "    results_glob: str,\n",
    "    const_results: dict[str, dict],\n",
    "    aggregate: str = \"min\",\n",
    "    method_filter_prefix: str | None = None,\n",
    "    exclude_substrings: list[str] | None = None,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    For a given eval_results_*.json family (3by3 or 5by5), build:\n",
    "\n",
    "      - rl_err_df:   RL relative error heatmap vs constant stepper (in %)\n",
    "      - ratio_df:    (PI relative error) / (RL relative error)  (dimensionless)\n",
    "\n",
    "    Both are DataFrames with:\n",
    "        index  = discretization label (e.g. '25x25', '50x50')\n",
    "        columns = alpha values (as strings)\n",
    "    \"\"\"\n",
    "    eval_files = sorted(Path(\".\").glob(results_glob))\n",
    "    if exclude_substrings:\n",
    "        eval_files = [\n",
    "            f for f in eval_files\n",
    "            if not any(sub in f.name for sub in exclude_substrings)\n",
    "        ]\n",
    "    if not eval_files:\n",
    "        print(f\"[WARN] No files for glob '{results_glob}' after excludes={exclude_substrings}\")\n",
    "        return None, None\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nLoading files for glob '{results_glob}' (excludes={exclude_substrings}):\")\n",
    "        print(\"  \", [f.name for f in eval_files])\n",
    "\n",
    "    all_data: dict[str, dict] = {}\n",
    "    for f in eval_files:\n",
    "        grid_label_full = f.stem.replace(\"eval_results_\", \"\")   # e.g. '3by3_25x25'\n",
    "        with open(f, \"r\", encoding=\"utf-8\") as fh:\n",
    "            data = json.load(fh)\n",
    "        all_data[grid_label_full] = data\n",
    "        if verbose:\n",
    "            print(f\"  Loaded {f.name}: keys={list(data.keys())}\")\n",
    "\n",
    "    # collect all alphas\n",
    "    all_alphas = set()\n",
    "    for grid_data in all_data.values():\n",
    "        for method_name in grid_data.keys():\n",
    "            if method_name == \"PI_baseline\":\n",
    "                continue\n",
    "            if method_filter_prefix and not method_name.startswith(method_filter_prefix):\n",
    "                continue\n",
    "            a = _parse_alpha(method_name)\n",
    "            if a is not None:\n",
    "                all_alphas.add(a)\n",
    "\n",
    "    if not all_alphas:\n",
    "        print(f\"[WARN] No RL methods with alpha=* in '{results_glob}'\")\n",
    "        return None, None\n",
    "\n",
    "    alphas_sorted = sorted(all_alphas)\n",
    "    alpha_labels = [f\"{a:g}\" for a in alphas_sorted]\n",
    "\n",
    "    full_labels = list(all_data.keys())\n",
    "    disc_labels = [lab.split(\"_\")[-1] for lab in full_labels]\n",
    "\n",
    "    rl_err_mat = np.full((len(full_labels), len(alphas_sorted)), np.nan, dtype=float)\n",
    "    ratio_mat  = np.full((len(full_labels), len(alphas_sorted)), np.nan, dtype=float)\n",
    "\n",
    "    for i, full_label in enumerate(full_labels):\n",
    "        grid_data = all_data[full_label]\n",
    "        disc_label = disc_labels[i]\n",
    "\n",
    "        const_entry = const_results.get(disc_label, None)\n",
    "        if const_entry is None:\n",
    "            if verbose:\n",
    "                print(f\"  [WARN] No constant-stepper result for grid '{disc_label}'\")\n",
    "            continue\n",
    "\n",
    "        s_const = const_entry.get(\"s_end\", np.nan)\n",
    "        if not (np.isfinite(s_const) and s_const != 0.0):\n",
    "            if verbose:\n",
    "                print(f\"  [WARN] Invalid s_const for grid '{disc_label}': {s_const}\")\n",
    "            continue\n",
    "\n",
    "        # RL errors per alpha (in %)\n",
    "        per_alpha_rl_errs: dict[float, list[float]] = {a: [] for a in alphas_sorted}\n",
    "\n",
    "        for method_name, method_data in grid_data.items():\n",
    "            if method_name == \"PI_baseline\":\n",
    "                continue\n",
    "            if method_filter_prefix and not method_name.startswith(method_filter_prefix):\n",
    "                continue\n",
    "\n",
    "            a = _parse_alpha(method_name)\n",
    "            if a is None:\n",
    "                continue\n",
    "\n",
    "            converged = bool(method_data.get(\"converged\", False))\n",
    "            success_rate = float(method_data.get(\"success_rate\", 0.0))\n",
    "            if not (converged or success_rate > 0.0):\n",
    "                continue\n",
    "\n",
    "            s_rl = method_data.get(\"s_end_mean\", None)\n",
    "            if s_rl is None:\n",
    "                continue\n",
    "            s_rl = float(s_rl)\n",
    "            if not np.isfinite(s_rl):\n",
    "                continue\n",
    "\n",
    "            # --- relative error in percent ---\n",
    "            err_rl = abs(s_rl - s_const) / abs(s_const) * 100.0\n",
    "            if not np.isfinite(err_rl):\n",
    "                continue\n",
    "\n",
    "            for a_target in per_alpha_rl_errs.keys():\n",
    "                if abs(a - a_target) < 1e-12:\n",
    "                    per_alpha_rl_errs[a_target].append(err_rl)\n",
    "                    break\n",
    "\n",
    "        # PI error (single value per grid, also in %)\n",
    "        pi_data = grid_data.get(\"PI_baseline\", {})\n",
    "        s_pi = pi_data.get(\"s_end_mean\", None)\n",
    "        if s_pi is None or not np.isfinite(float(s_pi)):\n",
    "            err_pi = np.nan\n",
    "        else:\n",
    "            s_pi = float(s_pi)\n",
    "            err_pi = abs(s_pi - s_const) / abs(s_const) * 100.0\n",
    "\n",
    "        # aggregate RL errors, and build ratio (dimensionless)\n",
    "        for j, a in enumerate(alphas_sorted):\n",
    "            rl_agg_err = _aggregate_values(per_alpha_rl_errs[a], aggregate)\n",
    "            if np.isfinite(rl_agg_err):\n",
    "                rl_err_mat[i, j] = rl_agg_err\n",
    "                if np.isfinite(err_pi) and rl_agg_err > 0.0:\n",
    "                    ratio_mat[i, j] = err_pi / rl_agg_err\n",
    "\n",
    "            if verbose and np.isfinite(rl_agg_err):\n",
    "                print(f\"  Grid {full_label:>12s}, alpha={a:g}: \"\n",
    "                      f\"err_RL={rl_agg_err:.3e} %, err_PI={err_pi:.3e} %, \"\n",
    "                      f\"ratio={ratio_mat[i,j]:.2f}\")\n",
    "\n",
    "    rl_err_df = pd.DataFrame(rl_err_mat, index=disc_labels, columns=alpha_labels)\n",
    "    ratio_df  = pd.DataFrame(ratio_mat,  index=disc_labels, columns=alpha_labels)\n",
    "\n",
    "    # sort rows by discretization (Nx)\n",
    "    def _disc_key(lbl: str):\n",
    "        try:\n",
    "            return int(lbl.split('x')[0])\n",
    "        except Exception:\n",
    "            return lbl\n",
    "\n",
    "    rl_err_df = rl_err_df.sort_index(key=lambda idx: [_disc_key(s) for s in idx])\n",
    "    ratio_df  = ratio_df.sort_index(key=lambda idx: [_disc_key(s) for s in idx])\n",
    "\n",
    "    # sort columns by alpha numerically\n",
    "    rl_err_df = rl_err_df.reindex(sorted(rl_err_df.columns, key=lambda s: float(s)), axis=1)\n",
    "    ratio_df  = ratio_df.reindex(sorted(ratio_df.columns,  key=lambda s: float(s)), axis=1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nRL relative error DataFrame (%):\")\n",
    "        print(rl_err_df)\n",
    "        print(\"\\nPI/RL error ratio DataFrame:\")\n",
    "        print(ratio_df)\n",
    "\n",
    "    return rl_err_df, ratio_df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Main plotting: 4-panel figure\n",
    "# ============================================================\n",
    "def create_error_dual_heatmaps(\n",
    "    const_output_dir: str = \".\",\n",
    "    glob_a: str = \"eval_results_3by3_*.json\",\n",
    "    glob_b: str = \"eval_results_5by5_*.json\",\n",
    "    aggregate: str = \"min\",\n",
    "    method_filter_prefix: str | None = None,\n",
    "    exclude_substrings_a: list[str] | None = None,\n",
    "    exclude_substrings_b: list[str] | None = [\"_PI_\"],\n",
    "    save_path: str = \"error_rel_and_ratio_3by3_vs_5by5.pdf\",\n",
    "    n_bins_seq: int = 10,\n",
    "    n_bins_div: int = 10,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    # 1) Constant-stepper results (separate for 3x3 and 5x5)\n",
    "    const_results_a = load_constant_stepper_results(\n",
    "        const_output_dir,\n",
    "        file_pattern=\"constant_stepper_*by*.npz\",\n",
    "        patch_size=3,\n",
    "        exclude_substrings=[\"_5by5\"],  # exclude the 5by5 variants\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    const_results_b = load_constant_stepper_results(\n",
    "        const_output_dir,\n",
    "        file_pattern=\"constant_stepper_*by*_5by5.npz\",\n",
    "        patch_size=5,\n",
    "        exclude_substrings=None,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    # 2) Error matrices for 3by3 and 5by5\n",
    "    rl_err_a, ratio_a = compute_error_dfs_for_dataset(\n",
    "        results_glob=glob_a,\n",
    "        const_results=const_results_a,\n",
    "        aggregate=aggregate,\n",
    "        method_filter_prefix=method_filter_prefix,\n",
    "        exclude_substrings=exclude_substrings_a,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    rl_err_b, ratio_b = compute_error_dfs_for_dataset(\n",
    "        results_glob=glob_b,\n",
    "        const_results=const_results_b,\n",
    "        aggregate=aggregate,\n",
    "        method_filter_prefix=method_filter_prefix,\n",
    "        exclude_substrings=exclude_substrings_b,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    if rl_err_a is None or rl_err_b is None or ratio_a is None or ratio_b is None:\n",
    "        print(\"[ERROR] One of the DataFrames is None; aborting.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Colormap + levels for RL relative error (sequential, in %)\n",
    "    # --------------------------------------------------------\n",
    "    vals_err_a = rl_err_a.values[np.isfinite(rl_err_a.values)]\n",
    "    vals_err_b = rl_err_b.values[np.isfinite(rl_err_b.values)]\n",
    "    if vals_err_a.size == 0 and vals_err_b.size == 0:\n",
    "        print(\"[ERROR] No finite RL errors.\")\n",
    "        return rl_err_a, rl_err_b, ratio_a, ratio_b\n",
    "\n",
    "    if vals_err_a.size == 0:\n",
    "        all_err_vals = vals_err_b\n",
    "    elif vals_err_b.size == 0:\n",
    "        all_err_vals = vals_err_a\n",
    "    else:\n",
    "        all_err_vals = np.concatenate([vals_err_a, vals_err_b])\n",
    "\n",
    "    levels_seq = _sequential_levels(all_err_vals, n_bins_seq)\n",
    "    ncolors_seq = len(levels_seq) - 1\n",
    "    base_cmap_seq = plt.get_cmap(\"cividis\")\n",
    "    cmap_seq_disc = ListedColormap(base_cmap_seq(np.linspace(0, 1, ncolors_seq)))\n",
    "    cmap_seq_disc.set_bad(color=\"white\")\n",
    "    norm_seq = BoundaryNorm(boundaries=levels_seq, ncolors=ncolors_seq)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Colormap + levels for error ratios (diverging, centered at 1)\n",
    "    # --------------------------------------------------------\n",
    "    vals_ratio_a = ratio_a.values[np.isfinite(ratio_a.values)]\n",
    "    vals_ratio_b = ratio_b.values[np.isfinite(ratio_b.values)]\n",
    "    if vals_ratio_a.size == 0 and vals_ratio_b.size == 0:\n",
    "        print(\"[ERROR] No finite error ratios.\")\n",
    "        return rl_err_a, rl_err_b, ratio_a, ratio_b\n",
    "\n",
    "    if vals_ratio_a.size == 0:\n",
    "        all_ratio_vals = vals_ratio_b\n",
    "    elif vals_ratio_b.size == 0:\n",
    "        all_ratio_vals = vals_ratio_a\n",
    "    else:\n",
    "        all_ratio_vals = np.concatenate([vals_ratio_a, vals_ratio_b])\n",
    "\n",
    "    vmin_sym, vmax_sym = _symmetric_limits_around_one(all_ratio_vals)\n",
    "    if n_bins_div % 2 == 1:\n",
    "        n_bins_div += 1\n",
    "    levels_div = _diverging_levels(vmin_sym, vmax_sym, n_bins_total=n_bins_div)\n",
    "    ncolors_div = len(levels_div) - 1\n",
    "\n",
    "    if _HAS_CMOCEAN:\n",
    "        base_cmap_div = cmocean.cm.balance\n",
    "    else:\n",
    "        base_cmap_div = plt.get_cmap(\"coolwarm\")\n",
    "\n",
    "    cmap_div_disc = ListedColormap(base_cmap_div(np.linspace(0, 1, ncolors_div)))\n",
    "    cmap_div_disc.set_bad(color=\"white\")\n",
    "    norm_div = BoundaryNorm(boundaries=levels_div, ncolors=ncolors_div)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Build the 2x2 figure\n",
    "    # --------------------------------------------------------\n",
    "    fig, axes = plt.subplots(\n",
    "        2, 2, figsize=(24, 14),\n",
    "        gridspec_kw={\"wspace\": 0.25, \"hspace\": 0.35}\n",
    "    )\n",
    "    ax_a, ax_b = axes[0]\n",
    "    ax_c, ax_d = axes[1]\n",
    "\n",
    "    # Colorbar axes (top for errors, bottom for ratios)\n",
    "    cax_top = fig.add_axes([0.92, 0.58, 0.02, 0.30])\n",
    "    cax_bot = fig.add_axes([0.92, 0.13, 0.02, 0.30])\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Panel (a): RL relative error, 3by3\n",
    "    # --------------------------------------------------------\n",
    "    ann_err_a = _annotations(rl_err_a)\n",
    "    sns.heatmap(\n",
    "        rl_err_a,\n",
    "        annot=ann_err_a.values,\n",
    "        fmt=\"\",\n",
    "        cmap=cmap_seq_disc,\n",
    "        norm=norm_seq,\n",
    "        mask=rl_err_a.isna(),\n",
    "        cbar=False,\n",
    "        annot_kws={\"size\": sizes - 8},\n",
    "        linewidths=0.5,\n",
    "        linecolor=\"gray\",\n",
    "        square=True,\n",
    "        ax=ax_a,\n",
    "    )\n",
    "    _overlay_na(ax_a, rl_err_a, fontsize=sizes - 6)\n",
    "    ax_a.set_xlabel(r'$\\alpha$', fontsize=sizes)\n",
    "    ax_a.set_ylabel(r'Discretization ($N_x$ by $N_z$)', fontsize=sizes)\n",
    "    ax_a.tick_params(axis='both', labelsize=sizes - 4)\n",
    "    ax_a.text(-0.10, 1.05, r'(a)',\n",
    "              transform=ax_a.transAxes,\n",
    "              fontsize=sizes,\n",
    "              va='bottom', ha='right')\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Panel (b): RL relative error, 5by5\n",
    "    # --------------------------------------------------------\n",
    "    ann_err_b = _annotations(rl_err_b)\n",
    "    sns.heatmap(\n",
    "        rl_err_b,\n",
    "        annot=ann_err_b.values,\n",
    "        fmt=\"\",\n",
    "        cmap=cmap_seq_disc,\n",
    "        norm=norm_seq,\n",
    "        mask=rl_err_b.isna(),\n",
    "        cbar=False,\n",
    "        annot_kws={\"size\": sizes - 8},\n",
    "        linewidths=0.5,\n",
    "        linecolor=\"gray\",\n",
    "        square=True,\n",
    "        ax=ax_b,\n",
    "    )\n",
    "    _overlay_na(ax_b, rl_err_b, fontsize=sizes - 6)\n",
    "    ax_b.set_xlabel(r'$\\alpha$', fontsize=sizes)\n",
    "    ax_b.set_ylabel(r'Discretization ($N_x$ by $N_z$)', fontsize=sizes)\n",
    "    ax_b.tick_params(axis='both', labelsize=sizes - 4)\n",
    "    ax_b.text(-0.10, 1.05, r'(b)',\n",
    "              transform=ax_b.transAxes,\n",
    "              fontsize=sizes,\n",
    "              va='bottom', ha='right')\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Shared colourbar for RL relative error (top, in %)\n",
    "    # --------------------------------------------------------\n",
    "    sm_err = ScalarMappable(norm=norm_seq, cmap=cmap_seq_disc)\n",
    "    sm_err.set_array([])\n",
    "    cbar_top = fig.colorbar(sm_err, cax=cax_top)\n",
    "    cbar_top.set_label(\n",
    "        r'Relative error [\\%]',\n",
    "        fontsize=sizes\n",
    "    )\n",
    "    cbar_top.ax.tick_params(labelsize=sizes - 4)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Panel (c): PI/RL error ratio, 3by3\n",
    "    # --------------------------------------------------------\n",
    "    ann_ratio_a = _annotations(ratio_a)\n",
    "    sns.heatmap(\n",
    "        ratio_a,\n",
    "        annot=ann_ratio_a.values,\n",
    "        fmt=\"\",\n",
    "        cmap=cmap_div_disc,\n",
    "        norm=norm_div,\n",
    "        mask=ratio_a.isna(),\n",
    "        cbar=False,\n",
    "        annot_kws={\"size\": sizes - 8},\n",
    "        linewidths=0.5,\n",
    "        linecolor=\"gray\",\n",
    "        square=True,\n",
    "        ax=ax_c,\n",
    "    )\n",
    "    _overlay_na(ax_c, ratio_a, fontsize=sizes - 6)\n",
    "    ax_c.set_xlabel(r'$\\alpha$', fontsize=sizes)\n",
    "    ax_c.set_ylabel(r'Discretization ($N_x$ by $N_z$)', fontsize=sizes)\n",
    "    ax_c.tick_params(axis='both', labelsize=sizes - 4)\n",
    "    ax_c.text(-0.10, 1.05, r'(c)',\n",
    "              transform=ax_c.transAxes,\n",
    "              fontsize=sizes,\n",
    "              va='bottom', ha='right')\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Panel (d): PI/RL error ratio, 5by5\n",
    "    # --------------------------------------------------------\n",
    "    ann_ratio_b = _annotations(ratio_b)\n",
    "    sns.heatmap(\n",
    "        ratio_b,\n",
    "        annot=ann_ratio_b.values,\n",
    "        fmt=\"\",\n",
    "        cmap=cmap_div_disc,\n",
    "        norm=norm_div,\n",
    "        mask=ratio_b.isna(),\n",
    "        cbar=False,\n",
    "        annot_kws={\"size\": sizes - 8},\n",
    "        linewidths=0.5,\n",
    "        linecolor=\"gray\",\n",
    "        square=True,\n",
    "        ax=ax_d,\n",
    "    )\n",
    "    _overlay_na(ax_d, ratio_b, fontsize=sizes - 6)\n",
    "    ax_d.set_xlabel(r'$\\alpha$', fontsize=sizes)\n",
    "    ax_d.set_ylabel(r'Discretization ($N_x$ by $N_z$)', fontsize=sizes)\n",
    "    ax_d.tick_params(axis='both', labelsize=sizes - 4)\n",
    "    ax_d.text(-0.10, 1.05, r'(d)',\n",
    "              transform=ax_d.transAxes,\n",
    "              fontsize=sizes,\n",
    "              va='bottom', ha='right')\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Shared colourbar for error ratio (bottom, dimensionless)\n",
    "    # --------------------------------------------------------\n",
    "    sm_ratio = ScalarMappable(norm=norm_div, cmap=cmap_div_disc)\n",
    "    sm_ratio.set_array([])\n",
    "    cbar_bot = fig.colorbar(sm_ratio, cax=cax_bot)\n",
    "\n",
    "    # ticks: include 1.0 explicitly\n",
    "    n_ticks_base = min(7, ncolors_div + 1)\n",
    "    base_ticks = np.linspace(vmin_sym, vmax_sym, n_ticks_base)\n",
    "    if len(base_ticks) > 0:\n",
    "        idx_closest = np.argmin(np.abs(base_ticks - 1.0))\n",
    "        base_ticks[idx_closest] = 1.0\n",
    "    ticks = np.sort(np.unique(base_ticks))\n",
    "    cbar_bot.set_ticks(ticks)\n",
    "    tick_labels = [f\"{t:.2f}\" if abs(t - 1.0) > 0.01 else \"1.00\" for t in ticks]\n",
    "    cbar_bot.set_ticklabels(tick_labels)\n",
    "\n",
    "    cbar_bot.set_label(\n",
    "        r'Relative error ratio (Heuristic/RL)',\n",
    "        fontsize=sizes\n",
    "    )\n",
    "    cbar_bot.ax.tick_params(labelsize=sizes - 4)\n",
    "\n",
    "    fig.tight_layout(rect=[0.03, 0.05, 0.90, 0.95])\n",
    "    fig.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"\\nSaved error heatmaps to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "    return rl_err_a, rl_err_b, ratio_a, ratio_b\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Example call\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    rl_err_3by3, rl_err_5by5, ratio_3by3, ratio_5by5 = create_error_dual_heatmaps(\n",
    "        const_output_dir=\".\",                  # where constant_stepper_*.npz live\n",
    "        glob_a=\"eval_results_3by3_*.json\",\n",
    "        glob_b=\"eval_results_5by5_*.json\",\n",
    "        aggregate=\"min\",                       # best RL per alpha\n",
    "        method_filter_prefix=None,            # or \"TQC \" if you only want TQC\n",
    "        exclude_substrings_a=None,\n",
    "        exclude_substrings_b=[\"_PI_\"],        # exclude eval_results_5by5_PI_*.json\n",
    "        save_path=\"figure8.pdf\",\n",
    "        n_bins_seq=10,\n",
    "        n_bins_div=10,\n",
    "        verbose=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c509b091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import BoundaryNorm, ListedColormap\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "def create_combined_runtime_error_dual_heatmap(\n",
    "    const_output_dir: str = \".\",\n",
    "    glob_a: str = \"eval_results_3by3_*.json\",\n",
    "    glob_b: str = \"eval_results_5by5_*.json\",\n",
    "    aggregate: str = \"min\",\n",
    "    method_filter_prefix: str | None = None,\n",
    "    exclude_substrings_a: list[str] | None = None,\n",
    "    exclude_substrings_b: list[str] | None = [\"_PI_\"],\n",
    "    save_path: str = \"combined_runtime_error_3by3_vs_5by5.pdf\",\n",
    "    n_bins_diverging: int = 10,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a 2-panel figure:\n",
    "\n",
    "      (a) 3x3 patch:  (runtime_ratio * error_ratio)\n",
    "      (b) 5x5 patch:  (runtime_ratio * error_ratio)\n",
    "\n",
    "    where\n",
    "      runtime_ratio = T_PI / T_RL             (from compute_runtime_ratio_df)\n",
    "      error_ratio   = err_PI / err_RL         (from compute_error_dfs_for_dataset,\n",
    "                                               with constant stepper as reference)\n",
    "\n",
    "    Both panels use the SAME diverging discrete colormap centered at 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 1) Constant-stepper results (3x3 and 5x5)\n",
    "    # --------------------------------------------------------\n",
    "    const_results_a = load_constant_stepper_results(\n",
    "        const_output_dir,\n",
    "        file_pattern=\"constant_stepper_*by*.npz\",\n",
    "        patch_size=3,\n",
    "        exclude_substrings=[\"_5by5\"],  # exclude the 5by5 variants\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    const_results_b = load_constant_stepper_results(\n",
    "        const_output_dir,\n",
    "        file_pattern=\"constant_stepper_*by*_5by5.npz\",\n",
    "        patch_size=5,\n",
    "        exclude_substrings=None,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2) Runtime ratios (PI / RL) for 3by3 and 5by5\n",
    "    # --------------------------------------------------------\n",
    "    runtime_ratio_a = compute_runtime_ratio_df(\n",
    "        results_glob=glob_a,\n",
    "        aggregate=aggregate,\n",
    "        method_filter_prefix=method_filter_prefix,\n",
    "        exclude_substrings=exclude_substrings_a,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    runtime_ratio_b = compute_runtime_ratio_df(\n",
    "        results_glob=glob_b,\n",
    "        aggregate=aggregate,\n",
    "        method_filter_prefix=method_filter_prefix,\n",
    "        exclude_substrings=exclude_substrings_b,  # exclude *_PI_ JSONs\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 3) Error ratios (err_PI / err_RL) via constant stepper\n",
    "    # --------------------------------------------------------\n",
    "    rl_err_a, ratio_err_a = compute_error_dfs_for_dataset(\n",
    "        results_glob=glob_a,\n",
    "        const_results=const_results_a,\n",
    "        aggregate=aggregate,\n",
    "        method_filter_prefix=method_filter_prefix,\n",
    "        exclude_substrings=exclude_substrings_a,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    rl_err_b, ratio_err_b = compute_error_dfs_for_dataset(\n",
    "        results_glob=glob_b,\n",
    "        const_results=const_results_b,\n",
    "        aggregate=aggregate,\n",
    "        method_filter_prefix=method_filter_prefix,\n",
    "        exclude_substrings=exclude_substrings_b,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    if (runtime_ratio_a is None or runtime_ratio_b is None or\n",
    "        ratio_err_a is None or ratio_err_b is None):\n",
    "        print(\"[ERROR] Missing runtime or error ratio DataFrames; aborting.\")\n",
    "        return None, None\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 4) Align indices/columns and form the combined metric\n",
    "    # --------------------------------------------------------\n",
    "    # Ensure same grid/alpha layout\n",
    "    runtime_ratio_a = runtime_ratio_a.reindex(\n",
    "        index=ratio_err_a.index, columns=ratio_err_a.columns\n",
    "    )\n",
    "    runtime_ratio_b = runtime_ratio_b.reindex(\n",
    "        index=ratio_err_b.index, columns=ratio_err_b.columns\n",
    "    )\n",
    "\n",
    "    combined_a = runtime_ratio_a * ratio_err_a\n",
    "    combined_b = runtime_ratio_b * ratio_err_b\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nCombined metric (3x3) = runtime_ratio * error_ratio:\")\n",
    "        print(combined_a)\n",
    "        print(\"\\nCombined metric (5x5) = runtime_ratio * error_ratio:\")\n",
    "        print(combined_b)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 5) Diverging colormap centered at 1 for the combined metric\n",
    "    # --------------------------------------------------------\n",
    "    vals_a = combined_a.values[np.isfinite(combined_a.values)]\n",
    "    vals_b = combined_b.values[np.isfinite(combined_b.values)]\n",
    "    if vals_a.size == 0 and vals_b.size == 0:\n",
    "        print(\"[ERROR] No finite combined metric values.\")\n",
    "        return combined_a, combined_b\n",
    "\n",
    "    if vals_a.size == 0:\n",
    "        all_vals = vals_b\n",
    "    elif vals_b.size == 0:\n",
    "        all_vals = vals_a\n",
    "    else:\n",
    "        all_vals = np.concatenate([vals_a, vals_b])\n",
    "\n",
    "    # symmetric limits around 1\n",
    "    vmin_sym, vmax_sym = _symmetric_limits_around_one(all_vals)\n",
    "\n",
    "    if n_bins_diverging % 2 == 1:\n",
    "        n_bins_diverging += 1\n",
    "    levels = _diverging_levels(vmin_sym, vmax_sym, n_bins_total=n_bins_diverging)\n",
    "    ncolors = len(levels) - 1\n",
    "\n",
    "    # base cmap: cmocean.balance if available, otherwise coolwarm\n",
    "    if _HAS_CMOCEAN:\n",
    "        base_cmap = cmocean.cm.balance\n",
    "    else:\n",
    "        base_cmap = plt.get_cmap(\"coolwarm\")\n",
    "\n",
    "    cmap_disc = ListedColormap(base_cmap(np.linspace(0, 1, ncolors)))\n",
    "    cmap_disc.set_bad(color=\"white\")\n",
    "    norm = BoundaryNorm(boundaries=levels, ncolors=ncolors)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 6) Build figure: 2 panels + 1 shared colorbar\n",
    "    # --------------------------------------------------------\n",
    "    fig = plt.figure(figsize=(24, 8))\n",
    "    gs = fig.add_gridspec(\n",
    "        nrows=1, ncols=2,\n",
    "        width_ratios=[1, 1],\n",
    "        wspace=0.2\n",
    "    )\n",
    "\n",
    "    ax_a = fig.add_subplot(gs[0, 0])\n",
    "    ax_b = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "    # colorbar axis\n",
    "    cax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "\n",
    "    # ---------- Panel (a): 3by3 ----------\n",
    "    ann_a = _annotations(combined_a)\n",
    "    sns.heatmap(\n",
    "        combined_a,\n",
    "        annot=ann_a.values,\n",
    "        fmt=\"\",\n",
    "        cmap=cmap_disc,\n",
    "        norm=norm,\n",
    "        mask=combined_a.isna(),\n",
    "        cbar=False,\n",
    "        annot_kws={\"size\": sizes - 8},\n",
    "        linewidths=0.5,\n",
    "        linecolor=\"gray\",\n",
    "        square=True,\n",
    "        ax=ax_a,\n",
    "    )\n",
    "    _overlay_na(ax_a, combined_a, fontsize=sizes - 6)\n",
    "    ax_a.set_xlabel(r'$\\alpha$', fontsize=sizes)\n",
    "    ax_a.set_ylabel(r'Discretization ($N_x$ by $N_z$)', fontsize=sizes)\n",
    "    ax_a.tick_params(axis='both', labelsize=sizes - 4)\n",
    "    ax_a.text(-0.10, 1.05, r'(a)',\n",
    "              transform=ax_a.transAxes,\n",
    "              fontsize=sizes,\n",
    "              va='bottom', ha='right')\n",
    "\n",
    "    # ---------- Panel (b): 5by5 ----------\n",
    "    ann_b = _annotations(combined_b)\n",
    "    sns.heatmap(\n",
    "        combined_b,\n",
    "        annot=ann_b.values,\n",
    "        fmt=\"\",\n",
    "        cmap=cmap_disc,\n",
    "        norm=norm,\n",
    "        mask=combined_b.isna(),\n",
    "        cbar=False,\n",
    "        annot_kws={\"size\": sizes - 8},\n",
    "        linewidths=0.5,\n",
    "        linecolor=\"gray\",\n",
    "        square=True,\n",
    "        ax=ax_b,\n",
    "    )\n",
    "    _overlay_na(ax_b, combined_b, fontsize=sizes - 6)\n",
    "    ax_b.set_xlabel(r'$\\alpha$', fontsize=sizes)\n",
    "    ax_b.set_ylabel(r'Discretization ($N_x$ by $N_z$)', fontsize=sizes)\n",
    "    ax_b.tick_params(axis='both', labelsize=sizes - 4)\n",
    "    ax_b.text(-0.10, 1.05, r'(b)',\n",
    "              transform=ax_b.transAxes,\n",
    "              fontsize=sizes,\n",
    "              va='bottom', ha='right')\n",
    "\n",
    "    # ---------- Shared colorbar ----------\n",
    "    sm = ScalarMappable(norm=norm, cmap=cmap_disc)\n",
    "    sm.set_array([])\n",
    "    cbar = fig.colorbar(sm, cax=cax)\n",
    "\n",
    "    # ticks: include 1.0 explicitly\n",
    "    n_ticks_base = min(7, ncolors + 1)\n",
    "    base_ticks = np.linspace(vmin_sym, vmax_sym, n_ticks_base)\n",
    "    if len(base_ticks) > 0:\n",
    "        idx_closest = np.argmin(np.abs(base_ticks - 1.0))\n",
    "        base_ticks[idx_closest] = 1.0\n",
    "    ticks = np.sort(np.unique(base_ticks))\n",
    "\n",
    "    cbar.set_ticks(ticks)\n",
    "    tick_labels = [f\"{t:.2f}\" if abs(t - 1.0) > 0.01 else \"1.00\" for t in ticks]\n",
    "    cbar.set_ticklabels(tick_labels)\n",
    "\n",
    "    cbar.set_label(\n",
    "        r'Combined performance ratio'\n",
    "        + \"\\n\"\n",
    "        + r'$\\left(\\dfrac{T_{\\mathrm{PI}}}{T_{\\mathrm{RL}}}\\right)'\n",
    "        + r'\\left[\\dfrac{|s_{\\mathrm{PI}}-s_{\\mathrm{const}}|/|s_{\\mathrm{const}}|}'\n",
    "        + r'{|s_{\\mathrm{RL}}-s_{\\mathrm{const}}|/|s_{\\mathrm{const}}|}\\right]$',\n",
    "        fontsize=sizes\n",
    "    )\n",
    "    cbar.ax.tick_params(labelsize=sizes - 4)\n",
    "\n",
    "    fig.tight_layout(rect=[0.03, 0.05, 0.90, 0.95])\n",
    "    fig.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"\\nSaved combined runtimeâ€“error heatmaps to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "    return combined_a, combined_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbfc5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import re\n",
    "# from pathlib import Path\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# from matplotlib.colors import BoundaryNorm, ListedColormap\n",
    "# from matplotlib.cm import ScalarMappable\n",
    "\n",
    "# from plants.faults import strikeslip\n",
    "\n",
    "# # ============================================================\n",
    "# # Global style (your settings)\n",
    "# # ============================================================\n",
    "# sizes = 30\n",
    "# plt.rcParams.update({\n",
    "#     'font.size': sizes,\n",
    "#     'axes.titlesize': sizes,\n",
    "#     'axes.labelsize': sizes,\n",
    "#     'xtick.labelsize': sizes,\n",
    "#     'ytick.labelsize': sizes,\n",
    "#     'legend.fontsize': sizes,\n",
    "#     'figure.titlesize': sizes,\n",
    "#     'text.usetex': True,\n",
    "#     'font.family': 'Times',\n",
    "#     'pdf.fonttype': 42,\n",
    "#     'ps.fonttype': 42,\n",
    "#     'text.latex.preamble': r'\\usepackage{amsmath}\\usepackage{mathptmx}',\n",
    "#     'axes.grid': False,\n",
    "# })\n",
    "\n",
    "# # optional cmocean\n",
    "# try:\n",
    "#     import cmocean\n",
    "#     _HAS_CMOCEAN = True\n",
    "# except Exception:\n",
    "#     _HAS_CMOCEAN = False\n",
    "#     print(\"[INFO] cmocean not found. Falling back to 'coolwarm'.\")\n",
    "\n",
    "\n",
    "# # ------------------------------------------------------------------\n",
    "# # Minimal helper: build fault & return Dscale and N_DOFS for a grid\n",
    "# # ------------------------------------------------------------------\n",
    "# def build_fault_for_grid(Nz: int, Nx: int, patch_size: int):\n",
    "#     \"\"\"\n",
    "#     Build a quasi-static strike-slip fault for given grid size and\n",
    "#     patch size (3 or 5) and return fault + N_DOFS.\n",
    "#     \"\"\"\n",
    "#     fault_local = strikeslip.qs_strikeslip_fault(\n",
    "#         zdepth=patch_size,\n",
    "#         xlength=patch_size,\n",
    "#         Nz=Nz,\n",
    "#         Nx=Nx,\n",
    "#         G=30000.0,\n",
    "#         rho=2.5e-3,\n",
    "#         zeta=0.8 / patch_size,\n",
    "#         Ks_path=\"./Data/\",\n",
    "#         gamma_s=25.0,\n",
    "#         gamma_w=10.0,\n",
    "#         sigma_ref=100.0,\n",
    "#         depth_ini=0.0,\n",
    "#         vinf=3.171e-10,\n",
    "#         Dmu_estimate=0.5,\n",
    "#     )\n",
    "\n",
    "#     return {\n",
    "#         \"fault\": fault_local,\n",
    "#         \"N_DOFS\": fault_local.N,\n",
    "#     }\n",
    "\n",
    "\n",
    "# # ------------------------------------------------------------------\n",
    "# # Load constant-stepper results for a given dataset (3by3 or 5by5)\n",
    "# # ------------------------------------------------------------------\n",
    "# def load_constant_stepper_results(\n",
    "#     output_dir: str,\n",
    "#     file_pattern: str,\n",
    "#     patch_size: int,\n",
    "#     exclude_substrings: list[str] | None = None,\n",
    "#     verbose: bool = True,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Load constant-stepper results from NPZ files, compute final mean slip\n",
    "#     in physical units using Dscale from the fault, and return a dict:\n",
    "\n",
    "#         label -> dict(runtime_s, s_end)\n",
    "\n",
    "#     label is e.g. '25x25', '50x50', ...\n",
    "#     \"\"\"\n",
    "#     out_path = Path(output_dir)\n",
    "#     files = sorted(out_path.glob(file_pattern))\n",
    "\n",
    "#     if exclude_substrings:\n",
    "#         files = [\n",
    "#             f for f in files\n",
    "#             if not any(sub in f.name for sub in exclude_substrings)\n",
    "#         ]\n",
    "\n",
    "#     if verbose:\n",
    "#         print(f\"\\nLoading constant-stepper files from {output_dir} with pattern '{file_pattern}':\")\n",
    "#         print(\"  Found:\", [f.name for f in files])\n",
    "\n",
    "#     const_results: dict[str, dict] = {}\n",
    "\n",
    "#     for npz_path in files:\n",
    "#         stem = npz_path.stem  # e.g. 'constant_stepper_50by50' or 'constant_stepper_50by50_5by5'\n",
    "#         core = stem.replace(\"constant_stepper_\", \"\")  # e.g. '50by50' or '50by50_5by5'\n",
    "#         grid_part = core.split(\"_\")[0]  # take '50by50'\n",
    "#         try:\n",
    "#             Nz_str, Nx_str = grid_part.split(\"by\")\n",
    "#             Nz, Nx = int(Nz_str), int(Nx_str)\n",
    "#         except Exception as e:\n",
    "#             if verbose:\n",
    "#                 print(f\"  [WARN] Could not parse Nz,Nx from {stem}: {e}\")\n",
    "#             continue\n",
    "\n",
    "#         # build fault to get Dscale and N_DOFS (patch_size = 3 or 5)\n",
    "#         grid = build_fault_for_grid(Nz, Nx, patch_size=patch_size)\n",
    "#         fault = grid[\"fault\"]\n",
    "#         N = grid[\"N_DOFS\"]\n",
    "#         Dscale = fault.Dscale\n",
    "\n",
    "#         data = np.load(npz_path)\n",
    "\n",
    "#         if \"ts1\" not in data.files or \"ys1\" not in data.files:\n",
    "#             if verbose:\n",
    "#                 print(f\"  [WARN] {npz_path.name} missing ts1/ys1; skipping.\")\n",
    "#             continue\n",
    "\n",
    "#         ys = data[\"ys1\"]\n",
    "#         if ys.shape[1] < 3 * N:\n",
    "#             if verbose:\n",
    "#                 print(f\"  [WARN] {npz_path.name}: ys shape {ys.shape} < 3*N={3*N}; skipping.\")\n",
    "#             continue\n",
    "\n",
    "#         # slip block (dimensionless), average over DOFs, then scale\n",
    "#         s_block = ys[:, 2*N:3*N]\n",
    "#         avg_s_dimless = np.mean(s_block, axis=1)\n",
    "#         avg_s_phys = avg_s_dimless * Dscale\n",
    "#         s_end = float(avg_s_phys[-1])\n",
    "\n",
    "#         # runtime if stored inside npz (optional)\n",
    "#         if \"runtime_s\" in data.files:\n",
    "#             runtime_s = float(data[\"runtime_s\"])\n",
    "#         elif \"wall_time\" in data.files:\n",
    "#             runtime_s = float(data[\"wall_time\"])\n",
    "#         else:\n",
    "#             runtime_s = np.nan\n",
    "#             if verbose:\n",
    "#                 print(f\"  [INFO] {npz_path.name}: no runtime stored; runtime_s=NaN.\")\n",
    "\n",
    "#         label = f\"{Nz}x{Nx}\"\n",
    "#         const_results[label] = dict(runtime_s=runtime_s, s_end=s_end)\n",
    "#         if verbose:\n",
    "#             print(f\"  {npz_path.name}: label={label}, runtime={runtime_s:.3f}s, s_end={s_end:.3e}\")\n",
    "\n",
    "#     return const_results\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # Helpers: loading + runtime ratio computation (PI / RL)\n",
    "# # ============================================================\n",
    "# def _parse_alpha(method_name: str) -> float | None:\n",
    "#     m = re.search(r\"alpha\\s*=\\s*([0-9.+-eE]+)\", method_name)\n",
    "#     if not m:\n",
    "#         return None\n",
    "#     try:\n",
    "#         return float(m.group(1))\n",
    "#     except Exception:\n",
    "#         return None\n",
    "\n",
    "\n",
    "# def _aggregate_values(vals: list[float], how: str) -> float:\n",
    "#     arr = np.array([v for v in vals if np.isfinite(v) and v > 0.0], dtype=float)\n",
    "#     if arr.size == 0:\n",
    "#         return np.nan\n",
    "#     how = how.lower()\n",
    "#     if how == \"min\":\n",
    "#         return float(np.min(arr))\n",
    "#     if how == \"mean\":\n",
    "#         return float(np.mean(arr))\n",
    "#     if how == \"max\":\n",
    "#         return float(np.max(arr))\n",
    "#     return float(np.min(arr))\n",
    "\n",
    "\n",
    "# def compute_runtime_ratio_df(\n",
    "#     results_glob: str,\n",
    "#     aggregate: str = \"min\",\n",
    "#     method_filter_prefix: str | None = None,\n",
    "#     exclude_substrings: list[str] | None = None,\n",
    "#     verbose: bool = True,\n",
    "# ) -> pd.DataFrame | None:\n",
    "#     \"\"\"\n",
    "#     Return DF: index=discretization (e.g. '25x25'),\n",
    "#                columns=alphas,\n",
    "#                entries = PI_runtime / RL_runtime.\n",
    "#     \"\"\"\n",
    "#     eval_files = sorted(Path(\".\").glob(results_glob))\n",
    "#     if exclude_substrings:\n",
    "#         eval_files = [\n",
    "#             f for f in eval_files\n",
    "#             if not any(sub in f.name for sub in exclude_substrings)\n",
    "#         ]\n",
    "#     if not eval_files:\n",
    "#         print(f\"[WARN] No files for glob '{results_glob}' after excludes={exclude_substrings}\")\n",
    "#         return None\n",
    "\n",
    "#     if verbose:\n",
    "#         print(f\"\\nLoading files for glob '{results_glob}' (excludes={exclude_substrings}):\")\n",
    "#         print(\"  \", [f.name for f in eval_files])\n",
    "\n",
    "#     all_data: dict[str, dict] = {}\n",
    "#     for f in eval_files:\n",
    "#         grid_label_full = f.stem.replace(\"eval_results_\", \"\")\n",
    "#         with open(f, \"r\", encoding=\"utf-8\") as fh:\n",
    "#             data = json.load(fh)\n",
    "#         all_data[grid_label_full] = data\n",
    "#         if verbose:\n",
    "#             print(f\"  Loaded {f.name}: keys={list(data.keys())}\")\n",
    "\n",
    "#     # collect all alphas from RL entries\n",
    "#     all_alphas = set()\n",
    "#     for grid_data in all_data.values():\n",
    "#         for method_name in grid_data.keys():\n",
    "#             if method_name == \"PI_baseline\":\n",
    "#                 continue\n",
    "#             if method_filter_prefix and not method_name.startswith(method_filter_prefix):\n",
    "#                 continue\n",
    "#             a = _parse_alpha(method_name)\n",
    "#             if a is not None:\n",
    "#                 all_alphas.add(a)\n",
    "\n",
    "#     if not all_alphas:\n",
    "#         print(f\"[WARN] No RL methods with alpha=* in '{results_glob}'\")\n",
    "#         return None\n",
    "\n",
    "#     alphas_sorted = sorted(all_alphas)\n",
    "#     alpha_labels = [f\"{a:g}\" for a in alphas_sorted]\n",
    "\n",
    "#     # index labels: strip e.g. \"3by3_25x25\" -> \"25x25\"\n",
    "#     full_labels = list(all_data.keys())\n",
    "#     disc_labels = [lab.split(\"_\")[-1] for lab in full_labels]\n",
    "\n",
    "#     ratio_mat = np.full((len(full_labels), len(alphas_sorted)), np.nan, dtype=float)\n",
    "\n",
    "#     for i, full_label in enumerate(full_labels):\n",
    "#         grid_data = all_data[full_label]\n",
    "#         pi_runtime = grid_data.get(\"PI_baseline\", {}).get(\"runtime_s\", np.nan)\n",
    "#         if not (np.isfinite(pi_runtime) and pi_runtime > 0.0):\n",
    "#             if verbose:\n",
    "#                 print(f\"  [WARN] No valid PI baseline for '{full_label}'\")\n",
    "#             continue\n",
    "\n",
    "#         per_alpha_values: dict[float, list[float]] = {a: [] for a in alphas_sorted}\n",
    "\n",
    "#         for method_name, method_data in grid_data.items():\n",
    "#             if method_name == \"PI_baseline\":\n",
    "#                 continue\n",
    "#             if method_filter_prefix and not method_name.startswith(method_filter_prefix):\n",
    "#                 continue\n",
    "\n",
    "#             a = _parse_alpha(method_name)\n",
    "#             if a is None:\n",
    "#                 continue\n",
    "\n",
    "#             converged = bool(method_data.get(\"converged\", False))\n",
    "#             success_rate = float(method_data.get(\"success_rate\", 0.0))\n",
    "#             if not (converged or success_rate > 0.0):\n",
    "#                 continue\n",
    "\n",
    "#             rl_runtime = method_data.get(\"runtime_s\", np.nan)\n",
    "#             if not (np.isfinite(rl_runtime) and rl_runtime > 0.0):\n",
    "#                 continue\n",
    "\n",
    "#             for a_target in per_alpha_values.keys():\n",
    "#                 if abs(a - a_target) < 1e-12:\n",
    "#                     per_alpha_values[a_target].append(float(rl_runtime))\n",
    "#                     break\n",
    "\n",
    "#         for j, a in enumerate(alphas_sorted):\n",
    "#             rl_agg = _aggregate_values(per_alpha_values[a], aggregate)\n",
    "#             if np.isfinite(rl_agg) and rl_agg > 0.0:\n",
    "#                 ratio_mat[i, j] = pi_runtime / rl_agg\n",
    "#                 if verbose:\n",
    "#                     print(\n",
    "#                         f\"  Grid {full_label:>12s}, alpha={a:g}: \"\n",
    "#                         f\"PI={pi_runtime:.3f}, RL({aggregate})={rl_agg:.3f}, \"\n",
    "#                         f\"ratio={ratio_mat[i,j]:.2f}\"\n",
    "#                     )\n",
    "\n",
    "#     ratio_df = pd.DataFrame(ratio_mat, index=disc_labels, columns=alpha_labels)\n",
    "\n",
    "#     # sort: discretization ascending, alpha ascending\n",
    "#     def _disc_key(lbl: str):\n",
    "#         try:\n",
    "#             return int(lbl.split(\"x\")[0])\n",
    "#         except Exception:\n",
    "#             return lbl\n",
    "\n",
    "#     ratio_df = ratio_df.sort_index(key=lambda idx: [_disc_key(s) for s in idx])\n",
    "#     ratio_df = ratio_df.reindex(sorted(ratio_df.columns, key=lambda s: float(s)), axis=1)\n",
    "\n",
    "#     if verbose:\n",
    "#         print(\"\\nRuntime ratio DataFrame (index=disc, columns=alpha):\")\n",
    "#         print(ratio_df)\n",
    "\n",
    "#     return ratio_df\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # Error DF: RL relative error & ratio err_PI / err_RL\n",
    "# # ============================================================\n",
    "# def compute_error_dfs_for_dataset(\n",
    "#     results_glob: str,\n",
    "#     const_results: dict,\n",
    "#     aggregate: str = \"min\",\n",
    "#     method_filter_prefix: str | None = None,\n",
    "#     exclude_substrings: list[str] | None = None,\n",
    "#     verbose: bool = True,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     For a given dataset (3by3 or 5by5), compute:\n",
    "\n",
    "#       - rl_err_df[grid, alpha]  = |s_RL - s_const| / |s_const|\n",
    "#       - ratio_err_df[grid, alpha] = (|s_PI - s_const| / |s_const|)\n",
    "#                                      / (|s_RL - s_const| / |s_const|)\n",
    "\n",
    "#     where s_const is the final mean slip from the constant-stepper\n",
    "#     for that discretization.\n",
    "#     \"\"\"\n",
    "#     eval_files = sorted(Path(\".\").glob(results_glob))\n",
    "#     if exclude_substrings:\n",
    "#         eval_files = [\n",
    "#             f for f in eval_files\n",
    "#             if not any(sub in f.name for sub in exclude_substrings)\n",
    "#         ]\n",
    "#     if not eval_files:\n",
    "#         print(f\"[WARN] No files for glob '{results_glob}' after excludes={exclude_substrings}\")\n",
    "#         return None, None\n",
    "\n",
    "#     if verbose:\n",
    "#         print(f\"\\nLoading files for error computation '{results_glob}' (excludes={exclude_substrings}):\")\n",
    "#         print(\"  \", [f.name for f in eval_files])\n",
    "\n",
    "#     all_data: dict[str, dict] = {}\n",
    "#     for f in eval_files:\n",
    "#         grid_label_full = f.stem.replace(\"eval_results_\", \"\")\n",
    "#         with open(f, \"r\", encoding=\"utf-8\") as fh:\n",
    "#             data = json.load(fh)\n",
    "#         all_data[grid_label_full] = data\n",
    "#         if verbose:\n",
    "#             print(f\"  Loaded {f.name}: keys={list(data.keys())}\")\n",
    "\n",
    "#     # collect all alphas\n",
    "#     all_alphas = set()\n",
    "#     for grid_data in all_data.values():\n",
    "#         for method_name in grid_data.keys():\n",
    "#             if method_name == \"PI_baseline\":\n",
    "#                 continue\n",
    "#             if method_filter_prefix and not method_name.startswith(method_filter_prefix):\n",
    "#                 continue\n",
    "#             a = _parse_alpha(method_name)\n",
    "#             if a is not None:\n",
    "#                 all_alphas.add(a)\n",
    "\n",
    "#     if not all_alphas:\n",
    "#         print(f\"[WARN] No RL methods with alpha=* in '{results_glob}'\")\n",
    "#         return None, None\n",
    "\n",
    "#     alphas_sorted = sorted(all_alphas)\n",
    "#     alpha_labels = [f\"{a:g}\" for a in alphas_sorted]\n",
    "\n",
    "#     full_labels = list(all_data.keys())\n",
    "#     disc_labels = [lab.split(\"_\")[-1] for lab in full_labels]\n",
    "\n",
    "#     rl_err_mat = np.full((len(full_labels), len(alphas_sorted)), np.nan, dtype=float)\n",
    "#     ratio_err_mat = np.full((len(full_labels), len(alphas_sorted)), np.nan, dtype=float)\n",
    "\n",
    "#     for i, full_label in enumerate(full_labels):\n",
    "#         disc_label = full_label.split(\"_\")[-1]  # e.g. \"25x25\"\n",
    "#         grid_data = all_data[full_label]\n",
    "\n",
    "#         const_entry = const_results.get(disc_label, None)\n",
    "#         if const_entry is None:\n",
    "#             if verbose:\n",
    "#                 print(f\"  [WARN] No constant-stepper reference for '{disc_label}'\")\n",
    "#             continue\n",
    "\n",
    "#         s_const = float(const_entry.get(\"s_end\", np.nan))\n",
    "#         if not np.isfinite(s_const):\n",
    "#             if verbose:\n",
    "#                 print(f\"  [WARN] Constant-stepper s_end not finite for '{disc_label}'\")\n",
    "#             continue\n",
    "\n",
    "#         denom = max(abs(s_const), 1e-16)\n",
    "\n",
    "#         # PI final slip (independent of alpha)\n",
    "#         pi_data = grid_data.get(\"PI_baseline\", {})\n",
    "#         s_pi = float(pi_data.get(\"s_end_mean\", np.nan))\n",
    "#         if np.isfinite(s_pi):\n",
    "#             err_pi = abs(s_pi - s_const) / denom\n",
    "#         else:\n",
    "#             err_pi = np.nan\n",
    "\n",
    "#         # gather RL errors per alpha\n",
    "#         per_alpha_errs: dict[float, list[float]] = {a: [] for a in alphas_sorted}\n",
    "\n",
    "#         for method_name, method_data in grid_data.items():\n",
    "#             if method_name == \"PI_baseline\":\n",
    "#                 continue\n",
    "#             if method_filter_prefix and not method_name.startswith(method_filter_prefix):\n",
    "#                 continue\n",
    "\n",
    "#             a = _parse_alpha(method_name)\n",
    "#             if a is None:\n",
    "#                 continue\n",
    "\n",
    "#             s_rl = float(method_data.get(\"s_end_mean\", np.nan))\n",
    "#             if not np.isfinite(s_rl):\n",
    "#                 continue\n",
    "\n",
    "#             err_rl = abs(s_rl - s_const) / denom\n",
    "#             if not np.isfinite(err_rl):\n",
    "#                 continue\n",
    "\n",
    "#             for a_target in per_alpha_errs.keys():\n",
    "#                 if abs(a - a_target) < 1e-12:\n",
    "#                     per_alpha_errs[a_target].append(err_rl)\n",
    "#                     break\n",
    "\n",
    "#         for j, a in enumerate(alphas_sorted):\n",
    "#             err_rl_agg = _aggregate_values(per_alpha_errs[a], aggregate)\n",
    "#             if np.isfinite(err_rl_agg) and err_rl_agg > 0.0:\n",
    "#                 rl_err_mat[i, j] = err_rl_agg\n",
    "#                 if np.isfinite(err_pi):\n",
    "#                     ratio_err_mat[i, j] = err_pi / err_rl_agg\n",
    "#                 if verbose:\n",
    "#                     print(\n",
    "#                         f\"  Grid {full_label:>12s}, alpha={a:g}: \"\n",
    "#                         f\"err_RL={err_rl_agg:.3e}, err_PI={err_pi:.3e}, \"\n",
    "#                         f\"ratio={ratio_err_mat[i,j]:.2f}\"\n",
    "#                     )\n",
    "\n",
    "#     rl_err_df = pd.DataFrame(rl_err_mat, index=disc_labels, columns=alpha_labels)\n",
    "#     ratio_err_df = pd.DataFrame(ratio_err_mat, index=disc_labels, columns=alpha_labels)\n",
    "\n",
    "#     # sort\n",
    "#     def _disc_key(lbl: str):\n",
    "#         try:\n",
    "#             return int(lbl.split(\"x\")[0])\n",
    "#         except Exception:\n",
    "#             return lbl\n",
    "\n",
    "#     rl_err_df = rl_err_df.sort_index(key=lambda idx: [_disc_key(s) for s in idx])\n",
    "#     rl_err_df = rl_err_df.reindex(sorted(rl_err_df.columns, key=lambda s: float(s)), axis=1)\n",
    "\n",
    "#     ratio_err_df = ratio_err_df.loc[rl_err_df.index, rl_err_df.columns]\n",
    "\n",
    "#     if verbose:\n",
    "#         print(\"\\nRL relative error DF:\")\n",
    "#         print(rl_err_df)\n",
    "#         print(\"\\nError-ratio DF (PI / RL):\")\n",
    "#         print(ratio_err_df)\n",
    "\n",
    "#     return rl_err_df, ratio_err_df\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # Plot helpers (annotations, NA overlay, diverging levels)\n",
    "# # ============================================================\n",
    "# def _annotations(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     if hasattr(df, \"map\"):\n",
    "#         return df.map(lambda x: f\"{x:.2f}\" if pd.notna(x) else \"\")\n",
    "#     return df.applymap(lambda x: f\"{x:.2f}\" if pd.notna(x) else \"\")\n",
    "\n",
    "\n",
    "# def _overlay_na(ax, df: pd.DataFrame, fontsize=12, color=\"black\"):\n",
    "#     for y in range(df.shape[0]):\n",
    "#         for x in range(df.shape[1]):\n",
    "#             if pd.isna(df.iat[y, x]):\n",
    "#                 ax.text(\n",
    "#                     x + 0.5,\n",
    "#                     y + 0.5,\n",
    "#                     \"N/A\",\n",
    "#                     ha=\"center\",\n",
    "#                     va=\"center\",\n",
    "#                     fontsize=fontsize,\n",
    "#                     color=color,\n",
    "#                 )\n",
    "\n",
    "\n",
    "# def _symmetric_limits_around_one(vals_all: np.ndarray):\n",
    "#     vals_all = vals_all[np.isfinite(vals_all)]\n",
    "#     if vals_all.size == 0:\n",
    "#         return 0.5, 1.5\n",
    "#     vmin_real = float(np.min(vals_all))\n",
    "#     vmax_real = float(np.max(vals_all))\n",
    "#     delta_low = max(0.0, 1.0 - vmin_real)\n",
    "#     delta_high = max(0.0, vmax_real - 1.0)\n",
    "#     half_span = max(delta_low, delta_high)\n",
    "#     vmin_sym = max(1.0 - half_span, 1e-12)\n",
    "#     vmax_sym = 1.0 + half_span\n",
    "#     return vmin_sym, vmax_sym\n",
    "\n",
    "\n",
    "# def _diverging_levels(vmin_sym: float, vmax_sym: float, n_bins_total: int):\n",
    "#     if n_bins_total % 2 == 1:\n",
    "#         n_bins_total += 1\n",
    "#     n_half = n_bins_total // 2\n",
    "#     lower = np.linspace(vmin_sym, 1.0, n_half + 1)[:-1]\n",
    "#     upper = np.linspace(1.0, vmax_sym, n_half + 1)\n",
    "#     return np.concatenate([lower, upper])  # len = n_bins_total + 1\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # Combined heatmap: runtime ratio * error ratio\n",
    "# # ============================================================\n",
    "# def create_combined_runtime_error_dual_heatmap(\n",
    "#     const_output_dir: str,\n",
    "#     glob_a: str = \"eval_results_3by3_*.json\",\n",
    "#     glob_b: str = \"eval_results_5by5_*.json\",\n",
    "#     aggregate: str = \"min\",\n",
    "#     method_filter_prefix: str | None = None,\n",
    "#     exclude_substrings_a: list[str] | None = None,\n",
    "#     exclude_substrings_b: list[str] | None = [\"_PI_\"],\n",
    "#     save_path: str = \"combined_runtime_error_3by3_vs_5by5.pdf\",\n",
    "#     n_bins_diverging: int = 10,\n",
    "#     verbose: bool = True,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Build a figure with two panels:\n",
    "\n",
    "#       (a) 3by3 data\n",
    "#       (b) 5by5 data\n",
    "\n",
    "#     Each panel shows the scalar field:\n",
    "\n",
    "#         combined = (PI_runtime / RL_runtime) * (err_PI / err_RL)\n",
    "\n",
    "#     where the reference slip is the constant-stepper result.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # --------------------------------------------------------\n",
    "#     # 1) Constant-stepper results for 3by3 and 5by5\n",
    "#     # --------------------------------------------------------\n",
    "#     const_results_a = load_constant_stepper_results(\n",
    "#         const_output_dir,\n",
    "#         file_pattern=\"constant_stepper_*by*.npz\",\n",
    "#         patch_size=3,\n",
    "#         exclude_substrings=[\"_5by5\"],  # exclude 5by5 variant here\n",
    "#         verbose=verbose,\n",
    "#     )\n",
    "#     const_results_b = load_constant_stepper_results(\n",
    "#         const_output_dir,\n",
    "#         file_pattern=\"constant_stepper_*by*_5by5.npz\",\n",
    "#         patch_size=5,\n",
    "#         exclude_substrings=None,\n",
    "#         verbose=verbose,\n",
    "#     )\n",
    "\n",
    "#     # --------------------------------------------------------\n",
    "#     # 2) Runtime ratios (PI / RL) for 3by3 and 5by5\n",
    "#     # --------------------------------------------------------\n",
    "#     runtime_ratio_a = compute_runtime_ratio_df(\n",
    "#         results_glob=glob_a,\n",
    "#         aggregate=aggregate,\n",
    "#         method_filter_prefix=method_filter_prefix,\n",
    "#         exclude_substrings=exclude_substrings_a,\n",
    "#         verbose=verbose,\n",
    "#     )\n",
    "#     runtime_ratio_b = compute_runtime_ratio_df(\n",
    "#         results_glob=glob_b,\n",
    "#         aggregate=aggregate,\n",
    "#         method_filter_prefix=method_filter_prefix,\n",
    "#         exclude_substrings=exclude_substrings_b,\n",
    "#         verbose=verbose,\n",
    "#     )\n",
    "\n",
    "#     # --------------------------------------------------------\n",
    "#     # 3) Error ratios (err_PI / err_RL) via constant stepper\n",
    "#     # --------------------------------------------------------\n",
    "#     rl_err_a, err_ratio_a = compute_error_dfs_for_dataset(\n",
    "#         results_glob=glob_a,\n",
    "#         const_results=const_results_a,\n",
    "#         aggregate=aggregate,\n",
    "#         method_filter_prefix=method_filter_prefix,\n",
    "#         exclude_substrings=exclude_substrings_a,\n",
    "#         verbose=verbose,\n",
    "#     )\n",
    "#     rl_err_b, err_ratio_b = compute_error_dfs_for_dataset(\n",
    "#         results_glob=glob_b,\n",
    "#         const_results=const_results_b,\n",
    "#         aggregate=aggregate,\n",
    "#         method_filter_prefix=method_filter_prefix,\n",
    "#         exclude_substrings=exclude_substrings_b,\n",
    "#         verbose=verbose,\n",
    "#     )\n",
    "\n",
    "#     if runtime_ratio_a is None or runtime_ratio_b is None:\n",
    "#         print(\"[ERROR] Runtime ratio DF is None; aborting.\")\n",
    "#         return None, None\n",
    "\n",
    "#     if err_ratio_a is None or err_ratio_b is None:\n",
    "#         print(\"[ERROR] Error ratio DF is None; aborting.\")\n",
    "#         return None, None\n",
    "\n",
    "#     # Align indices/columns for safety\n",
    "#     runtime_ratio_a = runtime_ratio_a.loc[rl_err_a.index, rl_err_a.columns]\n",
    "#     runtime_ratio_b = runtime_ratio_b.loc[rl_err_b.index, rl_err_b.columns]\n",
    "\n",
    "#     err_ratio_a = err_ratio_a.loc[runtime_ratio_a.index, runtime_ratio_a.columns]\n",
    "#     err_ratio_b = err_ratio_b.loc[runtime_ratio_b.index, runtime_ratio_b.columns]\n",
    "\n",
    "#     # Combined fields\n",
    "#     combined_a = runtime_ratio_a * err_ratio_a\n",
    "#     combined_b = runtime_ratio_b * err_ratio_b\n",
    "\n",
    "#     # --------------------------------------------------------\n",
    "#     # 4) Colour scale: diverging around 1\n",
    "#     # --------------------------------------------------------\n",
    "#     vals_a = combined_a.values[np.isfinite(combined_a.values)]\n",
    "#     vals_b = combined_b.values[np.isfinite(combined_b.values)]\n",
    "\n",
    "#     if vals_a.size == 0 and vals_b.size == 0:\n",
    "#         print(\"[ERROR] No finite combined values to plot.\")\n",
    "#         return combined_a, combined_b\n",
    "\n",
    "#     if vals_a.size == 0:\n",
    "#         all_vals = vals_b\n",
    "#     elif vals_b.size == 0:\n",
    "#         all_vals = vals_a\n",
    "#     else:\n",
    "#         all_vals = np.concatenate([vals_a, vals_b])\n",
    "\n",
    "#     vmin_sym, vmax_sym = _symmetric_limits_around_one(all_vals)\n",
    "\n",
    "#     if n_bins_diverging % 2 == 1:\n",
    "#         n_bins_diverging += 1\n",
    "#     levels = _diverging_levels(vmin_sym, vmax_sym, n_bins_total=n_bins_diverging)\n",
    "#     ncolors = len(levels) - 1\n",
    "\n",
    "#     # base cmap: cmocean.balance if available, otherwise coolwarm\n",
    "#     if _HAS_CMOCEAN:\n",
    "#         base_cmap = cmocean.cm.balance\n",
    "#     else:\n",
    "#         base_cmap = plt.get_cmap(\"coolwarm\")\n",
    "\n",
    "#     cmap_disc = ListedColormap(base_cmap(np.linspace(0, 1, ncolors)))\n",
    "#     cmap_disc.set_bad(color=\"white\")\n",
    "#     norm = BoundaryNorm(boundaries=levels, ncolors=ncolors)\n",
    "\n",
    "#     # --------------------------------------------------------\n",
    "#     # 5) Figure layout\n",
    "#     # --------------------------------------------------------\n",
    "#     fig = plt.figure(figsize=(24, 8))\n",
    "#     gs = fig.add_gridspec(\n",
    "#         nrows=1,\n",
    "#         ncols=2,\n",
    "#         width_ratios=[1, 1],\n",
    "#         wspace=0.2,\n",
    "#     )\n",
    "\n",
    "#     ax_a = fig.add_subplot(gs[0, 0])\n",
    "#     ax_b = fig.add_subplot(gs[0, 1])\n",
    "#     cax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # colourbar axis\n",
    "\n",
    "#     # Panel (a): 3by3\n",
    "#     ann_a = _annotations(combined_a)\n",
    "#     sns.heatmap(\n",
    "#         combined_a,\n",
    "#         annot=ann_a.values,\n",
    "#         fmt=\"\",\n",
    "#         cmap=cmap_disc,\n",
    "#         norm=norm,\n",
    "#         mask=combined_a.isna(),\n",
    "#         cbar=False,\n",
    "#         annot_kws={\"size\": sizes - 8},\n",
    "#         linewidths=0.5,\n",
    "#         linecolor=\"gray\",\n",
    "#         square=True,\n",
    "#         ax=ax_a,\n",
    "#     )\n",
    "#     _overlay_na(ax_a, combined_a, fontsize=sizes - 6)\n",
    "#     ax_a.set_xlabel(r\"$\\alpha$\", fontsize=sizes)\n",
    "#     ax_a.set_ylabel(r\"Discretization ($N_x$ by $N_z$)\", fontsize=sizes)\n",
    "#     ax_a.tick_params(axis=\"both\", labelsize=sizes - 4)\n",
    "#     ax_a.text(\n",
    "#         -0.07,\n",
    "#         1.05,\n",
    "#         r\"(a)\",\n",
    "#         transform=ax_a.transAxes,\n",
    "#         fontsize=sizes,\n",
    "#         va=\"bottom\",\n",
    "#         ha=\"right\",\n",
    "#     )\n",
    "\n",
    "#     # Panel (b): 5by5\n",
    "#     ann_b = _annotations(combined_b)\n",
    "#     sns.heatmap(\n",
    "#         combined_b,\n",
    "#         annot=ann_b.values,\n",
    "#         fmt=\"\",\n",
    "#         cmap=cmap_disc,\n",
    "#         norm=norm,\n",
    "#         mask=combined_b.isna(),\n",
    "#         cbar=False,\n",
    "#         annot_kws={\"size\": sizes - 8},\n",
    "#         linewidths=0.5,\n",
    "#         linecolor=\"gray\",\n",
    "#         square=True,\n",
    "#         ax=ax_b,\n",
    "#     )\n",
    "#     _overlay_na(ax_b, combined_b, fontsize=sizes - 6)\n",
    "#     ax_b.set_xlabel(r\"$\\alpha$\", fontsize=sizes)\n",
    "#     ax_b.set_ylabel(r\"Discretization ($N_x$ by $N_z$)\", fontsize=sizes)\n",
    "#     ax_b.tick_params(axis=\"both\", labelsize=sizes - 4)\n",
    "#     ax_b.text(\n",
    "#         -0.07,\n",
    "#         1.2,\n",
    "#         r\"(b)\",\n",
    "#         transform=ax_b.transAxes,\n",
    "#         fontsize=sizes,\n",
    "#         va=\"bottom\",\n",
    "#         ha=\"right\",\n",
    "#     )\n",
    "\n",
    "#     # Shared colourbar\n",
    "#     sm = ScalarMappable(norm=norm, cmap=cmap_disc)\n",
    "#     sm.set_array([])\n",
    "#     cbar = fig.colorbar(sm, cax=cax)\n",
    "\n",
    "#     # Ticks, ensuring 1.0 is visible\n",
    "#     n_ticks = 7\n",
    "#     base_ticks = np.linspace(vmin_sym, vmax_sym, n_ticks)\n",
    "#     if len(base_ticks) > 0:\n",
    "#         idx_closest = np.argmin(np.abs(base_ticks - 1.0))\n",
    "#         base_ticks[idx_closest] = 1.0\n",
    "\n",
    "#     ticks = np.sort(np.unique(base_ticks))\n",
    "#     ticks = np.array([ 0.5, 1,  2.5, 5])\n",
    "\n",
    "#     cbar.set_ticks(ticks)\n",
    "#     tick_labels = [f\"{t:.2f}\" if abs(t - 1.0) > 0.01 else \"1.00\" for t in ticks]\n",
    "#     cbar.set_ticklabels(tick_labels)\n",
    "#     cbar.set_label(\n",
    "#         r\"Proficiency\",\n",
    "#         fontsize=sizes,\n",
    "#     )\n",
    "#     cbar.ax.tick_params(labelsize=sizes - 4)\n",
    "\n",
    "#     fig.tight_layout(rect=[0.03, 0.05, 0.97, 0.95])\n",
    "#     fig.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "#     print(f\"Saved combined runtime-error dual heatmap to {save_path}\")\n",
    "#     plt.show()\n",
    "\n",
    "#     return combined_a, combined_b\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # Example call\n",
    "# # ============================================================\n",
    "# if __name__ == \"__main__\":\n",
    "#     combined_3by3, combined_5by5 = create_combined_runtime_error_dual_heatmap(\n",
    "#         const_output_dir=\".\",                   # where constant_stepper_*.npz live\n",
    "#         glob_a=\"eval_results_3by3_*.json\",\n",
    "#         glob_b=\"eval_results_5by5_*.json\",\n",
    "#         aggregate=\"min\",                        # best RL per alpha\n",
    "#         method_filter_prefix=None,              # or \"TQC \" for only TQC\n",
    "#         exclude_substrings_a=None,\n",
    "#         exclude_substrings_b=[\"_PI_\"],          # exclude eval_results_5by5_PI_*.json\n",
    "#         save_path=\"combined_runtime_error_3by3_vs_5by5.pdf\",\n",
    "#         n_bins_diverging=10,\n",
    "#         verbose=True,\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8590c072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from plants.faults import strikeslip  # your module\n",
    "\n",
    "# ============================================================\n",
    "# Global style (your settings)\n",
    "# ============================================================\n",
    "sizes = 30\n",
    "plt.rcParams.update({\n",
    "    'font.size': sizes,\n",
    "    'axes.titlesize': sizes,\n",
    "    'axes.labelsize': sizes,\n",
    "    'xtick.labelsize': sizes,\n",
    "    'ytick.labelsize': sizes,\n",
    "    'legend.fontsize': sizes,\n",
    "    'figure.titlesize': sizes,\n",
    "    'text.usetex': True,\n",
    "    'font.family': 'Times',\n",
    "    'pdf.fonttype': 42,\n",
    "    'ps.fonttype': 42,\n",
    "    'text.latex.preamble': r'\\usepackage{amsmath}\\usepackage{mathptmx}',\n",
    "    'axes.grid': False,\n",
    "})\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "\n",
    "def _disc_key(label: str) -> int:\n",
    "    \"\"\"Sort key for labels like '25x25' -> 25.\"\"\"\n",
    "    try:\n",
    "        return int(label.split(\"x\")[0])\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def _aggregate_values(vals, how: str = \"min\") -> float:\n",
    "    \"\"\"Aggregate a list of runtimes according to 'min', 'mean', or 'max'.\"\"\"\n",
    "    arr = np.array([v for v in vals if np.isfinite(v) and v > 0.0], dtype=float)\n",
    "    if arr.size == 0:\n",
    "        return np.nan\n",
    "    how = how.lower()\n",
    "    if how == \"min\":\n",
    "        return float(np.min(arr))\n",
    "    if how == \"mean\":\n",
    "        return float(np.mean(arr))\n",
    "    if how == \"max\":\n",
    "        return float(np.max(arr))\n",
    "    return float(np.min(arr))\n",
    "\n",
    "\n",
    "def build_fault_minimal(Nz: int, Nx: int, zdepth: float, xlength: float):\n",
    "    \"\"\"\n",
    "    Minimal fault builder for post-processing.\n",
    "\n",
    "    We only need:\n",
    "      - fault.Dscale\n",
    "      - fault.N  (number of DOFs)\n",
    "    \"\"\"\n",
    "    fault_local = strikeslip.qs_strikeslip_fault(\n",
    "        zdepth=zdepth,\n",
    "        xlength=xlength,\n",
    "        Nz=Nz,\n",
    "        Nx=Nx,\n",
    "        G=30000.0,\n",
    "        rho=2.5e-3,\n",
    "        zeta=0.8 / 3.0,        # as in your main setup\n",
    "        Ks_path=\"./Data/\",\n",
    "        gamma_s=25.0,\n",
    "        gamma_w=10.0,\n",
    "        sigma_ref=100.0,\n",
    "        depth_ini=0.0,\n",
    "        vinf=3.171e-10,\n",
    "        Dmu_estimate=0.5,\n",
    "    )\n",
    "    return {\n",
    "        \"fault\": fault_local,\n",
    "        \"N_DOFS\": fault_local.N,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Load constant-stepper reference (s_const)\n",
    "# ============================================================\n",
    "\n",
    "def load_constant_stepper_results(\n",
    "    output_dir: str,\n",
    "    file_pattern: str,\n",
    "    zdepth: float,\n",
    "    xlength: float,\n",
    "    is_5by5: bool,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load constant-stepper results from NPZ files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_dir : directory containing the NPZ files\n",
    "    file_pattern : glob pattern, e.g.\n",
    "        - \"constant_stepper_*by*.npz\"       (3-by-3 case)\n",
    "        - \"constant_stepper_*by*_5by5.npz\"  (5-by-5 case)\n",
    "    zdepth, xlength : physical dimensions (3 or 5)\n",
    "    is_5by5 : if True, only keep files whose stem contains '_5by5';\n",
    "              if False, skip files containing '_5by5'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[label] -> s_const_end  (physical final mean slip)\n",
    "        label like '25x25', '50x50', ...\n",
    "    \"\"\"\n",
    "    out_path = Path(output_dir)\n",
    "    files = sorted(out_path.glob(file_pattern))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nLoading constant-stepper from {output_dir} with pattern '{file_pattern}':\")\n",
    "        print(\"  Found:\", [f.name for f in files])\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for npz_path in files:\n",
    "        stem = npz_path.stem  # e.g. 'constant_stepper_25by25' or 'constant_stepper_25by25_5by5'\n",
    "\n",
    "        if is_5by5:\n",
    "            if \"_5by5\" not in stem:\n",
    "                continue\n",
    "        else:\n",
    "            if \"_5by5\" in stem:\n",
    "                continue\n",
    "\n",
    "        # Extract \"25by25\" part before any extra suffix\n",
    "        grid_part = stem.replace(\"constant_stepper_\", \"\")  # '25by25' or '25by25_5by5'\n",
    "        grid_main = grid_part.split(\"_\")[0]                # '25by25'\n",
    "        try:\n",
    "            Nz_str, Nx_str = grid_main.split(\"by\")\n",
    "            Nz, Nx = int(Nz_str), int(Nx_str)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"  [WARN] Could not parse Nz,Nx from {stem}: {e}\")\n",
    "            continue\n",
    "\n",
    "        grid = build_fault_minimal(Nz, Nx, zdepth=zdepth, xlength=xlength)\n",
    "        fault = grid[\"fault\"]\n",
    "        N = grid[\"N_DOFS\"]\n",
    "        Dscale = fault.Dscale\n",
    "\n",
    "        data = np.load(npz_path)\n",
    "\n",
    "        if \"ts1\" not in data.files or \"ys1\" not in data.files:\n",
    "            if verbose:\n",
    "                print(f\"  [WARN] {npz_path.name} missing ts1/ys1; skipping.\")\n",
    "            continue\n",
    "\n",
    "        ys = data[\"ys1\"]\n",
    "        if ys.shape[1] < 3 * N:\n",
    "            if verbose:\n",
    "                print(f\"  [WARN] {npz_path.name}: ys shape {ys.shape} < 3*N={3*N}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # slip block (dimensionless), average over DOFs, then scale\n",
    "        s_block = ys[:, 2 * N : 3 * N]\n",
    "        avg_s_dimless = np.mean(s_block, axis=1)\n",
    "        avg_s_phys = avg_s_dimless * Dscale\n",
    "        s_end = float(avg_s_phys[-1])\n",
    "\n",
    "        label = f\"{Nz}x{Nx}\"\n",
    "        results[label] = s_end\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"  {npz_path.name}: label={label}, s_const_end={s_end:.3e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Load PI results (runtime + final slip)\n",
    "# ============================================================\n",
    "\n",
    "def load_pi_results(glob_pattern: str,\n",
    "                    stem_prefix_to_strip: str,\n",
    "                    verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Load PI_baseline runtime and final slip s_end_mean from JSON files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[label] -> dict(runtime_s, s_end)\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    files = sorted(Path(\".\").glob(glob_pattern))\n",
    "    if verbose:\n",
    "        print(f\"\\nLoading PI_baseline from {glob_pattern}:\")\n",
    "        print(\"  Found:\", [f.name for f in files])\n",
    "\n",
    "    for path in files:\n",
    "        stem = path.stem  # 'eval_results_3by3_25x25' or 'eval_results_5by5_25x25'\n",
    "        label = stem.replace(stem_prefix_to_strip, \"\")  # '25x25', etc.\n",
    "\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as fh:\n",
    "                data = json.load(fh)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"  [WARN] Failed to read {path.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        pi_data = data.get(\"PI_baseline\", {})\n",
    "        runtime = float(pi_data.get(\"runtime_s\", np.nan))\n",
    "        s_end = pi_data.get(\"s_end_mean\", np.nan)\n",
    "        s_end = float(s_end) if s_end is not None else np.nan\n",
    "\n",
    "        if not np.isfinite(runtime) or not np.isfinite(s_end):\n",
    "            if verbose:\n",
    "                print(f\"  [WARN] {path.name}: invalid PI runtime/slip ({runtime}, {s_end})\")\n",
    "            continue\n",
    "\n",
    "        results[label] = dict(runtime_s=runtime, s_end=s_end)\n",
    "        if verbose:\n",
    "            print(f\"  {path.name}: label={label}, PI runtime={runtime:.3f}s, s_end={s_end:.3e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Load RESUMED RL (runtime + final slip)\n",
    "# ============================================================\n",
    "\n",
    "def load_resumed_rl_runtime_and_slip(\n",
    "    glob_pattern: str,\n",
    "    method_filter_prefix: str | None = None,\n",
    "    is_5by5: bool = False,\n",
    "    aggregate: str = \"min\",\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load RL runtimes and final slips from eval_results_RESUMED*.json.\n",
    "\n",
    "    For each file:\n",
    "      - filter RL methods (skip PI_baseline, optionally by prefix),\n",
    "      - among those, pick the method with MIN runtime,\n",
    "      - record its runtime and s_end_mean.\n",
    "\n",
    "    If multiple files map to the same label, we again keep the minimum runtime\n",
    "    (and its associated slip).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[label] -> dict(runtime_s, s_end)\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    files = sorted(Path(\".\").glob(glob_pattern))\n",
    "    if verbose:\n",
    "        print(f\"\\nLoading RESUMED RL from {glob_pattern}:\")\n",
    "        print(\"  Found:\", [f.name for f in files])\n",
    "\n",
    "    for path in files:\n",
    "        stem = path.stem\n",
    "        # Determine label (e.g. '25x25')\n",
    "        if is_5by5:\n",
    "            if \"5by5\" not in stem:\n",
    "                continue\n",
    "            label = stem.split(\"_\")[-1]  # eval_results_RESUMED_5by5_25x25 -> '25x25'\n",
    "        else:\n",
    "            if \"5by5\" in stem:\n",
    "                continue\n",
    "            parts = stem.replace(\"eval_results_RESUMED_\", \"\").split(\"_\")\n",
    "            label = parts[-1]  # eval_results_RESUMED_25x25 -> '25x25' (or similar)\n",
    "\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as fh:\n",
    "                data = json.load(fh)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"  [WARN] Failed to read {path.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        best_runtime = np.inf\n",
    "        best_s_end = np.nan\n",
    "\n",
    "        for method_name, method_data in data.items():\n",
    "            if method_name == \"PI_baseline\":\n",
    "                continue\n",
    "            if method_filter_prefix and not method_name.startswith(method_filter_prefix):\n",
    "                continue\n",
    "\n",
    "            converged = bool(method_data.get(\"converged\", False))\n",
    "            success_rate = float(method_data.get(\"success_rate\", 0.0))\n",
    "            if not (converged or success_rate > 0.0):\n",
    "                continue\n",
    "\n",
    "            rl_runtime = method_data.get(\"runtime_s\", np.nan)\n",
    "            s_end = method_data.get(\"s_end_mean\", np.nan)\n",
    "\n",
    "            if not (np.isfinite(rl_runtime) and rl_runtime > 0.0 and np.isfinite(s_end)):\n",
    "                continue\n",
    "\n",
    "            if rl_runtime < best_runtime:\n",
    "                best_runtime = float(rl_runtime)\n",
    "                best_s_end = float(s_end)\n",
    "\n",
    "        if not np.isfinite(best_runtime):\n",
    "            if verbose:\n",
    "                print(f\"  [WARN] {path.name}: no valid RL methods.\")\n",
    "            continue\n",
    "\n",
    "        # If label already present, keep the best across files\n",
    "        if label in results:\n",
    "            prev = results[label]\n",
    "            if best_runtime < prev[\"runtime_s\"]:\n",
    "                results[label] = dict(runtime_s=best_runtime, s_end=best_s_end)\n",
    "        else:\n",
    "            results[label] = dict(runtime_s=best_runtime, s_end=best_s_end)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"  {path.name}: label={label}, RL runtime={best_runtime:.3f}s, s_end={best_s_end:.3e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Main: dual scatter (a) runtime ratio, (b) error ratio\n",
    "# ============================================================\n",
    "\n",
    "def scatter_runtime_and_error_ratio_resumed(\n",
    "    const_output_dir: str = \".\",\n",
    "    pi_glob_3by3: str = \"eval_results_3by3_*.json\",\n",
    "    pi_glob_5by5: str = \"eval_results_5by5_*.json\",\n",
    "    rl_glob_3by3: str = \"eval_results_RESUMED_*.json\",\n",
    "    rl_glob_5by5: str = \"eval_results_RESUMED_5by5_*.json\",\n",
    "    method_filter_prefix: str | None = None,\n",
    "    aggregate: str = \"min\",\n",
    "    save_path: str = \"runtime_and_error_ratio_resumed_scatter.pdf\",\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a figure with two panels:\n",
    "\n",
    "    (a) runtime ratio (PI / RL_resumed) vs discretization, for 3x3 and 5x5.\n",
    "    (b) relative error ratio (err_PI / err_RL) vs discretization,\n",
    "        where errors are relative to the constant-stepper final slip.\n",
    "\n",
    "    Reference (for both 3x3 and 5x5) is s_const from constant_stepper_*.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Constant stepper (reference solution)\n",
    "    const_3 = load_constant_stepper_results(\n",
    "        output_dir=const_output_dir,\n",
    "        file_pattern=\"constant_stepper_*by*.npz\",\n",
    "        zdepth=3.0,\n",
    "        xlength=3.0,\n",
    "        is_5by5=False,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    const_5 = load_constant_stepper_results(\n",
    "        output_dir=const_output_dir,\n",
    "        file_pattern=\"constant_stepper_*by*_5by5.npz\",\n",
    "        zdepth=5.0,\n",
    "        xlength=5.0,\n",
    "        is_5by5=True,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    # 2) PI results (3by3 and 5by5)\n",
    "    pi_3 = load_pi_results(\n",
    "        glob_pattern=pi_glob_3by3,\n",
    "        stem_prefix_to_strip=\"eval_results_3by3_\",\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    pi_5 = load_pi_results(\n",
    "        glob_pattern=pi_glob_5by5,\n",
    "        stem_prefix_to_strip=\"eval_results_5by5_\",\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    # 3) RESUMED RL (3by3 and 5by5)\n",
    "    rl_3 = load_resumed_rl_runtime_and_slip(\n",
    "        glob_pattern=rl_glob_3by3,\n",
    "        method_filter_prefix=method_filter_prefix,\n",
    "        is_5by5=False,\n",
    "        aggregate=aggregate,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    rl_5 = load_resumed_rl_runtime_and_slip(\n",
    "        glob_pattern=rl_glob_5by5,\n",
    "        method_filter_prefix=method_filter_prefix,\n",
    "        is_5by5=True,\n",
    "        aggregate=aggregate,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    # 4) Build datasets for 3x3 and 5x5\n",
    "    labels_3 = sorted(\n",
    "        set(pi_3.keys()) & set(rl_3.keys()) & set(const_3.keys()),\n",
    "        key=_disc_key,\n",
    "    )\n",
    "    labels_5 = sorted(\n",
    "        set(pi_5.keys()) & set(rl_5.keys()) & set(const_5.keys()),\n",
    "        key=_disc_key,\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nLabels for 3x3:\", labels_3)\n",
    "        print(\"Labels for 5x5:\", labels_5)\n",
    "\n",
    "    def _collect_series(labels, pi_dict, rl_dict, const_dict):\n",
    "        xs = []\n",
    "        runtime_ratios = []\n",
    "        err_ratios = []\n",
    "\n",
    "        for lbl in labels:\n",
    "            try:\n",
    "                N = int(lbl.split(\"x\")[0])\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            pi_rt = pi_dict[lbl][\"runtime_s\"]\n",
    "            pi_s = pi_dict[lbl][\"s_end\"]\n",
    "            rl_rt = rl_dict[lbl][\"runtime_s\"]\n",
    "            rl_s = rl_dict[lbl][\"s_end\"]\n",
    "            s_const = const_dict[lbl]\n",
    "\n",
    "            if not (np.isfinite(pi_rt) and np.isfinite(rl_rt) and rl_rt > 0.0):\n",
    "                continue\n",
    "            if not (np.isfinite(pi_s) and np.isfinite(rl_s) and np.isfinite(s_const)):\n",
    "                continue\n",
    "            if s_const == 0.0:\n",
    "                continue\n",
    "\n",
    "            # runtime ratio\n",
    "            rr = pi_rt / rl_rt\n",
    "\n",
    "            # relative errors w.r.t. constant-stepper\n",
    "            err_pi = abs(pi_s - s_const) / abs(s_const)\n",
    "            err_rl = abs(rl_s - s_const) / abs(s_const)\n",
    "            if err_rl <= 0.0:\n",
    "                err_ratio = np.nan\n",
    "            else:\n",
    "                err_ratio = err_pi / err_rl\n",
    "\n",
    "            if not np.isfinite(rr) or not np.isfinite(err_ratio):\n",
    "                continue\n",
    "\n",
    "            xs.append(N)\n",
    "            runtime_ratios.append(rr)\n",
    "            err_ratios.append(err_ratio)\n",
    "\n",
    "        return np.array(xs, dtype=float), np.array(runtime_ratios, dtype=float), np.array(err_ratios, dtype=float)\n",
    "\n",
    "    x3, rr3, er3 = _collect_series(labels_3, pi_3, rl_3, const_3)\n",
    "    x5, rr5, er5 = _collect_series(labels_5, pi_5, rl_5, const_5)\n",
    "\n",
    "    # =======================================================\n",
    "    # 5) Plot: panel (a) runtime ratio, panel (b) error ratio\n",
    "    #    styled like your FOSM figure (no shared x-axis)\n",
    "    # =======================================================\n",
    "    fig, (ax_a, ax_b) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "    # Axis aesthetics: minor ticks, inward ticks, no gridlines\n",
    "    for ax in (ax_a, ax_b):\n",
    "        ax.minorticks_on()\n",
    "        ax.tick_params(axis='both', which='both', direction='in')\n",
    "        ax.tick_params(axis='both', which='major', length=6)\n",
    "        ax.tick_params(axis='both', which='minor', length=3)\n",
    "        ax.grid(False)\n",
    "        ax.margins(x=0.03, y=0.05)\n",
    "\n",
    "    # ---- Panel (a): runtime ratio PI / RL ----\n",
    "    if x3.size > 0:\n",
    "        ax_a.scatter(x3, rr3, marker=\"o\", s=140,\n",
    "                     label=r\"3 [km] $\\times$ 3 [km]\", zorder=3)\n",
    "        ax_a.plot(x3, rr3, linestyle=\"-\", linewidth=2, alpha=0.6, zorder=2)\n",
    "    if x5.size > 0:\n",
    "        ax_a.scatter(x5, rr5, marker=\"s\", s=140,\n",
    "                     label=r\"5 [km] $\\times$ 5 [km]\", zorder=3)\n",
    "        ax_a.plot(x5, rr5, linestyle=\"--\", linewidth=2, alpha=0.6, zorder=2)\n",
    "\n",
    "    ax_a.axhline(1.0, color=\"k\", linestyle=\":\", linewidth=1.5, zorder=1)\n",
    "\n",
    "    ax_a.set_xlabel(r\"Discretization size $N_x = N_z$\", fontsize=sizes)\n",
    "    ax_a.set_ylabel(r\"Runtime ratio (Heuristic / RL)\", fontsize=sizes)\n",
    "\n",
    "    all_x_a = np.concatenate([x3, x5]) if (x3.size and x5.size) else (x3 if x3.size else x5)\n",
    "    if all_x_a.size > 0:\n",
    "        xticks = np.array(sorted(np.unique(all_x_a))).astype(int)\n",
    "        ax_a.set_xticks(xticks)\n",
    "        ax_a.set_xticklabels([f\"{int(n)}\" for n in xticks])\n",
    "\n",
    "    ax_a.tick_params(axis=\"both\", labelsize=sizes - 4)\n",
    "    ax_a.legend(loc=\"best\", frameon=False)\n",
    "\n",
    "    # Panel label (a) â€“ to the left of the axis\n",
    "    ax_a.text(-0.22, 1.02, '(a)', transform=ax_a.transAxes,\n",
    "              va='top', ha='left')\n",
    "\n",
    "    # ---- Panel (b): error ratio (err_PI / err_RL) ----\n",
    "    if x3.size > 0:\n",
    "        ax_b.scatter(x3, er3, marker=\"o\", s=140,\n",
    "                     label=r\"3 [km] $\\times$ 3 [km]\", zorder=3)\n",
    "        ax_b.plot(x3, er3, linestyle=\"-\", linewidth=2, alpha=0.6, zorder=2)\n",
    "    if x5.size > 0:\n",
    "        ax_b.scatter(x5, er5, marker=\"s\", s=140,\n",
    "                     label=r\"5 [km] $\\times$ 5 [km]\", zorder=3)\n",
    "        ax_b.plot(x5, er5, linestyle=\"--\", linewidth=2, alpha=0.6, zorder=2)\n",
    "\n",
    "    # (No horizontal line here, as in your tweak)\n",
    "    ax_b.set_xlabel(r\"Discretization size $N_x = N_z$\", fontsize=sizes)\n",
    "    ax_b.set_ylabel(r\"Relative error ratio (Heuristic / RL)\", fontsize=sizes)\n",
    "\n",
    "    all_x_b = np.concatenate([x3, x5]) if (x3.size and x5.size) else (x3 if x3.size else x5)\n",
    "    if all_x_b.size > 0:\n",
    "        xticks_b = np.array(sorted(np.unique(all_x_b))).astype(int)\n",
    "        ax_b.set_xticks(xticks_b)\n",
    "        ax_b.set_xticklabels([f\"{int(n)}\" for n in xticks_b])\n",
    "\n",
    "    ax_b.tick_params(axis=\"both\", labelsize=sizes - 4)\n",
    "    # No legend on (b), to mirror your example figure\n",
    "\n",
    "    # Panel label (b) â€“ to the left of the axis\n",
    "    ax_b.text(-0.22, 1.02, '(b)', transform=ax_b.transAxes,\n",
    "              va='top', ha='left')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"\\nSaved dual scatter figure to {save_path}\")\n",
    "\n",
    "    # small summary table if you want to inspect\n",
    "    rows = []\n",
    "    for lbl in sorted(set(labels_3) | set(labels_5), key=_disc_key):\n",
    "        row = {\"grid\": lbl}\n",
    "        if lbl in labels_3:\n",
    "            N3 = int(lbl.split(\"x\")[0])\n",
    "            idx3 = np.where(x3 == N3)[0]\n",
    "            if idx3.size > 0:\n",
    "                row[\"N\"] = N3\n",
    "                row[\"runtime_ratio_3x3\"] = float(rr3[idx3[0]])\n",
    "                row[\"error_ratio_3x3\"] = float(er3[idx3[0]])\n",
    "        if lbl in labels_5:\n",
    "            N5 = int(lbl.split(\"x\")[0])\n",
    "            idx5 = np.where(x5 == N5)[0]\n",
    "            if idx5.size > 0:\n",
    "                row.setdefault(\"N\", N5)\n",
    "                row[\"runtime_ratio_5x5\"] = float(rr5[idx5[0]])\n",
    "                row[\"error_ratio_5x5\"] = float(er5[idx5[0]])\n",
    "        rows.append(row)\n",
    "\n",
    "    df_summary = pd.DataFrame(rows).set_index(\"grid\")\n",
    "    return df_summary\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Example call\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    df_summary = scatter_runtime_and_error_ratio_resumed(\n",
    "        const_output_dir=\".\",                         # where constant_stepper_*.npz live\n",
    "        pi_glob_3by3=\"eval_results_3by3_*.json\",\n",
    "        pi_glob_5by5=\"eval_results_5by5_*.json\",\n",
    "        rl_glob_3by3=\"eval_results_RESUMED_*.json\",\n",
    "        rl_glob_5by5=\"eval_results_RESUMED_5by5_*.json\",\n",
    "        method_filter_prefix=None,                   # or \"TQC \" if you only want TQC\n",
    "        aggregate=\"min\",                             # fastest RL\n",
    "        save_path=\"transfer_learned.pdf\",\n",
    "        verbose=True,\n",
    "    )\n",
    "    print(\"\\nSummary table:\")\n",
    "    print(df_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c1591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fem-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
